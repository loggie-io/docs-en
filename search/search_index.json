{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"blog/","text":".md-sidebar--secondary:not([hidden]) { visibility: hidden; } Blog \u00b6 Quickly Build a Scalable Cloud-Native Logging Architecture with Loggie \u00b6 ethfoo 2022-10-26 10 min read Abstract\uff1a Introduction and philosophy of Loggie Cases of using Loggie to collect logs in Kubernetes Using Loggie to build log collection architectures of different scales Continue reading Practice of building NetEase Shufan cloud-native log platform \u00b6 ethfoo 2022-03-11 8 min read NetEase has been exploring and practicing cloud native applications since 2015. As an important part of observability, the log collection platform has also undergone the evolution from traditional host to container, in order to support the large-scale cloud native transformation of various business departments within enterprise group. This article will describe the problems we encountered, solutions we used for evolution and transformation, and experiences and best practices we have precipitated in this process. Abstract\uff1a Operator-based log collection Dilemmas and challenges in large-scale scenarios The present and future of open-sourced Loggie Continue reading","title":"Index"},{"location":"blog/#blog","text":"","title":"Blog"},{"location":"blog/#quickly-build-a-scalable-cloud-native-logging-architecture-with-loggie","text":"ethfoo 2022-10-26 10 min read Abstract\uff1a Introduction and philosophy of Loggie Cases of using Loggie to collect logs in Kubernetes Using Loggie to build log collection architectures of different scales Continue reading","title":"Quickly Build a Scalable Cloud-Native Logging Architecture with Loggie"},{"location":"blog/#practice-of-building-netease-shufan-cloud-native-log-platform","text":"ethfoo 2022-03-11 8 min read NetEase has been exploring and practicing cloud native applications since 2015. As an important part of observability, the log collection platform has also undergone the evolution from traditional host to container, in order to support the large-scale cloud native transformation of various business departments within enterprise group. This article will describe the problems we encountered, solutions we used for evolution and transformation, and experiences and best practices we have precipitated in this process. Abstract\uff1a Operator-based log collection Dilemmas and challenges in large-scale scenarios The present and future of open-sourced Loggie Continue reading","title":"Practice of building NetEase Shufan cloud-native log platform"},{"location":"developer-guide/contributing/","text":"Contributing \u00b6 Note If you: have new ideas sumbmit feature report/fix bug contribute documentation help wanted It is recommended to raise Issues first, and describe your purpose or problem. Code/Documentation Contribution Process \u00b6 1. Create Local Project \u00b6 Fork Project \u00b6 Visit https://github.com/loggie-io/loggie , click Fork in the upper right corner, and fork it to your own GitHub repository. Clone to Local \u00b6 Clone the forked project to the local: cd ${ go -workspace } git clone git@github.com: ${ yourAccountId } /loggie.git 2. Local development \u00b6 Add Upstream \u00b6 In local project, add upstream remote: git remote add upstream https://github.com/loggie-io/loggie.git git remote set-url --push upstream no_push You can use git remote -v to check. Origin is the git address of your personal account fork, upstream is the official address of Loggie. Uptream is only used for local synchronization code (normally you do not have push permission). Create Branch \u00b6 Create your branch based on the latest main branch, usually starting with feat/fix/chore etc. git checkout -b feat-mywork After development, it is recommended to keep the code consistent with the upstream before submitting: git pull upstream -r Submit Code \u00b6 git commit git push origin feat-mywork Commit Specification Usually we use <type>(<scope>): <subject> : <type>: (necessary) feat: represents a new feature fix: bug fix chore: changes to the build process or assistant tools test: test related docs: documentation mofitied perf: performance optimization related changes refactor: code refactoring <scope>: (optional) core: Changes of the Loggie core code framework source: Some functions of the Loggie source have been added or changed sink: Some functions of the logie sink are added or changed interceptor: Some functions of the Logie interceptor have been added or changed. discovery: Service discovery and configuration monitor: monitor eventbus <subject>: (required)the description of the commit 3. Create PR \u00b6 Submit to GitHub \u00b6 Visit the Loggie project forked in your personal GitHub account, and Open pull request \u3002 Try to describe the PR's background, purpose, and changes as clearly as possible. If there are related Issues, they need to be associated. Please check before submitting: Do you need to add/modify relevant unit test/e2e/benchmark Whether related documents need to be added/modified Seek for Merge \u00b6 You can find someone in OWNERS, or by default add the loggie-robot account as Reviewers or Assignees of your PR.","title":"Contributing"},{"location":"developer-guide/contributing/#contributing","text":"Note If you: have new ideas sumbmit feature report/fix bug contribute documentation help wanted It is recommended to raise Issues first, and describe your purpose or problem.","title":"Contributing"},{"location":"developer-guide/contributing/#codedocumentation-contribution-process","text":"","title":"Code/Documentation Contribution Process"},{"location":"developer-guide/contributing/#1-create-local-project","text":"","title":"1. Create Local Project"},{"location":"developer-guide/contributing/#fork-project","text":"Visit https://github.com/loggie-io/loggie , click Fork in the upper right corner, and fork it to your own GitHub repository.","title":"Fork Project"},{"location":"developer-guide/contributing/#clone-to-local","text":"Clone the forked project to the local: cd ${ go -workspace } git clone git@github.com: ${ yourAccountId } /loggie.git","title":"Clone to Local"},{"location":"developer-guide/contributing/#2-local-development","text":"","title":"2. Local development"},{"location":"developer-guide/contributing/#add-upstream","text":"In local project, add upstream remote: git remote add upstream https://github.com/loggie-io/loggie.git git remote set-url --push upstream no_push You can use git remote -v to check. Origin is the git address of your personal account fork, upstream is the official address of Loggie. Uptream is only used for local synchronization code (normally you do not have push permission).","title":"Add Upstream"},{"location":"developer-guide/contributing/#create-branch","text":"Create your branch based on the latest main branch, usually starting with feat/fix/chore etc. git checkout -b feat-mywork After development, it is recommended to keep the code consistent with the upstream before submitting: git pull upstream -r","title":"Create Branch"},{"location":"developer-guide/contributing/#submit-code","text":"git commit git push origin feat-mywork Commit Specification Usually we use <type>(<scope>): <subject> : <type>: (necessary) feat: represents a new feature fix: bug fix chore: changes to the build process or assistant tools test: test related docs: documentation mofitied perf: performance optimization related changes refactor: code refactoring <scope>: (optional) core: Changes of the Loggie core code framework source: Some functions of the Loggie source have been added or changed sink: Some functions of the logie sink are added or changed interceptor: Some functions of the Logie interceptor have been added or changed. discovery: Service discovery and configuration monitor: monitor eventbus <subject>: (required)the description of the commit","title":"Submit Code"},{"location":"developer-guide/contributing/#3-create-pr","text":"","title":"3. Create PR"},{"location":"developer-guide/contributing/#submit-to-github","text":"Visit the Loggie project forked in your personal GitHub account, and Open pull request \u3002 Try to describe the PR's background, purpose, and changes as clearly as possible. If there are related Issues, they need to be associated. Please check before submitting: Do you need to add/modify relevant unit test/e2e/benchmark Whether related documents need to be added/modified","title":"Submit to GitHub"},{"location":"developer-guide/contributing/#seek-for-merge","text":"You can find someone in OWNERS, or by default add the loggie-robot account as Reviewers or Assignees of your PR.","title":"Seek for Merge"},{"location":"developer-guide/development/","text":"Local Development \u00b6 How to develop and debug Loggie locally. Local Project \u00b6 Golang Development Environment \u00b6 Loggie is written in Golang, please make sure you have Golang development environment. go version Code \u00b6 Please use your own GitHub account to fork Loggie. Then git clone it to local. git clone git@github.com:<Account>/loggie.git Local Environment \u00b6 Under normal circumstances, there are no special local dependencies. For different scenarios, you need to install or use external dependencies according to specific situations. It is recommended to use Kubernetes locally for deployment and management. Kubernetes \u00b6 Build Kubernetes Kind is recommended. Deploy Dependent Components Helm is recommended. Related components can be added and deployed using public Helm repository. Local Loggie Connects to Kubernetes If you need to use CRDs such as LogConfig, or debug in Kubernetes, you need to enable Kubernetes Discovery in the system configuration. loggie.yml discovery : enabled : true kubernetes : kubeconfig : ${home}/.kube/config Specify kubeconfig to connect to local Kubernetes APIServer. In addition, Loggie in the form of Agent will only monitor the Kubernetes Pod events of this node. You need to add -meta.nodeName in the Loggie CMD arguments to simulate the node where it is located. For example, you can kubectl get node to view all node names: NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 2y50d v1.21.0 Add -meta.nodeName=kind-control-plane in CMD arguments, so that Loggie can be considered to be deployed on the specific node after startup. Verify and Debug \u00b6 After Loggie is started by default, the output log format is JSON. If you are not used to it, you can add -log.jsonFormat=false in CMD arguments to turn it off. Try to use dev method locally, and make reasonable use of dev source and dev sink. for example: To develop source, configure dev sink for local verification To develop sink, configure dev source to simulate input or configure file source to collect local files To develop an interceptor, configure both dev source and dev sink for simulation Example: In pipelines, use dev sink to see the final output of the data sent to downstream. pipelines.yml sink : type : \"dev\" printEvents : true codec : pretty : true","title":"Local Development"},{"location":"developer-guide/development/#local-development","text":"How to develop and debug Loggie locally.","title":"Local Development"},{"location":"developer-guide/development/#local-project","text":"","title":"Local Project"},{"location":"developer-guide/development/#golang-development-environment","text":"Loggie is written in Golang, please make sure you have Golang development environment. go version","title":"Golang Development Environment"},{"location":"developer-guide/development/#code","text":"Please use your own GitHub account to fork Loggie. Then git clone it to local. git clone git@github.com:<Account>/loggie.git","title":"Code"},{"location":"developer-guide/development/#local-environment","text":"Under normal circumstances, there are no special local dependencies. For different scenarios, you need to install or use external dependencies according to specific situations. It is recommended to use Kubernetes locally for deployment and management.","title":"Local Environment"},{"location":"developer-guide/development/#kubernetes","text":"Build Kubernetes Kind is recommended. Deploy Dependent Components Helm is recommended. Related components can be added and deployed using public Helm repository. Local Loggie Connects to Kubernetes If you need to use CRDs such as LogConfig, or debug in Kubernetes, you need to enable Kubernetes Discovery in the system configuration. loggie.yml discovery : enabled : true kubernetes : kubeconfig : ${home}/.kube/config Specify kubeconfig to connect to local Kubernetes APIServer. In addition, Loggie in the form of Agent will only monitor the Kubernetes Pod events of this node. You need to add -meta.nodeName in the Loggie CMD arguments to simulate the node where it is located. For example, you can kubectl get node to view all node names: NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 2y50d v1.21.0 Add -meta.nodeName=kind-control-plane in CMD arguments, so that Loggie can be considered to be deployed on the specific node after startup.","title":"Kubernetes"},{"location":"developer-guide/development/#verify-and-debug","text":"After Loggie is started by default, the output log format is JSON. If you are not used to it, you can add -log.jsonFormat=false in CMD arguments to turn it off. Try to use dev method locally, and make reasonable use of dev source and dev sink. for example: To develop source, configure dev sink for local verification To develop sink, configure dev source to simulate input or configure file source to collect local files To develop an interceptor, configure both dev source and dev sink for simulation Example: In pipelines, use dev sink to see the final output of the data sent to downstream. pipelines.yml sink : type : \"dev\" printEvents : true codec : pretty : true","title":"Verify and Debug"},{"location":"developer-guide/release/","text":"Version and Release Process \u00b6 Project Involved \u00b6 Loggie project Loggie-installation deployment script project Loggie docs document project (Temporarily the latest version of the document. Multiple versions will be supported in the future.) Branching strategy \u00b6 Version Reference Semantic . Use the main branch as development branch, and all PRs are merged into main branch. New version uses the release branch. Format should be release-vA.B , such as release-v1.1 . Only the first two digits of the version are used here. Tag a specific version based on the release branch, for example v1.1.0 . Bugfix will definitely be merged into the main branch, and additionally cherry-pick-ed to the specific release branch according to the serious situation, and tagged at the same time. Minimum version needs to be upgraded, for example v1.1.1 . Steps of Publishing New Version \u00b6 1. Create a new release branch \u00b6 Confirm that all the features required by the version have been merged, and the main branch is the latest commit: git checkout main git pull upstream main -r Create a new release branch based on the main branch, for example release-v1.1 : git checkout -b release-v ${ A .B } 2. Fill in CHANGELOG \u00b6 Fill in CHANGELOG based on this branch and submit it to the project: git add . git commit -m 'Release: add v${A.B.C} changelog' git push upstream release-v ${ A .B.C } 3. Release Loggie-installation \u00b6 The steps of releasing Loggie-installation deployment script project is the same as the above steps 1 and 2. Note: Don't forget to modify the version number in Chart.yaml. 4. Regression test \u00b6 Perform regression test based on the release branch. After the release branch is pushed to Github, the action will be triggered to build the image. And the image will be used for regression testing. The test here will use the corresponding release branch in Loggie-installation, and regression test will verify the correctness of the deployment script. If the test finds a bug, submit a PR and merge it into the release branch, and re-run the regression test. 5. Tag the version \u00b6 After tests pass, tag the corresponding version based on the release branch, such as v1.1.0 git pull upstream release-v ${ A .B.C } git tag v ${ A .B.C } git push v ${ A .B.C } Note that both Loggie and Loggie-installation require to be tagged. 6. Publish the Release on GitHub \u00b6 Loggie has used github action to automatically build and publish release artifacts. Loggie installation needs to provide loggie-v${A.B.C}.tgz helm chart package. Based on the Loggie-installation of the corresponding version execute helm package ./helm-chart . 7. Update Loggie Docs \u00b6 Update Loggie docs and submit a PR. Modify the Loggie image and version number, etc. 8. Merge the Release Branch into Main \u00b6 Submit PR to merge the release branch into the main. You can wait the release for a period of time to be stabilized before merging. Note that both Loggie project and Loggie-installation are required. When There Is A BugFix \u00b6 If it is an unimportant bug, you can submit the modification based on the main branch. Merge into the release branch is not necessary. If it is an important bug that needs to be fixed, determine the release branch of the version that needs to be fixed (it can be one or more). In addition to submitting modification to the main branch, you also need to cherry-pick it to the specified release branch, and tag it at the same time (adding the minimum version number). Finally you need to confirm whether Loggie-installation and docs need to be modified synchronously.","title":"Release Process"},{"location":"developer-guide/release/#version-and-release-process","text":"","title":"Version and Release Process"},{"location":"developer-guide/release/#project-involved","text":"Loggie project Loggie-installation deployment script project Loggie docs document project (Temporarily the latest version of the document. Multiple versions will be supported in the future.)","title":"Project Involved"},{"location":"developer-guide/release/#branching-strategy","text":"Version Reference Semantic . Use the main branch as development branch, and all PRs are merged into main branch. New version uses the release branch. Format should be release-vA.B , such as release-v1.1 . Only the first two digits of the version are used here. Tag a specific version based on the release branch, for example v1.1.0 . Bugfix will definitely be merged into the main branch, and additionally cherry-pick-ed to the specific release branch according to the serious situation, and tagged at the same time. Minimum version needs to be upgraded, for example v1.1.1 .","title":"Branching strategy"},{"location":"developer-guide/release/#steps-of-publishing-new-version","text":"","title":"Steps of Publishing New Version"},{"location":"developer-guide/release/#1-create-a-new-release-branch","text":"Confirm that all the features required by the version have been merged, and the main branch is the latest commit: git checkout main git pull upstream main -r Create a new release branch based on the main branch, for example release-v1.1 : git checkout -b release-v ${ A .B }","title":"1. Create a new release branch"},{"location":"developer-guide/release/#2-fill-in-changelog","text":"Fill in CHANGELOG based on this branch and submit it to the project: git add . git commit -m 'Release: add v${A.B.C} changelog' git push upstream release-v ${ A .B.C }","title":"2. Fill in CHANGELOG"},{"location":"developer-guide/release/#3-release-loggie-installation","text":"The steps of releasing Loggie-installation deployment script project is the same as the above steps 1 and 2. Note: Don't forget to modify the version number in Chart.yaml.","title":"3. Release Loggie-installation"},{"location":"developer-guide/release/#4-regression-test","text":"Perform regression test based on the release branch. After the release branch is pushed to Github, the action will be triggered to build the image. And the image will be used for regression testing. The test here will use the corresponding release branch in Loggie-installation, and regression test will verify the correctness of the deployment script. If the test finds a bug, submit a PR and merge it into the release branch, and re-run the regression test.","title":"4. Regression test"},{"location":"developer-guide/release/#5-tag-the-version","text":"After tests pass, tag the corresponding version based on the release branch, such as v1.1.0 git pull upstream release-v ${ A .B.C } git tag v ${ A .B.C } git push v ${ A .B.C } Note that both Loggie and Loggie-installation require to be tagged.","title":"5. Tag the version"},{"location":"developer-guide/release/#6-publish-the-release-on-github","text":"Loggie has used github action to automatically build and publish release artifacts. Loggie installation needs to provide loggie-v${A.B.C}.tgz helm chart package. Based on the Loggie-installation of the corresponding version execute helm package ./helm-chart .","title":"6. Publish the Release on GitHub"},{"location":"developer-guide/release/#7-update-loggie-docs","text":"Update Loggie docs and submit a PR. Modify the Loggie image and version number, etc.","title":"7. Update Loggie Docs"},{"location":"developer-guide/release/#8-merge-the-release-branch-into-main","text":"Submit PR to merge the release branch into the main. You can wait the release for a period of time to be stabilized before merging. Note that both Loggie project and Loggie-installation are required.","title":"8. Merge the Release Branch into Main"},{"location":"developer-guide/release/#when-there-is-a-bugfix","text":"If it is an unimportant bug, you can submit the modification based on the main branch. Merge into the release branch is not necessary. If it is an important bug that needs to be fixed, determine the release branch of the version that needs to be fixed (it can be one or more). In addition to submitting modification to the main branch, you also need to cherry-pick it to the specified release branch, and tag it at the same time (adding the minimum version number). Finally you need to confirm whether Loggie-installation and docs need to be modified synchronously.","title":"When There Is A BugFix"},{"location":"developer-guide/code/coding-guide/","text":"Code Specification \u00b6 Developing Specification \u00b6 Common Golang Specification \u00b6 References: Effective Go Go Code Review Comments Uber Go Style Guide Loggie Specification \u00b6 Single responsibility. A component only does one thing, and does one thing well. Before starting any goroutine, you must first think about how to exit and code the exit logic. When returning err, think about whether you need to clean up goroutines that do not need to be started. Always remember that operations in go do not guarantee atomicity (such as string concatenation), except for those in chan, sync, etc. packages. When looping (for each), note that function variables share the same address. Reduce the use of global done chan (a large number of goroutines polling the same done chan will drastically reduce performance). It is more recommended to use the independent done chan of the component struct and provide the stop method for the upper layer to call to avoid goroutine leakage. Component should be stopped reasonably and gracefully (especially not to panic and cause the entire process to hang up). Do not close a chan used for reading and writing to exit the goroutine or stop the Component. There may be a panic if the closed chan is written. Use a separate done chan to indicate whether to quit. When stopping Component, pay attention to the order of closing and the release of resources, and do not cause other goroutines blocked (for example, the no-buffer chan for external writing is not processed, but the reading goroutine exits, causing the external writing goroutine blocked all the time) Encapsulate the life cycle of chan inside the chan owner. When using done chan to exit the goroutine, do not do anything to clean up resources. Defer is preferred for cleaning up resources, because defer can also clean up resources when goroutine panics, and cleanup logic in done chan may never be executed. time.after() is forbidden. Use time.NewTicker() and clear up ticker in time. Log \u00b6 Logs should be taken seriously. When making changes, take the time to log to make sure important things are recorded. Especially the key information of components such as pipeline name and source name. Log statements should be complete sentences, appropriately capitalized, for reading by people who are not familiar with the source code. Uses the functions preset by the project, such as log.Info() , log.Error() to print log. Go built-in fmt methods such as fmt.Printf() , fmt.Println() are not allowed. The project log system adopts zerolog . Log Level \u00b6 DEBUG: Status and hint information for observation during development. It needs to be shielded after the development is completed or version is officially released. However, when an abnormality that is difficult to troubleshoot occurs, it can provide more detailed internal information. INFO: Some suggestive information that has value to be retained even in a system that has been developed and officially launched. WARN: Slight warning, which indicates abnormal situations in the program but does not affect normal operation. ERROR: Common error, which is under control and will not cause cascading effects or huge effects. PANIC/FATAL: Fatal error, which will cause the system to crash and shut down and must be dealt with immediately. Generally used in the main function starting process, and should used with caution during normal operation. Please note that Loggie's multiple Pipelines are designed to be isolated from each other, and FATAL will affect all Pipelines. Difference between PANIC and FATAL PANIC finally calls panic(msg). FATAL finally calls os.Exit(1). Monitoring \u00b6 Any new components should come with appropriate metrics for monitoring to work properly. These metrics should be taken seriously. Only report useful metrics that will be used in production to monitor/alert the health of the system, or to troubleshoot issues. Unit Test \u00b6 New features need to include unit tests. Unit tests should test as little code as possible. Do not start the entire service. If there are external dependencies, please add e2e or integration tests Code Style \u00b6 go fmt Configuration item names use camel case. For example, use yaml:\"cleanDataTimeout,omitempty\" instead of yaml:\"clean_data_timeout,omitempty\" . Backward Bompatibility \u00b6 The protocol should support backward compatibility for no-downtime upgrades, which means that the same source and sink must be able to support requests of both new and old versions. The metadata format and data format should support backward compatibility, such as the protobuf definition of grcp. Copyright profile \u00b6 Copyright-text /* Copyright 2022 Loggie Authors Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */","title":"Specification"},{"location":"developer-guide/code/coding-guide/#code-specification","text":"","title":"Code Specification"},{"location":"developer-guide/code/coding-guide/#developing-specification","text":"","title":"Developing Specification"},{"location":"developer-guide/code/coding-guide/#common-golang-specification","text":"References: Effective Go Go Code Review Comments Uber Go Style Guide","title":"Common Golang Specification"},{"location":"developer-guide/code/coding-guide/#loggie-specification","text":"Single responsibility. A component only does one thing, and does one thing well. Before starting any goroutine, you must first think about how to exit and code the exit logic. When returning err, think about whether you need to clean up goroutines that do not need to be started. Always remember that operations in go do not guarantee atomicity (such as string concatenation), except for those in chan, sync, etc. packages. When looping (for each), note that function variables share the same address. Reduce the use of global done chan (a large number of goroutines polling the same done chan will drastically reduce performance). It is more recommended to use the independent done chan of the component struct and provide the stop method for the upper layer to call to avoid goroutine leakage. Component should be stopped reasonably and gracefully (especially not to panic and cause the entire process to hang up). Do not close a chan used for reading and writing to exit the goroutine or stop the Component. There may be a panic if the closed chan is written. Use a separate done chan to indicate whether to quit. When stopping Component, pay attention to the order of closing and the release of resources, and do not cause other goroutines blocked (for example, the no-buffer chan for external writing is not processed, but the reading goroutine exits, causing the external writing goroutine blocked all the time) Encapsulate the life cycle of chan inside the chan owner. When using done chan to exit the goroutine, do not do anything to clean up resources. Defer is preferred for cleaning up resources, because defer can also clean up resources when goroutine panics, and cleanup logic in done chan may never be executed. time.after() is forbidden. Use time.NewTicker() and clear up ticker in time.","title":"Loggie Specification"},{"location":"developer-guide/code/coding-guide/#log","text":"Logs should be taken seriously. When making changes, take the time to log to make sure important things are recorded. Especially the key information of components such as pipeline name and source name. Log statements should be complete sentences, appropriately capitalized, for reading by people who are not familiar with the source code. Uses the functions preset by the project, such as log.Info() , log.Error() to print log. Go built-in fmt methods such as fmt.Printf() , fmt.Println() are not allowed. The project log system adopts zerolog .","title":"Log"},{"location":"developer-guide/code/coding-guide/#log-level","text":"DEBUG: Status and hint information for observation during development. It needs to be shielded after the development is completed or version is officially released. However, when an abnormality that is difficult to troubleshoot occurs, it can provide more detailed internal information. INFO: Some suggestive information that has value to be retained even in a system that has been developed and officially launched. WARN: Slight warning, which indicates abnormal situations in the program but does not affect normal operation. ERROR: Common error, which is under control and will not cause cascading effects or huge effects. PANIC/FATAL: Fatal error, which will cause the system to crash and shut down and must be dealt with immediately. Generally used in the main function starting process, and should used with caution during normal operation. Please note that Loggie's multiple Pipelines are designed to be isolated from each other, and FATAL will affect all Pipelines. Difference between PANIC and FATAL PANIC finally calls panic(msg). FATAL finally calls os.Exit(1).","title":"Log Level"},{"location":"developer-guide/code/coding-guide/#monitoring","text":"Any new components should come with appropriate metrics for monitoring to work properly. These metrics should be taken seriously. Only report useful metrics that will be used in production to monitor/alert the health of the system, or to troubleshoot issues.","title":"Monitoring"},{"location":"developer-guide/code/coding-guide/#unit-test","text":"New features need to include unit tests. Unit tests should test as little code as possible. Do not start the entire service. If there are external dependencies, please add e2e or integration tests","title":"Unit Test"},{"location":"developer-guide/code/coding-guide/#code-style","text":"go fmt Configuration item names use camel case. For example, use yaml:\"cleanDataTimeout,omitempty\" instead of yaml:\"clean_data_timeout,omitempty\" .","title":"Code Style"},{"location":"developer-guide/code/coding-guide/#backward-bompatibility","text":"The protocol should support backward compatibility for no-downtime upgrades, which means that the same source and sink must be able to support requests of both new and old versions. The metadata format and data format should support backward compatibility, such as the protobuf definition of grcp.","title":"Backward Bompatibility"},{"location":"developer-guide/code/coding-guide/#copyright-profile","text":"Copyright-text /* Copyright 2022 Loggie Authors Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */","title":"Copyright profile"},{"location":"developer-guide/component/component-guide/","text":"It is recommended to refer to existing components to develop a new component. public interface \u00b6 // component lifecycle interface type Lifecycle interface { // Initialize component, such as initializing the Kafka connection Init ( context Context ) // Start component. For example, start consuming Kafka Start () // Stop component. Stop () } // component description interface type Describable interface { // For example, source. Category () Category // For example, kafka. Type () Type // Customized description String () string } // interface for getting configuration type Config interface { // to get configuration Config () interface {} } // Component type Component interface { // component lifecycle interface Lifecycle // component description interface Describable // interface for getting configuration Config } Source Component \u00b6 The source component is connected to the data source input. To develop a new source plug-in, you need to implement the following interfaces. // source component interface type Source interface { Component Producer // Confirm that the sink is successful and then submit. Commit ( events [] Event ) } // Source component needs to implement this producer interface. type Producer interface { // docking data source ProductLoop ( productFunc ProductFunc ) } Sink Component \u00b6 The sink component is connected to the output end. To develop a new sink plug-in, you need to implement the following interfaces. // sink component interface type Sink interface { Component Consumer } // Sink component needs to implement this consumer interface. type Consumer interface { // docking output Consume ( batch Batch ) Result } Interceptor Component \u00b6 The interceptor component intercepts events. To develop a new interceptor plug-in, you need to implement the following interfaces. // interceptor component interface type Interceptor interface { Component // Intercept processing Intercept ( invoker Invoker , invocation Invocation ) api . Result } Note Please note that the newly added components need to be registered in the import of pkg/include/include.go.","title":"Component Development"},{"location":"developer-guide/component/component-guide/#public-interface","text":"// component lifecycle interface type Lifecycle interface { // Initialize component, such as initializing the Kafka connection Init ( context Context ) // Start component. For example, start consuming Kafka Start () // Stop component. Stop () } // component description interface type Describable interface { // For example, source. Category () Category // For example, kafka. Type () Type // Customized description String () string } // interface for getting configuration type Config interface { // to get configuration Config () interface {} } // Component type Component interface { // component lifecycle interface Lifecycle // component description interface Describable // interface for getting configuration Config }","title":"public interface"},{"location":"developer-guide/component/component-guide/#source-component","text":"The source component is connected to the data source input. To develop a new source plug-in, you need to implement the following interfaces. // source component interface type Source interface { Component Producer // Confirm that the sink is successful and then submit. Commit ( events [] Event ) } // Source component needs to implement this producer interface. type Producer interface { // docking data source ProductLoop ( productFunc ProductFunc ) }","title":"Source Component"},{"location":"developer-guide/component/component-guide/#sink-component","text":"The sink component is connected to the output end. To develop a new sink plug-in, you need to implement the following interfaces. // sink component interface type Sink interface { Component Consumer } // Sink component needs to implement this consumer interface. type Consumer interface { // docking output Consume ( batch Batch ) Result }","title":"Sink Component"},{"location":"developer-guide/component/component-guide/#interceptor-component","text":"The interceptor component intercepts events. To develop a new interceptor plug-in, you need to implement the following interfaces. // interceptor component interface type Interceptor interface { Component // Intercept processing Intercept ( invoker Invoker , invocation Invocation ) api . Result } Note Please note that the newly added components need to be registered in the import of pkg/include/include.go.","title":"Interceptor Component"},{"location":"getting-started/overview/","text":"Loggie Roaming Guide \u00b6 Welcome to the world of Loggie. Learning \u00b6 Wondering what Loggie is? Learn about Core Concepts . If you want to experience Loggie at once, please refer to Quick Start . Interested in the reasons we developed Loggie, the advantages of Loggie, and the comparison with similar projects, Please click here . Using \u00b6 Want to give Loggie a try but don't know where to start? Maybe first think about it as a whole: how to collect business logs, and how to build a log structure that is perfect and suitable for actual needs? Here is a reference . Then, you can choose a suitable business scenario , and view Loggie's best practices in Kubernetes. This also introduces how to deploy Loggie according to the actual situation. In addition, to see how specific components are used, please refer to Configuration . Wondering \u00b6 Having trouble using Loggie? Please raise issues or contact us. Scan the WeChat code to join Loggie discussion group: (The old WeChat group is full, please scan the code again if you fail) Participating \u00b6 If you are interested in the specific implementation of Loggie and want to participate in the development of Loggie. If you want to develop your own plugin. Please refer to Development Manual .","title":"Overview"},{"location":"getting-started/overview/#loggie-roaming-guide","text":"Welcome to the world of Loggie.","title":"Loggie Roaming Guide"},{"location":"getting-started/overview/#learning","text":"Wondering what Loggie is? Learn about Core Concepts . If you want to experience Loggie at once, please refer to Quick Start . Interested in the reasons we developed Loggie, the advantages of Loggie, and the comparison with similar projects, Please click here .","title":"Learning"},{"location":"getting-started/overview/#using","text":"Want to give Loggie a try but don't know where to start? Maybe first think about it as a whole: how to collect business logs, and how to build a log structure that is perfect and suitable for actual needs? Here is a reference . Then, you can choose a suitable business scenario , and view Loggie's best practices in Kubernetes. This also introduces how to deploy Loggie according to the actual situation. In addition, to see how specific components are used, please refer to Configuration .","title":"Using"},{"location":"getting-started/overview/#wondering","text":"Having trouble using Loggie? Please raise issues or contact us. Scan the WeChat code to join Loggie discussion group: (The old WeChat group is full, please scan the code again if you fail)","title":"Wondering"},{"location":"getting-started/overview/#participating","text":"If you are interested in the specific implementation of Loggie and want to participate in the development of Loggie. If you want to develop your own plugin. Please refer to Development Manual .","title":"Participating"},{"location":"getting-started/install/kubernetes/","text":"Deployment in Kubernetes \u00b6 Deploy Loggie DaemonSet \u00b6 Make sure you have kubectl and helm executable locally. kubectl ( download ) helm ( download ) Download helm-chart \u00b6 VERSION = v1.3.0 helm pull https://github.com/loggie-io/installation/releases/download/ ${ VERSION } /loggie- ${ VERSION } .tgz && tar xvzf loggie- ${ VERSION } .tgz Please replace <VERSION> above with the specific version number such as v1.3.0, which can be found release tag . Modify Configuration \u00b6 cd into chart directory: cd installation/helm-chart Check values.yml, and modify it as you like. Following are the currently configurable parameters: Image \u00b6 image : loggieio/loggie:main loggie image. All images are available on docker hub . Resource \u00b6 resources : limits : cpu : 2 memory : 2Gi requests : cpu : 100m memory : 100Mi The limit/request resource of Loggie Agent can be modified according to the actual condition. Additional CMD Arguments \u00b6 extraArgs : {} The additional CMD arguments of Loggie. For example, if you want to use the debug log level, and do not want to use the json format for log, it can be modified as: extraArgs : log.level : debug log.jsonFormat : false Extra Mount \u00b6 extraVolumeMounts : - mountPath : /var/log/pods name : podlogs - mountPath : /var/lib/kubelet/pods name : kubelet - mountPath : /var/lib/docker name : docker extraVolumes : - hostPath : path : /var/log/pods type : DirectoryOrCreate name : podlogs - hostPath : path : /var/lib/kubelet/pods type : DirectoryOrCreate name : kubelet - hostPath : path : /var/lib/docker type : DirectoryOrCreate name : docker It is recommended to mount the above directories according to the actual condition. Because Loggie itself is also deployed in a container, Loggie also needs to mount some volumes of nodes to collect logs. Otherwise, log files cannot be seen inside the Loggie container, and cannot be collected. Here is a brief list of what paths need to be mounted when loggie collect different kinds of log: Collect stdout : Loggie collects from /var/log/pods, so Loggie needs to mount: volumeMounts : - mountPath : /var/log/pods name : podlogs - mountPath : /var/lib/docker name : docker volumes : - hostPath : path : /var/log/pods type : DirectoryOrCreate name : podlogs - hostPath : path : /var/lib/docker type : DirectoryOrCreate name : docker But it is possible that log files under /var/log/pods will be soft-linked to the root path of docker. The default is /var/lib/docker . At this time, /var/lib/docker needs to be mounted as well. If other runtime is used, such as containerd, there is no need to mount /var/lib/docker , Loggie will look for the actual standard output path from /var/log/pods . Collect the logs mounted by the service Pod using HostPath : For example, if the business pods uniformly mount the logs to the /data/logs path of the node, you need to mount the path: volumeMounts : - mountPath : /data/logs name : logs volumes : - hostPath : path : /data/logs type : DirectoryOrCreate name : logs Collect the logs mounted by the business Pod using EmptyDir : By default, emtpyDir will be in the /var/lib/kubelet/pods path of the node, so Loggie needs to mount this path. If configuration of kubelet is modified, it needs to be modified synchronously: volumeMounts : - mountPath : /var/lib/kubelet/pods name : kubelet volumes : - hostPath : path : /var/lib/kubelet/pods type : DirectoryOrCreate name : kubelet Collect the logs mounted by the service Pod using PV : Same as using EmptyDir. No mount and rootFsCollectionEnabled: true : Loggie will automatically find the actual path in the container from the rootfs of the docker, and the root path of the docker needs to be mounted at this time: volumeMounts : - mountPath : /var/lib/docker name : docker volumes : - hostPath : path : /var/lib/docker type : DirectoryOrCreate name : docker If the actual root path of docker is modified, the volumeMount and volume here need to be modified synchronously. For example, if the root path is modified to /data/docker , the mount is as follows: volumeMounts : - mountPath : /data/docker name : docker volumes : - hostPath : path : /data/docker type : DirectoryOrCreate name : docker Note: Loggie needs to record the status of the collected files (offset, etc.) to avoid collecting files from the beginning after restarting. The default mounting path is /data/logie.db, so the /data/loggie--{{ template \"loggie.name\" . }} directory is mounted. Schedule \u00b6 nodeSelector : {} affinity : {} # podAntiAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # - labelSelector: # matchExpressions: # - key: app # operator: In # values: # - loggie # topologyKey: \"kubernetes.io/hostname\" You can use nodeSelector and affinity to control the scheduling of Loggie Pods. For details, please refer to the Kubernetes documentation. tolerations : [] # - effect: NoExecute # operator: Exists # - effect: NoSchedule # operator: Exists If a node has its own taints, the Loggie Pod cannot be scheduled to the node. If you need to ignore the taints, you can add the corresponding tolerations. Updating Strategy \u00b6 updateStrategy : type : RollingUpdate Can be RollingUpdate or OnDelete . Global Configuration \u00b6 config : loggie : reload : enabled : true period : 10s monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~ discovery : enabled : true kubernetes : containerRuntime : containerd fields : container.name : containername logConfig : logconfig namespace : namespace node.name : nodename pod.name : podname http : enabled : true port : 9196 For detailed description, please refer to Configuration . It should be noted that if you use tools such as Kind to deploy Kubernetes locally, Kind will use the containerd runtime by default. In this case, you need to add containerRuntime: containerd to specify the containerd runtime. Service \u00b6 If Loggie wants to receive data sent by other services, it needs to expose its own services through service. Under normal circumstances, Loggie in Agent mode only needs to expose its own management port. servicePorts : - name : monitor port : 9196 targetPort : 9196 Deploy \u00b6 For the initial deployment, we specify that the deployment is under the loggie namespace, and let helm automatically create the namespace. helm install loggie ./ -nloggie --create-namespace If loggie namespace has been created in your environment, you can ignore -nloggie and --create-namespace . Of course, you can use your own namespace. Kubernetes version issue failed to install CRD crds/crds.yaml: unable to recognize \"\": no matches for kind \"CustomResourceDefinition\" in version \"apiextensions.k8s.io/v1\" If you have a similar problem during helm install, it means that your Kubernetes version is too low and does not support the apiextensions.k8s.io/v1 version CRD. Loggie temporarily retains the CRD of the v1beta1 version, please delete the v1beta1 version in the charts, rm loggie/crds/crds.yaml and reinstall it. Check deployment status \u00b6 After execution, use the helm command to check the deployment status: helm list -nloggie Result should be like: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION loggie loggie 1 2021-11-30 18:06:16.976334232 +0800 CST deployed loggie-v0.1.0 v0.1.0 At the same time, you can also use the kubectl command to check whether the Pod has been created. kubectl -nloggie get po Result should be like: loggie-sxxwh 1/1 Running 0 5m21s 10.244.0.5 kind-control-plane <none> <none> Deploy Loggie Aggregator \u00b6 Deploying Aggregator is basically the same as deploying Agent. In Helm chart we provide aggregator config . Modify as enabled: true . StatefulSet method is provided in the helm chart, and you can also modify it to deployment and other methods according to your needs. At the same time, you can add content in values.yaml according to the cases: nodeSelector or affinity. tolerations according to whether the node has taint. Make the Aggregator StatefulSet scheduled only on certain nodes. add port for service to receive data. For example, to use Grpc source, default 6066 needs to be specified. servicePorts : - name : grpc port : 6066 targetPort : 6066 add cluster field in discovery.kubernetes , which indicates the name of the aggregator cluster, which is used to distinguish Agent or other Loggie clusters, as shown below: config : loggie : discovery : enabled : true kubernetes : cluster : aggregator Command reference: helm install loggie-aggregator ./ -nloggie-aggregator --create-namespace Note The Loggie aggregator can also be deployed using Deployment or StatefulSet. Please refer to DaemonSet to modify the helm chart by yourself.","title":"Kubernetes"},{"location":"getting-started/install/kubernetes/#deployment-in-kubernetes","text":"","title":"Deployment in Kubernetes"},{"location":"getting-started/install/kubernetes/#deploy-loggie-daemonset","text":"Make sure you have kubectl and helm executable locally. kubectl ( download ) helm ( download )","title":"Deploy Loggie DaemonSet"},{"location":"getting-started/install/kubernetes/#download-helm-chart","text":"VERSION = v1.3.0 helm pull https://github.com/loggie-io/installation/releases/download/ ${ VERSION } /loggie- ${ VERSION } .tgz && tar xvzf loggie- ${ VERSION } .tgz Please replace <VERSION> above with the specific version number such as v1.3.0, which can be found release tag .","title":"Download helm-chart"},{"location":"getting-started/install/kubernetes/#modify-configuration","text":"cd into chart directory: cd installation/helm-chart Check values.yml, and modify it as you like. Following are the currently configurable parameters:","title":"Modify Configuration"},{"location":"getting-started/install/kubernetes/#image","text":"image : loggieio/loggie:main loggie image. All images are available on docker hub .","title":"Image"},{"location":"getting-started/install/kubernetes/#resource","text":"resources : limits : cpu : 2 memory : 2Gi requests : cpu : 100m memory : 100Mi The limit/request resource of Loggie Agent can be modified according to the actual condition.","title":"Resource"},{"location":"getting-started/install/kubernetes/#additional-cmd-arguments","text":"extraArgs : {} The additional CMD arguments of Loggie. For example, if you want to use the debug log level, and do not want to use the json format for log, it can be modified as: extraArgs : log.level : debug log.jsonFormat : false","title":"Additional CMD Arguments"},{"location":"getting-started/install/kubernetes/#extra-mount","text":"extraVolumeMounts : - mountPath : /var/log/pods name : podlogs - mountPath : /var/lib/kubelet/pods name : kubelet - mountPath : /var/lib/docker name : docker extraVolumes : - hostPath : path : /var/log/pods type : DirectoryOrCreate name : podlogs - hostPath : path : /var/lib/kubelet/pods type : DirectoryOrCreate name : kubelet - hostPath : path : /var/lib/docker type : DirectoryOrCreate name : docker It is recommended to mount the above directories according to the actual condition. Because Loggie itself is also deployed in a container, Loggie also needs to mount some volumes of nodes to collect logs. Otherwise, log files cannot be seen inside the Loggie container, and cannot be collected. Here is a brief list of what paths need to be mounted when loggie collect different kinds of log: Collect stdout : Loggie collects from /var/log/pods, so Loggie needs to mount: volumeMounts : - mountPath : /var/log/pods name : podlogs - mountPath : /var/lib/docker name : docker volumes : - hostPath : path : /var/log/pods type : DirectoryOrCreate name : podlogs - hostPath : path : /var/lib/docker type : DirectoryOrCreate name : docker But it is possible that log files under /var/log/pods will be soft-linked to the root path of docker. The default is /var/lib/docker . At this time, /var/lib/docker needs to be mounted as well. If other runtime is used, such as containerd, there is no need to mount /var/lib/docker , Loggie will look for the actual standard output path from /var/log/pods . Collect the logs mounted by the service Pod using HostPath : For example, if the business pods uniformly mount the logs to the /data/logs path of the node, you need to mount the path: volumeMounts : - mountPath : /data/logs name : logs volumes : - hostPath : path : /data/logs type : DirectoryOrCreate name : logs Collect the logs mounted by the business Pod using EmptyDir : By default, emtpyDir will be in the /var/lib/kubelet/pods path of the node, so Loggie needs to mount this path. If configuration of kubelet is modified, it needs to be modified synchronously: volumeMounts : - mountPath : /var/lib/kubelet/pods name : kubelet volumes : - hostPath : path : /var/lib/kubelet/pods type : DirectoryOrCreate name : kubelet Collect the logs mounted by the service Pod using PV : Same as using EmptyDir. No mount and rootFsCollectionEnabled: true : Loggie will automatically find the actual path in the container from the rootfs of the docker, and the root path of the docker needs to be mounted at this time: volumeMounts : - mountPath : /var/lib/docker name : docker volumes : - hostPath : path : /var/lib/docker type : DirectoryOrCreate name : docker If the actual root path of docker is modified, the volumeMount and volume here need to be modified synchronously. For example, if the root path is modified to /data/docker , the mount is as follows: volumeMounts : - mountPath : /data/docker name : docker volumes : - hostPath : path : /data/docker type : DirectoryOrCreate name : docker Note: Loggie needs to record the status of the collected files (offset, etc.) to avoid collecting files from the beginning after restarting. The default mounting path is /data/logie.db, so the /data/loggie--{{ template \"loggie.name\" . }} directory is mounted.","title":"Extra Mount"},{"location":"getting-started/install/kubernetes/#schedule","text":"nodeSelector : {} affinity : {} # podAntiAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # - labelSelector: # matchExpressions: # - key: app # operator: In # values: # - loggie # topologyKey: \"kubernetes.io/hostname\" You can use nodeSelector and affinity to control the scheduling of Loggie Pods. For details, please refer to the Kubernetes documentation. tolerations : [] # - effect: NoExecute # operator: Exists # - effect: NoSchedule # operator: Exists If a node has its own taints, the Loggie Pod cannot be scheduled to the node. If you need to ignore the taints, you can add the corresponding tolerations.","title":"Schedule"},{"location":"getting-started/install/kubernetes/#updating-strategy","text":"updateStrategy : type : RollingUpdate Can be RollingUpdate or OnDelete .","title":"Updating Strategy"},{"location":"getting-started/install/kubernetes/#global-configuration","text":"config : loggie : reload : enabled : true period : 10s monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~ discovery : enabled : true kubernetes : containerRuntime : containerd fields : container.name : containername logConfig : logconfig namespace : namespace node.name : nodename pod.name : podname http : enabled : true port : 9196 For detailed description, please refer to Configuration . It should be noted that if you use tools such as Kind to deploy Kubernetes locally, Kind will use the containerd runtime by default. In this case, you need to add containerRuntime: containerd to specify the containerd runtime.","title":"Global Configuration"},{"location":"getting-started/install/kubernetes/#service","text":"If Loggie wants to receive data sent by other services, it needs to expose its own services through service. Under normal circumstances, Loggie in Agent mode only needs to expose its own management port. servicePorts : - name : monitor port : 9196 targetPort : 9196","title":"Service"},{"location":"getting-started/install/kubernetes/#deploy","text":"For the initial deployment, we specify that the deployment is under the loggie namespace, and let helm automatically create the namespace. helm install loggie ./ -nloggie --create-namespace If loggie namespace has been created in your environment, you can ignore -nloggie and --create-namespace . Of course, you can use your own namespace. Kubernetes version issue failed to install CRD crds/crds.yaml: unable to recognize \"\": no matches for kind \"CustomResourceDefinition\" in version \"apiextensions.k8s.io/v1\" If you have a similar problem during helm install, it means that your Kubernetes version is too low and does not support the apiextensions.k8s.io/v1 version CRD. Loggie temporarily retains the CRD of the v1beta1 version, please delete the v1beta1 version in the charts, rm loggie/crds/crds.yaml and reinstall it.","title":"Deploy"},{"location":"getting-started/install/kubernetes/#check-deployment-status","text":"After execution, use the helm command to check the deployment status: helm list -nloggie Result should be like: NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION loggie loggie 1 2021-11-30 18:06:16.976334232 +0800 CST deployed loggie-v0.1.0 v0.1.0 At the same time, you can also use the kubectl command to check whether the Pod has been created. kubectl -nloggie get po Result should be like: loggie-sxxwh 1/1 Running 0 5m21s 10.244.0.5 kind-control-plane <none> <none>","title":"Check deployment status"},{"location":"getting-started/install/kubernetes/#deploy-loggie-aggregator","text":"Deploying Aggregator is basically the same as deploying Agent. In Helm chart we provide aggregator config . Modify as enabled: true . StatefulSet method is provided in the helm chart, and you can also modify it to deployment and other methods according to your needs. At the same time, you can add content in values.yaml according to the cases: nodeSelector or affinity. tolerations according to whether the node has taint. Make the Aggregator StatefulSet scheduled only on certain nodes. add port for service to receive data. For example, to use Grpc source, default 6066 needs to be specified. servicePorts : - name : grpc port : 6066 targetPort : 6066 add cluster field in discovery.kubernetes , which indicates the name of the aggregator cluster, which is used to distinguish Agent or other Loggie clusters, as shown below: config : loggie : discovery : enabled : true kubernetes : cluster : aggregator Command reference: helm install loggie-aggregator ./ -nloggie-aggregator --create-namespace Note The Loggie aggregator can also be deployed using Deployment or StatefulSet. Please refer to DaemonSet to modify the helm chart by yourself.","title":"Deploy Loggie Aggregator"},{"location":"getting-started/install/node/","text":"Host deployment \u00b6 Loggie uses Golang to be compiled into binary, and can be deployed to various systems according to needs. Here we provide a reference for deploying Loggie using systemd. Pre-check \u00b6 Operating System: Linux System Architecture: amd64 Distribution supports systemd The current release only contains binary executables generated by GOOS=linux GOARCH=amd64. For other systems and architectures, please cross-compile based on the source code. Download Binary \u00b6 VERSION=v1.3.0 mkdir /opt/loggie && curl https://github.com/loggie-io/loggie/releases/download/${VERSION}/loggie-linux-amd64 -o /opt/loggie/loggie && chmod +x /opt/loggie/loggie Please replace <VERSION> above with the specific version number. Add Configuration File \u00b6 Please create the configuration according to the actual needs, the following is for reference: loggie.yml \u00b6 loggie.yml cat << EOF > /opt/loggie/loggie.yml loggie : monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~ reload : enabled : true period : 10s http : enabled : true port : 9196 EOF pipelines.yml \u00b6 pipelines.yml cat << EOF > /opt/loggie/pipelines.yml pipelines : - name : local sources : - type : file name : demo paths : - /tmp/log/*.log sink : type : dev printEvents : true codec : pretty : true EOF Add systemd Configuration \u00b6 cat << EOF > /lib/systemd/system/loggie.service [Unit] Description=Loggie Documentation=https://loggie-io.github.io/docs/getting-started/install/node/ [Service] MemoryMax=200M ExecStart=/opt/loggie/loggie -config.system=/opt/loggie/loggie.yml -config.pipeline=/opt/loggie/pipelines.yml Restart=always [Install] WantedBy=multi-user.target EOF Start up \u00b6 Make configuration take effect systemctl daemon-reload Then set it to start on boot: systemctl enable loggie Then you can start Loggie: systemctl start loggie After Loggie is started, you can check status at any time: systemctl status loggie","title":"Host"},{"location":"getting-started/install/node/#host-deployment","text":"Loggie uses Golang to be compiled into binary, and can be deployed to various systems according to needs. Here we provide a reference for deploying Loggie using systemd.","title":"Host deployment"},{"location":"getting-started/install/node/#pre-check","text":"Operating System: Linux System Architecture: amd64 Distribution supports systemd The current release only contains binary executables generated by GOOS=linux GOARCH=amd64. For other systems and architectures, please cross-compile based on the source code.","title":"Pre-check"},{"location":"getting-started/install/node/#download-binary","text":"VERSION=v1.3.0 mkdir /opt/loggie && curl https://github.com/loggie-io/loggie/releases/download/${VERSION}/loggie-linux-amd64 -o /opt/loggie/loggie && chmod +x /opt/loggie/loggie Please replace <VERSION> above with the specific version number.","title":"Download Binary"},{"location":"getting-started/install/node/#add-configuration-file","text":"Please create the configuration according to the actual needs, the following is for reference:","title":"Add Configuration File"},{"location":"getting-started/install/node/#loggieyml","text":"loggie.yml cat << EOF > /opt/loggie/loggie.yml loggie : monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~ reload : enabled : true period : 10s http : enabled : true port : 9196 EOF","title":"loggie.yml"},{"location":"getting-started/install/node/#pipelinesyml","text":"pipelines.yml cat << EOF > /opt/loggie/pipelines.yml pipelines : - name : local sources : - type : file name : demo paths : - /tmp/log/*.log sink : type : dev printEvents : true codec : pretty : true EOF","title":"pipelines.yml"},{"location":"getting-started/install/node/#add-systemd-configuration","text":"cat << EOF > /lib/systemd/system/loggie.service [Unit] Description=Loggie Documentation=https://loggie-io.github.io/docs/getting-started/install/node/ [Service] MemoryMax=200M ExecStart=/opt/loggie/loggie -config.system=/opt/loggie/loggie.yml -config.pipeline=/opt/loggie/pipelines.yml Restart=always [Install] WantedBy=multi-user.target EOF","title":"Add systemd Configuration"},{"location":"getting-started/install/node/#start-up","text":"Make configuration take effect systemctl daemon-reload Then set it to start on boot: systemctl enable loggie Then you can start Loggie: systemctl start loggie After Loggie is started, you can check status at any time: systemctl status loggie","title":"Start up"},{"location":"getting-started/intro/core-concept/","text":"Core Concepts \u00b6 Loggie is a lightweight, high-performance, cloud-native log collection agent and middleware processing aggregator based on Golang. It supports multiple pipelines and hot-swappable components, and provides: One-stack log solution \uff1asupports log transfer, filtering, parsing, segmentation, log alarming, etc. Cloud-native log collection \uff1afast and convenient container log collection, dynamic configuration delivery in native Kubernetes. Production-level features \uff1aLoggie has absorbed our long-term large-scale operation and maintenance experience, and has formed full-range observability, rapid troubleshooting, abnormality warning, and automated operation and maintenance. Architecture \u00b6 Concept \u00b6 Core Data Flow \u00b6 Source : Input source, representing a specific input source. A pipeline can have multiple different input sources. For example, the file source represents the log file collection source, and the Kafka source represents the Kafka reading source. Sink : Output source, representing a specific output source. A Pipeline can have one type of output source, but there can be multiple parallel instances. For example, elasticsearch sink indicates that log data will be sent to the remote elasticsearch server. Interceptor : Interceptor, representing a log data processing component. Different interceptors have their own functions like log parsing, segmentation, conversion, current limiting, etc. A Pipeline can have multiple interceptors, and the data flow is processed through multiple Interceptors in a chain. Queue : Queue. Currently memory queue is implemented. Pipeline : Pipeline. Source/interceptor/queue/sink together form a Pipeline. Different Pipelines are isolated. Management and Control \u00b6 Discovery : The distribution of dynamic configuration, currently mainly for configuration in Kubernetes. By creating CRD instances such as LogConfig, container logs can be collected. Various types of host configuration centers will be supported successively. Monitor EventBus : Each component can publish data to eventBus topic, and a specific listener monitors the Topic and consumes it. Eventbus is mainly used to expose or transmit monitoring data. Reloader : For dynamic updates of configuration.","title":"Core Concepts"},{"location":"getting-started/intro/core-concept/#core-concepts","text":"Loggie is a lightweight, high-performance, cloud-native log collection agent and middleware processing aggregator based on Golang. It supports multiple pipelines and hot-swappable components, and provides: One-stack log solution \uff1asupports log transfer, filtering, parsing, segmentation, log alarming, etc. Cloud-native log collection \uff1afast and convenient container log collection, dynamic configuration delivery in native Kubernetes. Production-level features \uff1aLoggie has absorbed our long-term large-scale operation and maintenance experience, and has formed full-range observability, rapid troubleshooting, abnormality warning, and automated operation and maintenance.","title":"Core Concepts"},{"location":"getting-started/intro/core-concept/#architecture","text":"","title":"Architecture"},{"location":"getting-started/intro/core-concept/#concept","text":"","title":"Concept"},{"location":"getting-started/intro/core-concept/#core-data-flow","text":"Source : Input source, representing a specific input source. A pipeline can have multiple different input sources. For example, the file source represents the log file collection source, and the Kafka source represents the Kafka reading source. Sink : Output source, representing a specific output source. A Pipeline can have one type of output source, but there can be multiple parallel instances. For example, elasticsearch sink indicates that log data will be sent to the remote elasticsearch server. Interceptor : Interceptor, representing a log data processing component. Different interceptors have their own functions like log parsing, segmentation, conversion, current limiting, etc. A Pipeline can have multiple interceptors, and the data flow is processed through multiple Interceptors in a chain. Queue : Queue. Currently memory queue is implemented. Pipeline : Pipeline. Source/interceptor/queue/sink together form a Pipeline. Different Pipelines are isolated.","title":"Core Data Flow"},{"location":"getting-started/intro/core-concept/#management-and-control","text":"Discovery : The distribution of dynamic configuration, currently mainly for configuration in Kubernetes. By creating CRD instances such as LogConfig, container logs can be collected. Various types of host configuration centers will be supported successively. Monitor EventBus : Each component can publish data to eventBus topic, and a specific listener monitors the Topic and consumes it. Eventbus is mainly used to expose or transmit monitoring data. Reloader : For dynamic updates of configuration.","title":"Management and Control"},{"location":"getting-started/quick-start/kubernetes/","text":"Quick Start: Collecting Pod Logs in Kubernetes \u00b6 The following article will show you how to quickly collect Pod logs by creating a LogConfig CRD in a Kubernetes cluster. 1. Prepare Kubernetes \u00b6 You can use an existing Kubernetes cluster, or deploy Kubernetes . It is recommended to use Kind to build a Kubernetes cluster locally. The operations need to be used locally: kubectl ( download ) helm ( download ) Make sure you have kubectl and helm executable locally. 2. Deploy Loggie DaemonSet \u00b6 You can view all released deployment charts on the installation page. You can choose: Download the chart and deploy it \u00b6 VERSION = v1.3.0 helm pull https://github.com/loggie-io/installation/releases/download/ ${ VERSION } /loggie- ${ VERSION } .tgz && tar xvzf loggie- ${ VERSION } .tgz Try to modify values.yaml in it. Please replace the <VERSION> above with the specific version number. Deploy: helm install loggie ./loggie -nloggie --create-namespace You can also: Deploy directly\uff1a \u00b6 helm install loggie -nloggie --create-namespace https://github.com/loggie-io/installation/releases/download/ ${ VERSION } /loggie- ${ VERSION } .tgz Please replace the <VERSION> above with the specific version number. Want to use image of another version? In order to facilitate the experience of the latest fixes and features, we provide the image version of the main branch after each merge, which can be selected here . At the same time, you can add to the helm install command --set image=loggieio/loggie:vX.Y.Z to specify a specific Loggie image. Problems with deployment? If you have problems when trying to deploy, or the following demonstration operation fails in your environment, please refer to deploy Loggie in Kubernetes and modify the relevant configuration. 3. Collect Logs \u00b6 Loggie defines Kubernetes CRD LogConfig. A LogConfig represents a log collection task for collecting a set of Pods. 3.1 Create Pods to Be Collected \u00b6 We first create a Pod for log collection. kubectl create deploy nginx --image = nginx Next, the standard output log \"stdout\" of this Nginx Pod will be collected. 3.2 Define the Sink \u00b6 Next, we create a Sink instance (CRD defined by Loggie), indicating the backend for log sending. For the convenience of demonstration, here we send the log to the log of the Loggie Agent itself and print it. cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : default spec : sink : | type: dev printEvents: true EOF You can use kubectl get sink to view the Sink that has been created. 3.3 Define Collection Tasks \u00b6 Loggie defines CRD LogConfig, which represents a log collection task. We create a LogConfig example as follows: cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : nginx namespace : default spec : selector : type : pod labelSelector : app : nginx pipeline : sources : | - type: file name: mylog paths: - stdout sinkRef : default EOF As you can see, the sinkRef uses the reference just created sink default CR . we can also use the sink field directly in Logconfig: cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : nginx namespace : default spec : selector : type : pod labelSelector : app : nginx pipeline : sources : | - type: file name: mylog paths: - stdout sink : | type: dev printEvents: true codec: type: json pretty: true EOF After creation, we can use kubectl get lgc to view the created CRD instance. At the same time, we can also use kubectl describe lgc nginx to get the latest status by viewing the events of LogConfig. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal syncSuccess 52s loggie/kind-control-plane Sync type pod [nginx-6799fc88d8-5cb67] success The above nginx LogConfig chooses Pods to collect log through the spec.selector. Here we use app: nginx to choose the nginx Pod that was just created. spec.pipeline represents Loggie's Pipeline configuration. We only collect logs from the standard output of the container, so we fill stdout in paths. 4. View Log \u00b6 First find the nginx pod node where it is located: kubectl get po -owide -l app = nginx Then we find the Loggie of that node: kubectl -nloggie get po -owide | grep ${ node } Use kubectl -nloggie logs -f ${ logge -pod } to check the log printed by Loggie, which shows the collected nginx log. More \u00b6 The above is just a simple demonstration. If you have problems with the deployment or want to know more about how to use Loggie in Kubernetes... A more comprehensive introduction to deployment: Deploy Loggie in Kubernetes Best practices for log collection in Kubernetes: Collect logs in Kubernetes","title":"Kubernetes"},{"location":"getting-started/quick-start/kubernetes/#quick-start-collecting-pod-logs-in-kubernetes","text":"The following article will show you how to quickly collect Pod logs by creating a LogConfig CRD in a Kubernetes cluster.","title":"Quick Start: Collecting Pod Logs in Kubernetes"},{"location":"getting-started/quick-start/kubernetes/#1-prepare-kubernetes","text":"You can use an existing Kubernetes cluster, or deploy Kubernetes . It is recommended to use Kind to build a Kubernetes cluster locally. The operations need to be used locally: kubectl ( download ) helm ( download ) Make sure you have kubectl and helm executable locally.","title":"1. Prepare Kubernetes"},{"location":"getting-started/quick-start/kubernetes/#2-deploy-loggie-daemonset","text":"You can view all released deployment charts on the installation page. You can choose:","title":"2. Deploy Loggie DaemonSet"},{"location":"getting-started/quick-start/kubernetes/#download-the-chart-and-deploy-it","text":"VERSION = v1.3.0 helm pull https://github.com/loggie-io/installation/releases/download/ ${ VERSION } /loggie- ${ VERSION } .tgz && tar xvzf loggie- ${ VERSION } .tgz Try to modify values.yaml in it. Please replace the <VERSION> above with the specific version number. Deploy: helm install loggie ./loggie -nloggie --create-namespace You can also:","title":"Download the chart and deploy it"},{"location":"getting-started/quick-start/kubernetes/#deploy-directly","text":"helm install loggie -nloggie --create-namespace https://github.com/loggie-io/installation/releases/download/ ${ VERSION } /loggie- ${ VERSION } .tgz Please replace the <VERSION> above with the specific version number. Want to use image of another version? In order to facilitate the experience of the latest fixes and features, we provide the image version of the main branch after each merge, which can be selected here . At the same time, you can add to the helm install command --set image=loggieio/loggie:vX.Y.Z to specify a specific Loggie image. Problems with deployment? If you have problems when trying to deploy, or the following demonstration operation fails in your environment, please refer to deploy Loggie in Kubernetes and modify the relevant configuration.","title":"Deploy directly\uff1a"},{"location":"getting-started/quick-start/kubernetes/#3-collect-logs","text":"Loggie defines Kubernetes CRD LogConfig. A LogConfig represents a log collection task for collecting a set of Pods.","title":"3. Collect Logs"},{"location":"getting-started/quick-start/kubernetes/#31-create-pods-to-be-collected","text":"We first create a Pod for log collection. kubectl create deploy nginx --image = nginx Next, the standard output log \"stdout\" of this Nginx Pod will be collected.","title":"3.1 Create Pods to Be Collected"},{"location":"getting-started/quick-start/kubernetes/#32-define-the-sink","text":"Next, we create a Sink instance (CRD defined by Loggie), indicating the backend for log sending. For the convenience of demonstration, here we send the log to the log of the Loggie Agent itself and print it. cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : default spec : sink : | type: dev printEvents: true EOF You can use kubectl get sink to view the Sink that has been created.","title":"3.2 Define the Sink"},{"location":"getting-started/quick-start/kubernetes/#33-define-collection-tasks","text":"Loggie defines CRD LogConfig, which represents a log collection task. We create a LogConfig example as follows: cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : nginx namespace : default spec : selector : type : pod labelSelector : app : nginx pipeline : sources : | - type: file name: mylog paths: - stdout sinkRef : default EOF As you can see, the sinkRef uses the reference just created sink default CR . we can also use the sink field directly in Logconfig: cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : nginx namespace : default spec : selector : type : pod labelSelector : app : nginx pipeline : sources : | - type: file name: mylog paths: - stdout sink : | type: dev printEvents: true codec: type: json pretty: true EOF After creation, we can use kubectl get lgc to view the created CRD instance. At the same time, we can also use kubectl describe lgc nginx to get the latest status by viewing the events of LogConfig. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal syncSuccess 52s loggie/kind-control-plane Sync type pod [nginx-6799fc88d8-5cb67] success The above nginx LogConfig chooses Pods to collect log through the spec.selector. Here we use app: nginx to choose the nginx Pod that was just created. spec.pipeline represents Loggie's Pipeline configuration. We only collect logs from the standard output of the container, so we fill stdout in paths.","title":"3.3 Define Collection Tasks"},{"location":"getting-started/quick-start/kubernetes/#4-view-log","text":"First find the nginx pod node where it is located: kubectl get po -owide -l app = nginx Then we find the Loggie of that node: kubectl -nloggie get po -owide | grep ${ node } Use kubectl -nloggie logs -f ${ logge -pod } to check the log printed by Loggie, which shows the collected nginx log.","title":"4. View Log"},{"location":"getting-started/quick-start/kubernetes/#more","text":"The above is just a simple demonstration. If you have problems with the deployment or want to know more about how to use Loggie in Kubernetes... A more comprehensive introduction to deployment: Deploy Loggie in Kubernetes Best practices for log collection in Kubernetes: Collect logs in Kubernetes","title":"More"},{"location":"getting-started/quick-start/node/","text":"Quick Start: Host Log Collection \u00b6 We will demonstrate the simplest scenario of collecting host log files. 1. Download the Executable File \u00b6 Please find a Linux server host and download the Loggie binary executable file. VERSION = v1.3.0 curl -LJ https://github.com/loggie-io/loggie/releases/download/ ${ VERSION } /loggie-linux-amd64 -o loggie Please replace <VERSION> above with the specific version number. 2. Add Configuration File \u00b6 We first use the dev sink to print the log file collected by the file source to the standard output. Copy the following content into pipelines.yml. pipelines.yml cat << EOF > pipelines.yml pipelines : - name : demo sources : - type : file name : mylog paths : - \"/var/log/*.log\" sink : type : dev printEvents : true EOF Here we create a pipeline named demo, and then define a source (type: file), indicating that files that match *.log in directory /var/log need to be collected. After the file is collected, it will be sent to the dev sink, which simply prints the collected file to standard output. The pipeline file represents the business-related configuration we want such as input and output. In addition to the pipeline configuration file, Loggie also needs to have a global configuration file. // loggie.yml cat << EOF > loggie.yml loggie : reload : enabled : true period : 10s EOF Here we only display a relatively simple configuration, which means that Loggie's dynamic configuration reload function is turned on, and the interval of check is 10s. After adding the above two configuration files on the host, we can start Loggie. 3. Run \u00b6 ./loggie -config.system = ./loggie.yml -config.pipeline = ./pipelines.yml -log.jsonFormat = false Fill the file paths of logie.yml and pipelines.yml in CMD arguments. Normal startup log shows that Loggie has started to work normally. Contents of files matching /var/log/*.log will be printed to standard output.","title":"Host"},{"location":"getting-started/quick-start/node/#quick-start-host-log-collection","text":"We will demonstrate the simplest scenario of collecting host log files.","title":"Quick Start: Host Log Collection"},{"location":"getting-started/quick-start/node/#1-download-the-executable-file","text":"Please find a Linux server host and download the Loggie binary executable file. VERSION = v1.3.0 curl -LJ https://github.com/loggie-io/loggie/releases/download/ ${ VERSION } /loggie-linux-amd64 -o loggie Please replace <VERSION> above with the specific version number.","title":"1. Download the Executable File"},{"location":"getting-started/quick-start/node/#2-add-configuration-file","text":"We first use the dev sink to print the log file collected by the file source to the standard output. Copy the following content into pipelines.yml. pipelines.yml cat << EOF > pipelines.yml pipelines : - name : demo sources : - type : file name : mylog paths : - \"/var/log/*.log\" sink : type : dev printEvents : true EOF Here we create a pipeline named demo, and then define a source (type: file), indicating that files that match *.log in directory /var/log need to be collected. After the file is collected, it will be sent to the dev sink, which simply prints the collected file to standard output. The pipeline file represents the business-related configuration we want such as input and output. In addition to the pipeline configuration file, Loggie also needs to have a global configuration file. // loggie.yml cat << EOF > loggie.yml loggie : reload : enabled : true period : 10s EOF Here we only display a relatively simple configuration, which means that Loggie's dynamic configuration reload function is turned on, and the interval of check is 10s. After adding the above two configuration files on the host, we can start Loggie.","title":"2. Add Configuration File"},{"location":"getting-started/quick-start/node/#3-run","text":"./loggie -config.system = ./loggie.yml -config.pipeline = ./pipelines.yml -log.jsonFormat = false Fill the file paths of logie.yml and pipelines.yml in CMD arguments. Normal startup log shows that Loggie has started to work normally. Contents of files matching /var/log/*.log will be printed to standard output.","title":"3. Run"},{"location":"getting-started/quick-start/quick-start/","text":"Quick Start \u00b6 Loggie is a log collection agent and an aggregator for transfer. It includes multiple pipelines, and each pipeline is composed of Source, Interceptor, and Sink. Based on this plug-in design, Loggie is not limited to log collection. By configuring different Source/Interceptor/Sink, Loggie can realize various functions. For simplicity, here we start by collecting logs. Choose your deployment environment \uff1a Kubernetes \u00b6 Host \u00b6","title":"Overview"},{"location":"getting-started/quick-start/quick-start/#quick-start","text":"Loggie is a log collection agent and an aggregator for transfer. It includes multiple pipelines, and each pipeline is composed of Source, Interceptor, and Sink. Based on this plug-in design, Loggie is not limited to log collection. By configuring different Source/Interceptor/Sink, Loggie can realize various functions. For simplicity, here we start by collecting logs. Choose your deployment environment \uff1a","title":"Quick Start"},{"location":"getting-started/quick-start/quick-start/#kubernetes","text":"","title":"  Kubernetes"},{"location":"getting-started/quick-start/quick-start/#host","text":"","title":"  Host"},{"location":"getting-started/roadmap/roadmap-2023/","text":"2023 Loggie RoadMap \u00b6 More Components and Functional Extensions \u00b6 Persistent queue. Stream processing capabilities: aggregation, computing, etc. (similar to pulsar's funtion, or lightweight flink). Source: http ... Sink: clickhouse, influxdb, s3, hdfs, etc. WASM form supports custom log parsing processing. Supports serverless expansion and shrinkage indicators (like Knative/KEDA), and realizes automatic expansion and shrinkage of aggregator analysis and processing. Cloud Native and Kubernetes \u00b6 Support automatic injection of Loggie sidecar. opentelemetry compatibility and support. Service Discovery \u00b6 Loggie dashboard: Provides a front-end page for configuration management.","title":2023},{"location":"getting-started/roadmap/roadmap-2023/#2023-loggie-roadmap","text":"","title":"2023 Loggie RoadMap"},{"location":"getting-started/roadmap/roadmap-2023/#more-components-and-functional-extensions","text":"Persistent queue. Stream processing capabilities: aggregation, computing, etc. (similar to pulsar's funtion, or lightweight flink). Source: http ... Sink: clickhouse, influxdb, s3, hdfs, etc. WASM form supports custom log parsing processing. Supports serverless expansion and shrinkage indicators (like Knative/KEDA), and realizes automatic expansion and shrinkage of aggregator analysis and processing.","title":"More Components and Functional Extensions"},{"location":"getting-started/roadmap/roadmap-2023/#cloud-native-and-kubernetes","text":"Support automatic injection of Loggie sidecar. opentelemetry compatibility and support.","title":"Cloud Native and Kubernetes"},{"location":"getting-started/roadmap/roadmap-2023/#service-discovery","text":"Loggie dashboard: Provides a front-end page for configuration management.","title":"Service Discovery"},{"location":"reference/","text":"Configuration \u00b6 The configuration of Loggie is mainly divided into two categories: System Configuration \u00b6 The global system configuration, which can be assigned by -config.system in CMD arguments, includes the following: monitor : monitor related configuration discovery : service discovery and configuration delivery reload : hot reload for dynamic configuration defaults : global default configuration http : http port used for management and monitoring loggie.yml # loggie.yml loggie : monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~ discovery : enabled : false reload : enabled : true period : 10s defaults : sink : type : dev sources : - type : file watcher : cleanFiles : maxHistory : 1 http : enabled : true port : 9196 Pipeline Configuration \u00b6 Pipeline configuration, specified by -config.pipeline in CMD arguments, indicates the Source, Sink, Queue and Interceptor used by the pipeline. Source : Multiple Sources can be configured for each Pipeline. Interceptor : Multiple Interceptors can be configured for each Pipeline. Sink : One Sink can be configured for each Pipeline. Queue : The default is the channel queue. Generally no configuration is required pipeline.yml pipelines : - name : demo # pipeline name, required sources : - type : ${sourceType} name : access # source name, required ... interceptors : - type : ${interceptorType} ... sink : type : ${sinkType} ... Kubernetes CRD \u00b6 Loggie defines the following CRDs for issuing configurations in Kubernetes: LogConfig : namespace level, representing a Pipeline configuration that can be used to collect container logs of Pods. ClusterLogConfig : cluster level, indicating a pipeline configuration, used for cluster-level cross-Namespace collection of Pod container logs, collection of logs on Node nodes, and distribution of general pipeline configuration for a Loggie cluster. Sink : cluster level, representing a sink configuration, which can be referenced in LogConfig/ClusterLogConfig. Interceptors : cluster level, representing an interceptors group, which can be referenced in LogConfig. Note The pipeline in ClusterLogConfig/LogConfig can define sink and interceptor, which are used for the sink/interceptor of this pipeline. If you want to reuse sinks or interceptors in multiple ClusterLogConfig/LogConfig, you can create a Sink/Interceptor CR and use sinkRef/interceptorRef in ClusterLogConfig/LogConfig for reference.","title":"Overview"},{"location":"reference/#configuration","text":"The configuration of Loggie is mainly divided into two categories:","title":"Configuration"},{"location":"reference/#system-configuration","text":"The global system configuration, which can be assigned by -config.system in CMD arguments, includes the following: monitor : monitor related configuration discovery : service discovery and configuration delivery reload : hot reload for dynamic configuration defaults : global default configuration http : http port used for management and monitoring loggie.yml # loggie.yml loggie : monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~ discovery : enabled : false reload : enabled : true period : 10s defaults : sink : type : dev sources : - type : file watcher : cleanFiles : maxHistory : 1 http : enabled : true port : 9196","title":"System Configuration"},{"location":"reference/#pipeline-configuration","text":"Pipeline configuration, specified by -config.pipeline in CMD arguments, indicates the Source, Sink, Queue and Interceptor used by the pipeline. Source : Multiple Sources can be configured for each Pipeline. Interceptor : Multiple Interceptors can be configured for each Pipeline. Sink : One Sink can be configured for each Pipeline. Queue : The default is the channel queue. Generally no configuration is required pipeline.yml pipelines : - name : demo # pipeline name, required sources : - type : ${sourceType} name : access # source name, required ... interceptors : - type : ${interceptorType} ... sink : type : ${sinkType} ...","title":"Pipeline Configuration"},{"location":"reference/#kubernetes-crd","text":"Loggie defines the following CRDs for issuing configurations in Kubernetes: LogConfig : namespace level, representing a Pipeline configuration that can be used to collect container logs of Pods. ClusterLogConfig : cluster level, indicating a pipeline configuration, used for cluster-level cross-Namespace collection of Pod container logs, collection of logs on Node nodes, and distribution of general pipeline configuration for a Loggie cluster. Sink : cluster level, representing a sink configuration, which can be referenced in LogConfig/ClusterLogConfig. Interceptors : cluster level, representing an interceptors group, which can be referenced in LogConfig. Note The pipeline in ClusterLogConfig/LogConfig can define sink and interceptor, which are used for the sink/interceptor of this pipeline. If you want to reuse sinks or interceptors in multiple ClusterLogConfig/LogConfig, you can create a Sink/Interceptor CR and use sinkRef/interceptorRef in ClusterLogConfig/LogConfig for reference.","title":"Kubernetes CRD"},{"location":"reference/discovery/kubernetes/clusterlogconfig/","text":"ClusterLogConfig \u00b6 Cluster-level CRDs that can be used to: Collect Pod logs of any Namespace Collect logs on Nodes Deliver the Pipeline configuration to the specified Loggie cluster Example apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : test spec : selector : type : node nodeSelector : nodepool : test pipeline : sources : | - type: file name: messages paths: - /var/log/messages sinkRef : default spec.selector \u00b6 Indicates the scope to which the Pipeline configuration applies type: pod \u00b6 Select a batch of Pods for log collection through Pipeline configuration field type required default description labelSelector map true Use this label to match Pods, support using * to match value, such as app: * Example spec : selector : type : pod labelSelector : app : nginx Indicates that logs of all Pods logs with label app: nginx under the namespace are collected. Warning When using type: pod , pipeline can only use file source. Only log collecting in this scene. type: node \u00b6 Deliver the Pipeline configuration to the batch of nodes. field type required default description nodeSelector map true Select the node to deliver the configuration through label Example spec : selector : type : node nodeSelector : nodepool : test Indicates that the configured Pipelines will be delivered to all nodes with nodepool: test . type: cluster \u00b6 To deliver the Pipeline configuration to a Loggie cluster, it usually needs to be used with the cluster specified cluster name in the field. Example spec : selector : cluster : aggregator type : cluster Indicates that the configured Pipelines are delivered to cluster whose cluster is aggregator. cluster \u00b6 field type required default description cluster string false \"\" Indicates Loggie cluster that should be delivered configuration. When deploying multiple sets of Loggie, it is used with the global system configuration discovery.kubernetes.cluster . spec.pipeline \u00b6 The configuration is consistent with LogConfig.","title":"ClusterLogConfig"},{"location":"reference/discovery/kubernetes/clusterlogconfig/#clusterlogconfig","text":"Cluster-level CRDs that can be used to: Collect Pod logs of any Namespace Collect logs on Nodes Deliver the Pipeline configuration to the specified Loggie cluster Example apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : test spec : selector : type : node nodeSelector : nodepool : test pipeline : sources : | - type: file name: messages paths: - /var/log/messages sinkRef : default","title":"ClusterLogConfig"},{"location":"reference/discovery/kubernetes/clusterlogconfig/#specselector","text":"Indicates the scope to which the Pipeline configuration applies","title":"spec.selector"},{"location":"reference/discovery/kubernetes/clusterlogconfig/#type-pod","text":"Select a batch of Pods for log collection through Pipeline configuration field type required default description labelSelector map true Use this label to match Pods, support using * to match value, such as app: * Example spec : selector : type : pod labelSelector : app : nginx Indicates that logs of all Pods logs with label app: nginx under the namespace are collected. Warning When using type: pod , pipeline can only use file source. Only log collecting in this scene.","title":"type: pod"},{"location":"reference/discovery/kubernetes/clusterlogconfig/#type-node","text":"Deliver the Pipeline configuration to the batch of nodes. field type required default description nodeSelector map true Select the node to deliver the configuration through label Example spec : selector : type : node nodeSelector : nodepool : test Indicates that the configured Pipelines will be delivered to all nodes with nodepool: test .","title":"type: node"},{"location":"reference/discovery/kubernetes/clusterlogconfig/#type-cluster","text":"To deliver the Pipeline configuration to a Loggie cluster, it usually needs to be used with the cluster specified cluster name in the field. Example spec : selector : cluster : aggregator type : cluster Indicates that the configured Pipelines are delivered to cluster whose cluster is aggregator.","title":"type: cluster"},{"location":"reference/discovery/kubernetes/clusterlogconfig/#cluster","text":"field type required default description cluster string false \"\" Indicates Loggie cluster that should be delivered configuration. When deploying multiple sets of Loggie, it is used with the global system configuration discovery.kubernetes.cluster .","title":"cluster"},{"location":"reference/discovery/kubernetes/clusterlogconfig/#specpipeline","text":"The configuration is consistent with LogConfig.","title":"spec.pipeline"},{"location":"reference/discovery/kubernetes/interceptors/","text":"Interceptor \u00b6 Represents an interceptor group. Used to be referenced in LogConfig/ClusterLogConfig. Example apiVersion : loggie.io/v1beta1 kind : Interceptor metadata : name : default spec : interceptors : | - type: rateLimit qps: 90000 spec.interceptors \u00b6 Use the \"|\" symbol to indicate an entire interceptors configuration list, which is consistent with the configuration in Pipelines.","title":"Interceptor"},{"location":"reference/discovery/kubernetes/interceptors/#interceptor","text":"Represents an interceptor group. Used to be referenced in LogConfig/ClusterLogConfig. Example apiVersion : loggie.io/v1beta1 kind : Interceptor metadata : name : default spec : interceptors : | - type: rateLimit qps: 90000","title":"Interceptor"},{"location":"reference/discovery/kubernetes/interceptors/#specinterceptors","text":"Use the \"|\" symbol to indicate an entire interceptors configuration list, which is consistent with the configuration in Pipelines.","title":"spec.interceptors"},{"location":"reference/discovery/kubernetes/logconfig/","text":"Logconfig \u00b6 A namespace-level CRD, which represents a log collection task, is used to collect Pod container logs. Example define sink/interceptor directly apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : tomcat namespace : default spec : selector : type : pod labelSelector : app : tomcat pipeline : sources : | - type: file name: common paths: - stdout sink : | type: dev printEvents: false interceptors : | - type: rateLimit qps: 90000 reference sink/interceptor apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : nginx namespace : default spec : selector : type : pod labelSelector : app : nginx pipeline : sources : | - type: file name: mylog paths: - stdout sinkRef : default interceptorRef : default spec.selector \u00b6 Indicates the scope of the Pipeline configuration. You can choose to collect a batch of Pods logs. type: pod \u00b6 Collect Pods logs field type required default description labelSelector map true Use this label to match Pods, support using * to match value, such as app: * Example spec : selector : type : pod labelSelector : app : nginx Indicates that logs of all Pods logs with label app: nginx under the namespace are collected. Warning When using type: pod , pipeline can only use file source. Only log collecting in this scene. cluster \u00b6 field type required default description cluster string false \"\" Indicates the configuration of the specified delivery Loggie cluster. When deploying multiple sets of Loggie, it is used in with the global system configuration discovery.kubernetes.cluster spec.pipeline \u00b6 Indicates one Pipeline, and multiple Pipelines are not supported. The difference from Pipelines in the configuration file is: sources is actually a string. \uff5c used in yaml to keep newlines. sinkRef, which represents the referenced Sink CRD instance. interceptorRef, which represents the referenced Interceptor CRD instance. sources \u00b6 In LogConfig, when type: pod , several parameters specifically for containerization are added into file source : field type required default description containerName string false indicates the container name of the specified collection, it is recommended to fill in when the Pod contains multiple containers excludeContainerPatterns string array false excluded container names, in regular expression form matchFields false add information from Pod to Fields matchFields.labelKey string array false Specify the Label Key value on the Pod. For example, the Pod contains Label: app: demo , and fill labelKey: app in here. In this case, the label app: demo on the Pod will be added to the file source fields, and the collected logs will be added with the label information. Suitable for pods that matched with different labels. \"*\" is supported to get all labels. matchFields.annotationKey string array false Similar to the above labelKey. Inject annotations of pod. \"*\" is supported matchFields.env string array false Similar to the above labelKey. Inject env of pod. \"*\" is supported Example apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : nginx namespace : default spec : selector : type : pod labelSelector : app : nginx pipeline : sources : | - type: file name: mylog containerName: nginx matchFields: labelKey: [\"app\"] paths: - stdout interceptors \u00b6 field type required default description interceptors string false Indicates the interceptor of the Pipeline, which is used in a similar way to the sources above sink \u00b6 field type required default description sink string false Represents the sink of the Pipeline, which is used in a similar way to the sources above If you want the sink and interceptor to be reused between different ClusterLogConfig/LogConfig, you can use the following ref method: sinkRef \u00b6 field type required default description sinkRef string false Represents the Sink CR referenced by this Pipeline interceptorRef \u00b6 field type required default description interceptorRef string false Represents the Interceptor CR referenced by this Pipeline","title":"LogConfig"},{"location":"reference/discovery/kubernetes/logconfig/#logconfig","text":"A namespace-level CRD, which represents a log collection task, is used to collect Pod container logs. Example define sink/interceptor directly apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : tomcat namespace : default spec : selector : type : pod labelSelector : app : tomcat pipeline : sources : | - type: file name: common paths: - stdout sink : | type: dev printEvents: false interceptors : | - type: rateLimit qps: 90000 reference sink/interceptor apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : nginx namespace : default spec : selector : type : pod labelSelector : app : nginx pipeline : sources : | - type: file name: mylog paths: - stdout sinkRef : default interceptorRef : default","title":"Logconfig"},{"location":"reference/discovery/kubernetes/logconfig/#specselector","text":"Indicates the scope of the Pipeline configuration. You can choose to collect a batch of Pods logs.","title":"spec.selector"},{"location":"reference/discovery/kubernetes/logconfig/#type-pod","text":"Collect Pods logs field type required default description labelSelector map true Use this label to match Pods, support using * to match value, such as app: * Example spec : selector : type : pod labelSelector : app : nginx Indicates that logs of all Pods logs with label app: nginx under the namespace are collected. Warning When using type: pod , pipeline can only use file source. Only log collecting in this scene.","title":"type: pod"},{"location":"reference/discovery/kubernetes/logconfig/#cluster","text":"field type required default description cluster string false \"\" Indicates the configuration of the specified delivery Loggie cluster. When deploying multiple sets of Loggie, it is used in with the global system configuration discovery.kubernetes.cluster","title":"cluster"},{"location":"reference/discovery/kubernetes/logconfig/#specpipeline","text":"Indicates one Pipeline, and multiple Pipelines are not supported. The difference from Pipelines in the configuration file is: sources is actually a string. \uff5c used in yaml to keep newlines. sinkRef, which represents the referenced Sink CRD instance. interceptorRef, which represents the referenced Interceptor CRD instance.","title":"spec.pipeline"},{"location":"reference/discovery/kubernetes/logconfig/#sources","text":"In LogConfig, when type: pod , several parameters specifically for containerization are added into file source : field type required default description containerName string false indicates the container name of the specified collection, it is recommended to fill in when the Pod contains multiple containers excludeContainerPatterns string array false excluded container names, in regular expression form matchFields false add information from Pod to Fields matchFields.labelKey string array false Specify the Label Key value on the Pod. For example, the Pod contains Label: app: demo , and fill labelKey: app in here. In this case, the label app: demo on the Pod will be added to the file source fields, and the collected logs will be added with the label information. Suitable for pods that matched with different labels. \"*\" is supported to get all labels. matchFields.annotationKey string array false Similar to the above labelKey. Inject annotations of pod. \"*\" is supported matchFields.env string array false Similar to the above labelKey. Inject env of pod. \"*\" is supported Example apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : nginx namespace : default spec : selector : type : pod labelSelector : app : nginx pipeline : sources : | - type: file name: mylog containerName: nginx matchFields: labelKey: [\"app\"] paths: - stdout","title":"sources"},{"location":"reference/discovery/kubernetes/logconfig/#interceptors","text":"field type required default description interceptors string false Indicates the interceptor of the Pipeline, which is used in a similar way to the sources above","title":"interceptors"},{"location":"reference/discovery/kubernetes/logconfig/#sink","text":"field type required default description sink string false Represents the sink of the Pipeline, which is used in a similar way to the sources above If you want the sink and interceptor to be reused between different ClusterLogConfig/LogConfig, you can use the following ref method:","title":"sink"},{"location":"reference/discovery/kubernetes/logconfig/#sinkref","text":"field type required default description sinkRef string false Represents the Sink CR referenced by this Pipeline","title":"sinkRef"},{"location":"reference/discovery/kubernetes/logconfig/#interceptorref","text":"field type required default description interceptorRef string false Represents the Interceptor CR referenced by this Pipeline","title":"interceptorRef"},{"location":"reference/discovery/kubernetes/sink/","text":"Sink \u00b6 Represents a sink configuration. Used to be referenced in LogConfig/ClusterLogConfig. Example apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : default spec : sink : | type: elasticsearch index: \"loggie\" hosts: [\"elasticsearch-master.default.svc:9200\"] spec.sink \u00b6 Use the \"|\" symbol to indicate a sink configuration, which is consistent with the configuration in Pipelines.","title":"Sink"},{"location":"reference/discovery/kubernetes/sink/#sink","text":"Represents a sink configuration. Used to be referenced in LogConfig/ClusterLogConfig. Example apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : default spec : sink : | type: elasticsearch index: \"loggie\" hosts: [\"elasticsearch-master.default.svc:9200\"]","title":"Sink"},{"location":"reference/discovery/kubernetes/sink/#specsink","text":"Use the \"|\" symbol to indicate a sink configuration, which is consistent with the configuration in Pipelines.","title":"spec.sink"},{"location":"reference/global/args/","text":"System Parameters \u00b6 -config.from : The default is file, that is, the default configuration method using the file. Optional: env, in which case the configuration will be read from environment variables (reload is not supported at this time). -config.system : The default is loggie.yml, which indicates the path and file name of the specified Loghie system configuration. (if -config.from=env , configuration will be read from environment variables\uff09 -config.pipeline : The default is pipelines.yml, which indicates the path where the Pipeline configuration file is located. You need to fill in the path that matches the glob, such as the specific path and file name /etc/loggie/pipelines.yml , or glob matching, such as /etc/loggie/*.yml . (if -config.from=env , configuration will be read from environment variables) Warning Note that if config.pipeline=/etc/loggie , loggie file in /etc will be matched instead of files in /etc/loggie . Please avoid similar settings. -meta.nodeName : By default, the hostname of the system will be used. In Kubernetes, the Downward API will be used to inject nodeName. In general, no separate configuration is required Log Parameters: \u00b6 -log.level : log level, default info, can be configured as debug, info, warn and error. -log.jsonFormat : whether to output the log in json format, default false. -log.enableStdout : whether to output standard output log, default true. -log.enableFile : whether to output log files, default false. -log.directory : path of the log file, default /var/log. It takes effect when log.enableFile=true. -log.filename : name of the log file, default loggie.log, generally used with log.directory. -log.maxSize : maximum file size when the log is rotated, default 1024MB. -log.maxBackups : maximum number of files to keep in log rotation, default 3. -log.maxAge : maximum number of days for log rotation, default 7. -log.timeFormat : time format of each line of log output, default 2006-01-02 15:04:05 . -log.noColor : whether the output has color beautification, default false. If outputting to a log file, it is recommended to set it to true to avoid introducing additional formatting. Info Loggie's log rotation uses lumberjack library.","title":"Start Arguments"},{"location":"reference/global/args/#system-parameters","text":"-config.from : The default is file, that is, the default configuration method using the file. Optional: env, in which case the configuration will be read from environment variables (reload is not supported at this time). -config.system : The default is loggie.yml, which indicates the path and file name of the specified Loghie system configuration. (if -config.from=env , configuration will be read from environment variables\uff09 -config.pipeline : The default is pipelines.yml, which indicates the path where the Pipeline configuration file is located. You need to fill in the path that matches the glob, such as the specific path and file name /etc/loggie/pipelines.yml , or glob matching, such as /etc/loggie/*.yml . (if -config.from=env , configuration will be read from environment variables) Warning Note that if config.pipeline=/etc/loggie , loggie file in /etc will be matched instead of files in /etc/loggie . Please avoid similar settings. -meta.nodeName : By default, the hostname of the system will be used. In Kubernetes, the Downward API will be used to inject nodeName. In general, no separate configuration is required","title":"System Parameters"},{"location":"reference/global/args/#log-parameters","text":"-log.level : log level, default info, can be configured as debug, info, warn and error. -log.jsonFormat : whether to output the log in json format, default false. -log.enableStdout : whether to output standard output log, default true. -log.enableFile : whether to output log files, default false. -log.directory : path of the log file, default /var/log. It takes effect when log.enableFile=true. -log.filename : name of the log file, default loggie.log, generally used with log.directory. -log.maxSize : maximum file size when the log is rotated, default 1024MB. -log.maxBackups : maximum number of files to keep in log rotation, default 3. -log.maxAge : maximum number of days for log rotation, default 7. -log.timeFormat : time format of each line of log output, default 2006-01-02 15:04:05 . -log.noColor : whether the output has color beautification, default false. If outputting to a log file, it is recommended to set it to true to avoid introducing additional formatting. Info Loggie's log rotation uses lumberjack library.","title":"Log Parameters:"},{"location":"reference/global/defaults/","text":"Defaults \u00b6 defaults is used to set default values \u200b\u200bin the Pipelines configuration. Takes effect when no value is set in the Pipeline, or is used to override the default parameter. Example defaults : sources : - type : file watcher : cleanFiles : maxHistory : 10 sink : type : dev printEvents : true sources \u00b6 Consistent with source in Pipeline. When Pipelines is configured with a source of the same type, value of the field that is not filled in will be overwritten. exapmle: sources : - type : file watcher : cleanFiles : maxHistory : 10 If the Pipeline is configured with a file source, you can set the global file cleaning retention period to 10 days, instead of setting it in each Pipeline's file source. sink \u00b6 Consistent with the sink in Pipeline, if the cluster only needs to set a global sink output source, it only needs to be configured here once to avoid filling in each Pipeline. interceptors \u00b6 The interceptors configured in defaults will be added to the interceptors defined in the pipeline, but the interceptors in the pipeline will override the interceptors of the same type in defaults. If you do not want to cover the same type of interceptor, but add the same type of interceptor, and process them in turn, you need to fill in the name field for identification. The normalize interceptor has been defined in defaults as follows: defaults : interceptors : - type : normalize processor : - addMeta : ~ If the normalize interceptor is defined in the pipelines as follows: pipelines : interceptors : - type : normalize processor : - drop : ... At this time, the normalize interceptor in defaults will be overwritten and will not take effect. If we want to execute the normalize interceptor in defaults first, and then execute the normalize interceptor in the pipeline, we can change the defaults to: defaults : interceptors : - type : normalize name : global # for identification order : 500 # execution order. the larger, the latter to be executed. default 900. processor : - addMeta : ~ Loggie will add 3 built-in interceptors metric \u3001 maxbytes \u3001 retry by default. If you need to add other default interceptors, the above built-in interceptors will be overwritten, so it is strongly recommended to add built-in interceptors at this time, unless you confirm that you do not need the built-in interceptors. queue \u00b6 Channel queue by default.","title":"defaults"},{"location":"reference/global/defaults/#defaults","text":"defaults is used to set default values \u200b\u200bin the Pipelines configuration. Takes effect when no value is set in the Pipeline, or is used to override the default parameter. Example defaults : sources : - type : file watcher : cleanFiles : maxHistory : 10 sink : type : dev printEvents : true","title":"Defaults"},{"location":"reference/global/defaults/#sources","text":"Consistent with source in Pipeline. When Pipelines is configured with a source of the same type, value of the field that is not filled in will be overwritten. exapmle: sources : - type : file watcher : cleanFiles : maxHistory : 10 If the Pipeline is configured with a file source, you can set the global file cleaning retention period to 10 days, instead of setting it in each Pipeline's file source.","title":"sources"},{"location":"reference/global/defaults/#sink","text":"Consistent with the sink in Pipeline, if the cluster only needs to set a global sink output source, it only needs to be configured here once to avoid filling in each Pipeline.","title":"sink"},{"location":"reference/global/defaults/#interceptors","text":"The interceptors configured in defaults will be added to the interceptors defined in the pipeline, but the interceptors in the pipeline will override the interceptors of the same type in defaults. If you do not want to cover the same type of interceptor, but add the same type of interceptor, and process them in turn, you need to fill in the name field for identification. The normalize interceptor has been defined in defaults as follows: defaults : interceptors : - type : normalize processor : - addMeta : ~ If the normalize interceptor is defined in the pipelines as follows: pipelines : interceptors : - type : normalize processor : - drop : ... At this time, the normalize interceptor in defaults will be overwritten and will not take effect. If we want to execute the normalize interceptor in defaults first, and then execute the normalize interceptor in the pipeline, we can change the defaults to: defaults : interceptors : - type : normalize name : global # for identification order : 500 # execution order. the larger, the latter to be executed. default 900. processor : - addMeta : ~ Loggie will add 3 built-in interceptors metric \u3001 maxbytes \u3001 retry by default. If you need to add other default interceptors, the above built-in interceptors will be overwritten, so it is strongly recommended to add built-in interceptors at this time, unless you confirm that you do not need the built-in interceptors.","title":"interceptors"},{"location":"reference/global/defaults/#queue","text":"Channel queue by default.","title":"queue"},{"location":"reference/global/discovery/","text":"Discovery \u00b6 Configuration related to service discovery and configuration delivery, mainly included in Kubernetes-related global configuration. Example discovery : enabled : true kubernetes : # Choose: docker or containerd containerRuntime : containerd # Collect log files inside the container from the root filesystem of the container, no need to mount the volume rootFsCollectionEnabled : false # Automatically parse and convert the wrapped container standard output format into the original log content parseStdout : false # If set to true, it means that the pipeline configuration generated does not contain specific Pod paths and meta information, # and these data will be dynamically obtained by the file source, thereby reducing the number of configuration changes and reloads. dynamicContainerLog : false # Automatically add fields when selector.type is pod in logconfig/clusterlogconfig typePodFields : logconfig : \"${_k8s.logconfig}\" namespace : \"${_k8s.pod.namespace}\" nodename : \"${_k8s.node.name}\" podname : \"${_k8s.pod.name}\" containername : \"${_k8s.pod.container.name}\" typeNodeFields : nodename : \"${_k8s.node.name}\" clusterlogconfig : \"${_k8s.clusterlogconfig}\" os : \"${_k8s.node.nodeInfo.osImage}\" enabled \u00b6 field type required default description enabled bool false Whether to enable the service discovery configuration delivery kubernetes \u00b6 field type required default description cluster string false Identifies the Loggie cluster name. Loggie supports deploying multiple sets of Loggie in a Kubernetes cluster. It can be specified in the LogConfig CRD by selector.cluster to specify the Loggie cluster to be configured. kubeconfig string false Specifies the kubeconfig file used to connect the Kubernetes API. Usually, it is not required when Loggie is deployed to a Kubernetes cluster. If Loggie is deployed outside Kubernetes (local debug), you need to specify the kubeconfig file. master string false Specify the master address for requesting the Kubernetes cluster API. Generally it do not need to be filled in inCluster mode containerRuntime string false docker container runtime, docker or containerd rootFsCollectionEnabled bool false false Whether to enable the collection of logs in the root filesystem when log volumes are not mounted. parseStdout bool false false Whether to enable automatic extraction of container standard output raw content dynamicContainerLog bool false false Whether to enable the dynamic container log configuration. The configuration file will not render the specific path and dynamic fields fields after opening, which can effectively avoid the frequent rendering of the configuration caused by Pod changes in large-scale containerization scenarios, and significantly reduce the number of reloads, especially on a single node. When the number of Pods is large and clusterlogconfig is used to match a large number of Pods, it is generally recommended to set it true. kubeletRootDir string false /var/lib/kubelet root path of kubelet podLogDirPrefix string false /var/log/pods The path where kubernetes place pod standard output by default typePodFields map false The kubernetes related meta info automatically added when logconfig/clusterlogconfig use selector type: pod . The key is key of added meta info, and the value should be specified in the form of ${_k8s.XX}. Fixed key:value fields are also supported typeNodeFields map false The kubernetes related meta info automatically added when logconfig/clusterlogconfig use selector type: node . The key is key of added meta info, and the value should be specified in the form of ${_k8s.XX}. Fixed key:value fields are also supported Variables supported by typePodFields \u00b6 \"${_k8s.XX}\" can be replaced by the following parameters: field type required default description ${_k8s.logconfig} string false add logConfig name as meta ${_k8s.node.name} string false add node name as meta ${_k8s.node.ip} string false add node ip as meta ${_k8s.pod.namespace} string false add namespace as meta ${_k8s.pod.name} string false add pod name as meta ${_k8s.pod.ip} string false add pod ip as meta ${_k8s.pod.uid} string false add pod uid as meta ${_k8s.pod.container.name} string false add container name as meta ${_k8s.pod.container.id} string false add container id as meta ${_k8s.pod.container.image} string false add container image as meta ${_k8s.workload.kind} string false add Deployment/Statefulset/DaemonSet/Job kind as meta ${_k8s.workload.name} string false add Deployment/Statefulset/DaemonSet/Job name as meta Variables supported by typeNodeFields \u00b6 \"${_k8s.XX}\" can be replaced by the following parameters: field type required default description ${_k8s.clusterlogconfig} string false add clusterlogconfig nameas meta ${_k8s.node.name} string false add node nameas meta ${_k8s.node.addresses.InternalIP} string false add node InternalIP as meta ${_k8s.node.addresses.Hostname} string false add node Hostname as meta ${_k8s.node.nodeInfo.kernelVersion} string false add node kernelVersion as meta ${_k8s.node.nodeInfo.osImage} string false add node osImageas meta ${_k8s.node.nodeInfo.containerRuntimeVersion} string false add node containerRuntimeVersion as meta ${_k8s.node.nodeInfo.kubeletVersion} string false add node kubeletVersionas meta ${_k8s.node.nodeInfo.kubeProxyVersion} string false add node kubeProxyVersionas meta ${_k8s.node.nodeInfo.operatingSystem} string false add node operatingSystemas meta ${_k8s.node.nodeInfo.architecture} string false add node architectureas meta ${_k8s.node.labels. } string false add a label of node as meta. replace <key> with a specific label key ${_k8s.node.annotations. } string false add an annotation of node as meta. replace <key> with a specific annotation key","title":"discovery"},{"location":"reference/global/discovery/#discovery","text":"Configuration related to service discovery and configuration delivery, mainly included in Kubernetes-related global configuration. Example discovery : enabled : true kubernetes : # Choose: docker or containerd containerRuntime : containerd # Collect log files inside the container from the root filesystem of the container, no need to mount the volume rootFsCollectionEnabled : false # Automatically parse and convert the wrapped container standard output format into the original log content parseStdout : false # If set to true, it means that the pipeline configuration generated does not contain specific Pod paths and meta information, # and these data will be dynamically obtained by the file source, thereby reducing the number of configuration changes and reloads. dynamicContainerLog : false # Automatically add fields when selector.type is pod in logconfig/clusterlogconfig typePodFields : logconfig : \"${_k8s.logconfig}\" namespace : \"${_k8s.pod.namespace}\" nodename : \"${_k8s.node.name}\" podname : \"${_k8s.pod.name}\" containername : \"${_k8s.pod.container.name}\" typeNodeFields : nodename : \"${_k8s.node.name}\" clusterlogconfig : \"${_k8s.clusterlogconfig}\" os : \"${_k8s.node.nodeInfo.osImage}\"","title":"Discovery"},{"location":"reference/global/discovery/#enabled","text":"field type required default description enabled bool false Whether to enable the service discovery configuration delivery","title":"enabled"},{"location":"reference/global/discovery/#kubernetes","text":"field type required default description cluster string false Identifies the Loggie cluster name. Loggie supports deploying multiple sets of Loggie in a Kubernetes cluster. It can be specified in the LogConfig CRD by selector.cluster to specify the Loggie cluster to be configured. kubeconfig string false Specifies the kubeconfig file used to connect the Kubernetes API. Usually, it is not required when Loggie is deployed to a Kubernetes cluster. If Loggie is deployed outside Kubernetes (local debug), you need to specify the kubeconfig file. master string false Specify the master address for requesting the Kubernetes cluster API. Generally it do not need to be filled in inCluster mode containerRuntime string false docker container runtime, docker or containerd rootFsCollectionEnabled bool false false Whether to enable the collection of logs in the root filesystem when log volumes are not mounted. parseStdout bool false false Whether to enable automatic extraction of container standard output raw content dynamicContainerLog bool false false Whether to enable the dynamic container log configuration. The configuration file will not render the specific path and dynamic fields fields after opening, which can effectively avoid the frequent rendering of the configuration caused by Pod changes in large-scale containerization scenarios, and significantly reduce the number of reloads, especially on a single node. When the number of Pods is large and clusterlogconfig is used to match a large number of Pods, it is generally recommended to set it true. kubeletRootDir string false /var/lib/kubelet root path of kubelet podLogDirPrefix string false /var/log/pods The path where kubernetes place pod standard output by default typePodFields map false The kubernetes related meta info automatically added when logconfig/clusterlogconfig use selector type: pod . The key is key of added meta info, and the value should be specified in the form of ${_k8s.XX}. Fixed key:value fields are also supported typeNodeFields map false The kubernetes related meta info automatically added when logconfig/clusterlogconfig use selector type: node . The key is key of added meta info, and the value should be specified in the form of ${_k8s.XX}. Fixed key:value fields are also supported","title":"kubernetes"},{"location":"reference/global/discovery/#variables-supported-by-typepodfields","text":"\"${_k8s.XX}\" can be replaced by the following parameters: field type required default description ${_k8s.logconfig} string false add logConfig name as meta ${_k8s.node.name} string false add node name as meta ${_k8s.node.ip} string false add node ip as meta ${_k8s.pod.namespace} string false add namespace as meta ${_k8s.pod.name} string false add pod name as meta ${_k8s.pod.ip} string false add pod ip as meta ${_k8s.pod.uid} string false add pod uid as meta ${_k8s.pod.container.name} string false add container name as meta ${_k8s.pod.container.id} string false add container id as meta ${_k8s.pod.container.image} string false add container image as meta ${_k8s.workload.kind} string false add Deployment/Statefulset/DaemonSet/Job kind as meta ${_k8s.workload.name} string false add Deployment/Statefulset/DaemonSet/Job name as meta","title":"Variables supported by typePodFields"},{"location":"reference/global/discovery/#variables-supported-by-typenodefields","text":"\"${_k8s.XX}\" can be replaced by the following parameters: field type required default description ${_k8s.clusterlogconfig} string false add clusterlogconfig nameas meta ${_k8s.node.name} string false add node nameas meta ${_k8s.node.addresses.InternalIP} string false add node InternalIP as meta ${_k8s.node.addresses.Hostname} string false add node Hostname as meta ${_k8s.node.nodeInfo.kernelVersion} string false add node kernelVersion as meta ${_k8s.node.nodeInfo.osImage} string false add node osImageas meta ${_k8s.node.nodeInfo.containerRuntimeVersion} string false add node containerRuntimeVersion as meta ${_k8s.node.nodeInfo.kubeletVersion} string false add node kubeletVersionas meta ${_k8s.node.nodeInfo.kubeProxyVersion} string false add node kubeProxyVersionas meta ${_k8s.node.nodeInfo.operatingSystem} string false add node operatingSystemas meta ${_k8s.node.nodeInfo.architecture} string false add node architectureas meta ${_k8s.node.labels. } string false add a label of node as meta. replace <key> with a specific label key ${_k8s.node.annotations. } string false add an annotation of node as meta. replace <key> with a specific annotation key","title":"Variables supported by typeNodeFields"},{"location":"reference/global/http/","text":"Http \u00b6 The Http port used by Loggie itself provides monitoring metrics, internal operation and maintenance and other interfaces. Example http : enabled : true host : \"0.0.0.0\" port : 9196 field type required default description enabled bool false false whether to enable http host string false 0.0.0.0 http listening host port http false 9196 http listening port Tips It is generally recommended to open the http port. if Loggie is deployed in Kubernetes or containers, please pay attention to whether port conflicts exist when using hostNetwork, and whether the monitoring host is exposed to the public network and whether there are security risks.","title":"http"},{"location":"reference/global/http/#http","text":"The Http port used by Loggie itself provides monitoring metrics, internal operation and maintenance and other interfaces. Example http : enabled : true host : \"0.0.0.0\" port : 9196 field type required default description enabled bool false false whether to enable http host string false 0.0.0.0 http listening host port http false 9196 http listening port Tips It is generally recommended to open the http port. if Loggie is deployed in Kubernetes or containers, please pay attention to whether port conflicts exist when using hostNetwork, and whether the monitoring host is exposed to the public network and whether there are security risks.","title":"Http"},{"location":"reference/global/monitor/","text":"Monitor \u00b6 Monitor the event bus. All components can export their own metrics data, which are consumed and processed by listeners. Refer to here for details. Example monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~","title":"monitor"},{"location":"reference/global/monitor/#monitor","text":"Monitor the event bus. All components can export their own metrics data, which are consumed and processed by listeners. Refer to here for details. Example monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~","title":"Monitor"},{"location":"reference/global/reload/","text":"Reload \u00b6 reload will regularly check the configuration file specified by -config.pipeline . If it detects that the content of the file has changed, it will restart the pipeline with the changed configuration, and the unmodified pipelines will not be affected. Example reload : enabled : true period : 10s field type required default description enabled bool false false whether to enable reload period time.Duration false 10s the time interval for reload detection. Not recommended to be set too short, otherwise it may increase CPU consumption","title":"reload"},{"location":"reference/global/reload/#reload","text":"reload will regularly check the configuration file specified by -config.pipeline . If it detects that the content of the file has changed, it will restart the pipeline with the changed configuration, and the unmodified pipelines will not be affected. Example reload : enabled : true period : 10s field type required default description enabled bool false false whether to enable reload period time.Duration false 10s the time interval for reload detection. Not recommended to be set too short, otherwise it may increase CPU consumption","title":"Reload"},{"location":"reference/global/var/","text":"Field Dynamic Variable \u00b6 In many scenarios, we need to dynamically obtain a field in the event. For example: The topic in the kafka sink, or the index in the elasticsearch sink, is dynamically generated according to the value of a field in the event. We can use ${a.b} to get value. In transformer interceptor, operate a field: copy(a.b, a.c). Take the following event is an example: { \"fields\" : { \"svc\" : \"test\" , } } The kafka sink topic is configured as log-${fields.svc} , and finally generated as log-test . Caution You can generally use . to denote nested fields. However, if the field itself contains . , it needs to be [] enclosed to avoid mistaking it for a nested field. For example: { \"fields\" : { \"a.b\" : \"demo\" , } } Use ${fields.[a.b]} to represent a.b .","title":"Field Variable"},{"location":"reference/global/var/#field-dynamic-variable","text":"In many scenarios, we need to dynamically obtain a field in the event. For example: The topic in the kafka sink, or the index in the elasticsearch sink, is dynamically generated according to the value of a field in the event. We can use ${a.b} to get value. In transformer interceptor, operate a field: copy(a.b, a.c). Take the following event is an example: { \"fields\" : { \"svc\" : \"test\" , } } The kafka sink topic is configured as log-${fields.svc} , and finally generated as log-test . Caution You can generally use . to denote nested fields. However, if the field itself contains . , it needs to be [] enclosed to avoid mistaking it for a nested field. For example: { \"fields\" : { \"a.b\" : \"demo\" , } } Use ${fields.[a.b]} to represent a.b .","title":"Field Dynamic Variable"},{"location":"reference/monitor/filesource/","text":"filesource listener \u00b6 The monitoring of real-time file collection indicates the progress and status of the current log collection, including the file name, collection progress, and QPS. Configuration \u00b6 field type required default description period time.Duration false 10s The interval at which listener consumes and processes data. Metrics \u00b6 LABELS: pipeline: Indicates the name of the pipeline where it is located. source: Indicates the name of the source where it is located. filename: indicates the file name. file_size \u00b6 # HELP file size # TYPE loggie_filesource_file_size gauge loggie_filesource_file_size{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\"} 2048 HELP: Indicates the file size of a current log file when it is collected. TYPE: gauge file_offset \u00b6 # HELP file offset # TYPE loggie_filesource_file_offset gauge loggie_filesource_file_offset{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\"} 1024 HELP: Indicates the current progress of a log file being collected, the offset of the current file being read. TYPE: gauge line_number \u00b6 # HELP current read line number # TYPE loggie_filesource_line_number gauge loggie_filesource_line_number{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\"} 20 HELP: Indicates the number of lines currently read when a log file is currently being collected TYPE: gauge line_qps \u00b6 # HELP current read line qps # TYPE loggie_filesource_line_qps gauge loggie_filesource_line_qps{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\"} 48 HELP: Indicates the number of lines read per second when a log file is currently being collected TYPE: gauge","title":"filesource"},{"location":"reference/monitor/filesource/#filesource-listener","text":"The monitoring of real-time file collection indicates the progress and status of the current log collection, including the file name, collection progress, and QPS.","title":"filesource listener"},{"location":"reference/monitor/filesource/#configuration","text":"field type required default description period time.Duration false 10s The interval at which listener consumes and processes data.","title":"Configuration"},{"location":"reference/monitor/filesource/#metrics","text":"LABELS: pipeline: Indicates the name of the pipeline where it is located. source: Indicates the name of the source where it is located. filename: indicates the file name.","title":"Metrics"},{"location":"reference/monitor/filesource/#file_size","text":"# HELP file size # TYPE loggie_filesource_file_size gauge loggie_filesource_file_size{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\"} 2048 HELP: Indicates the file size of a current log file when it is collected. TYPE: gauge","title":"file_size"},{"location":"reference/monitor/filesource/#file_offset","text":"# HELP file offset # TYPE loggie_filesource_file_offset gauge loggie_filesource_file_offset{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\"} 1024 HELP: Indicates the current progress of a log file being collected, the offset of the current file being read. TYPE: gauge","title":"file_offset"},{"location":"reference/monitor/filesource/#line_number","text":"# HELP current read line number # TYPE loggie_filesource_line_number gauge loggie_filesource_line_number{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\"} 20 HELP: Indicates the number of lines currently read when a log file is currently being collected TYPE: gauge","title":"line_number"},{"location":"reference/monitor/filesource/#line_qps","text":"# HELP current read line qps # TYPE loggie_filesource_line_qps gauge loggie_filesource_line_qps{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\"} 48 HELP: Indicates the number of lines read per second when a log file is currently being collected TYPE: gauge","title":"line_qps"},{"location":"reference/monitor/filewatcher/","text":"filewatcher listener \u00b6 Regularly check the file collection status and expose indicators, including file name, ackOffset, modification time, size, etc. Configuration \u00b6 field type required default description period time.Duration false 5m periodic inspection interval checkUnFinishedTimeout time.Duration false 24h The timeout period for checking whether the file has been collected. If it is detected that the last modification time of the file is before checkUnFinishedTimeout and the collection of the file has not been completed, it will be marked as unfinished in the metrics, which can be used to check whether there is any file that has not been collected for a long time. Metrics \u00b6 Global Level \u00b6 total_file_count \u00b6 # HELP file count total # TYPE loggie_filewatcher_total_file_count gauge loggie_filewatcher_total_file_count{} 20 HELP: Indicates the total number of active files currently detected by Loggie. TYPE: gauge inactive_file_count \u00b6 # HELP inactive file count # TYPE loggie_filewatcher_inactive_file_count gauge loggie_filewatcher_inactive_file_count{} 20 HELP: Indicates the total number of inactive files currently detected by Loggie. TYPE: gauge File Level \u00b6 The file level includes the following prometheus labels: LABELS: pipeline: Indicates the name of the pipeline where it is located. source: Indicates the name of the source where it is located. filename: indicates the file name. status: Indicates the file status. pending: File has been detected, may have been collected or be still being collected. unfinished: The modify time of the file to present has exceeded checkUnFinishedTimeout . ignored: File is ignored. Possibly over ignore_older . Since the period of timed scanning is 5 minutes by default, the following indicators may have a certain degree of delay. file_size \u00b6 # HELP file size # TYPE loggie_filewatcher_file_size gauge loggie_filewatcher_file_size{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\", status=\"pending\"} 2048 HELP: Indicates the total size of the file TYPE: gauge file_ack_offset \u00b6 # HELP file ack offset # TYPE loggie_filewatcher_file_ack_offset gauge loggie_filewatcher_file_ack_offset{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\", status=\"pending\"} 1024 HELP: Indicates the offset after the file is collected and the ack has been received, which can be understood as the offset of the file that has been successfully sent. TYPE: gauge file_last_modify \u00b6 # HELP file last modify timestamp # TYPE loggie_filewatcher_file_last_modify gauge loggie_filewatcher_file_last_modify{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\", status=\"pending\"} 2343214422 HELP: The last modification time of the file. TYPE: gauge","title":"filewatcher"},{"location":"reference/monitor/filewatcher/#filewatcher-listener","text":"Regularly check the file collection status and expose indicators, including file name, ackOffset, modification time, size, etc.","title":"filewatcher listener"},{"location":"reference/monitor/filewatcher/#configuration","text":"field type required default description period time.Duration false 5m periodic inspection interval checkUnFinishedTimeout time.Duration false 24h The timeout period for checking whether the file has been collected. If it is detected that the last modification time of the file is before checkUnFinishedTimeout and the collection of the file has not been completed, it will be marked as unfinished in the metrics, which can be used to check whether there is any file that has not been collected for a long time.","title":"Configuration"},{"location":"reference/monitor/filewatcher/#metrics","text":"","title":"Metrics"},{"location":"reference/monitor/filewatcher/#global-level","text":"","title":"Global Level"},{"location":"reference/monitor/filewatcher/#total_file_count","text":"# HELP file count total # TYPE loggie_filewatcher_total_file_count gauge loggie_filewatcher_total_file_count{} 20 HELP: Indicates the total number of active files currently detected by Loggie. TYPE: gauge","title":"total_file_count"},{"location":"reference/monitor/filewatcher/#inactive_file_count","text":"# HELP inactive file count # TYPE loggie_filewatcher_inactive_file_count gauge loggie_filewatcher_inactive_file_count{} 20 HELP: Indicates the total number of inactive files currently detected by Loggie. TYPE: gauge","title":"inactive_file_count"},{"location":"reference/monitor/filewatcher/#file-level","text":"The file level includes the following prometheus labels: LABELS: pipeline: Indicates the name of the pipeline where it is located. source: Indicates the name of the source where it is located. filename: indicates the file name. status: Indicates the file status. pending: File has been detected, may have been collected or be still being collected. unfinished: The modify time of the file to present has exceeded checkUnFinishedTimeout . ignored: File is ignored. Possibly over ignore_older . Since the period of timed scanning is 5 minutes by default, the following indicators may have a certain degree of delay.","title":"File Level"},{"location":"reference/monitor/filewatcher/#file_size","text":"# HELP file size # TYPE loggie_filewatcher_file_size gauge loggie_filewatcher_file_size{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\", status=\"pending\"} 2048 HELP: Indicates the total size of the file TYPE: gauge","title":"file_size"},{"location":"reference/monitor/filewatcher/#file_ack_offset","text":"# HELP file ack offset # TYPE loggie_filewatcher_file_ack_offset gauge loggie_filewatcher_file_ack_offset{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\", status=\"pending\"} 1024 HELP: Indicates the offset after the file is collected and the ack has been received, which can be understood as the offset of the file that has been successfully sent. TYPE: gauge","title":"file_ack_offset"},{"location":"reference/monitor/filewatcher/#file_last_modify","text":"# HELP file last modify timestamp # TYPE loggie_filewatcher_file_last_modify gauge loggie_filewatcher_file_last_modify{pipeline=\"xxx\", source=\"access\", filename=\"/var/log/a.log\", status=\"pending\"} 2343214422 HELP: The last modification time of the file. TYPE: gauge","title":"file_last_modify"},{"location":"reference/monitor/logalert/","text":"logAlert listener \u00b6 Used for sending log alerts. Configuration \u00b6 field type required default description alertManagerAddress string arrays true alertManager addresses bufferSize int false 100 The size of the buffer sent by the logAlert. Unit is the number of alert events. batchTimeout time.Duration false 10s The maximum sending time of each alarm batch. batchSize int false 10 The maximum number of alert included in each alarm batch.","title":"logAlert"},{"location":"reference/monitor/logalert/#logalert-listener","text":"Used for sending log alerts.","title":"logAlert listener"},{"location":"reference/monitor/logalert/#configuration","text":"field type required default description alertManagerAddress string arrays true alertManager addresses bufferSize int false 100 The size of the buffer sent by the logAlert. Unit is the number of alert events. batchTimeout time.Duration false 10s The maximum sending time of each alarm batch. batchSize int false 10 The maximum number of alert included in each alarm batch.","title":"Configuration"},{"location":"reference/monitor/overview/","text":"Monitor \u00b6 Event bus. All components can export their own metrics data, which are consumed and processed by listeners. Example monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~ logger \u00b6 Loggie supports outputting metrics to logs, which can be configured through logger. field type required default description logger.enabled bool false false whether to open logger.period time.Duration false 10s The time interval of indicator printing. It is recommended to extend the interval when the amount of data is large, such as 30s, 5m logger.pretty bool false false whether the printed indicator json needs to be displayed in a friendly manner. logger.additionLogEnabled bool false false Whether it is necessary to output the printed metrics to another log file separately. In the case of a large amount of data, if we configure the printing interval to be short, this switch can be turned on to avoid too many metrics log interference. logger.additionLogConfig false Log configuration parameters for extra output. logger.additionLogConfig.directory bool false /data/loggie/log log directory for extra output logger.additionLogConfig.maxBackups int false metrics.log The maximum number of files to keep in log rotation. The default is 3 logger.additionLogConfig.maxSize int false 1024 the maximum file size (in MB) when the log is rotated logger.additionLogConfig.maxAge int false 14 Maximum number of days to be retained for log rotation logger.additionLogConfig.timeFormat string false 2006-01-02 15:04:05 Time format for each line of log output listeners \u00b6 Indicates specific activated listeners. If the configuration is not filled in, it is closed. If the Listener is not activated, the related indicators will not be processed and exposed.","title":"Overview"},{"location":"reference/monitor/overview/#monitor","text":"Event bus. All components can export their own metrics data, which are consumed and processed by listeners. Example monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ sink : ~","title":"Monitor"},{"location":"reference/monitor/overview/#logger","text":"Loggie supports outputting metrics to logs, which can be configured through logger. field type required default description logger.enabled bool false false whether to open logger.period time.Duration false 10s The time interval of indicator printing. It is recommended to extend the interval when the amount of data is large, such as 30s, 5m logger.pretty bool false false whether the printed indicator json needs to be displayed in a friendly manner. logger.additionLogEnabled bool false false Whether it is necessary to output the printed metrics to another log file separately. In the case of a large amount of data, if we configure the printing interval to be short, this switch can be turned on to avoid too many metrics log interference. logger.additionLogConfig false Log configuration parameters for extra output. logger.additionLogConfig.directory bool false /data/loggie/log log directory for extra output logger.additionLogConfig.maxBackups int false metrics.log The maximum number of files to keep in log rotation. The default is 3 logger.additionLogConfig.maxSize int false 1024 the maximum file size (in MB) when the log is rotated logger.additionLogConfig.maxAge int false 14 Maximum number of days to be retained for log rotation logger.additionLogConfig.timeFormat string false 2006-01-02 15:04:05 Time format for each line of log output","title":"logger"},{"location":"reference/monitor/overview/#listeners","text":"Indicates specific activated listeners. If the configuration is not filled in, it is closed. If the Listener is not activated, the related indicators will not be processed and exposed.","title":"listeners"},{"location":"reference/monitor/queue/","text":"queue listener \u00b6 Configuration \u00b6 field type required default description period time.Duration false 10s The interval at which listener consumes and processes data. Metrics \u00b6 LABELS: pipeline: Indicates the name of the pipeline where it is located. type: the type of queue capacity \u00b6 # HELP queue capacity # TYPE loggie_queue_capacity gauge loggie_queue_capacity{pipeline=\"xxx\", type=\"channel\"} 2048 HELP: The capacity of the current queue TYPE: gauge size \u00b6 # HELP queue size # TYPE loggie_queue_size gauge loggie_queue_size{pipeline=\"xxx\", type=\"channel\"} 2048 HELP: The length of the current queue being used TYPE: gauge fill_percentage \u00b6 # HELP how full is queue # TYPE loggie_queue_fill_percentage gauge loggie_queue_fill_percentage{pipeline=\"xxx\", type=\"channel\"} 50 HELP: The percentage used by the current queue TYPE: gauge","title":"queue"},{"location":"reference/monitor/queue/#queue-listener","text":"","title":"queue listener"},{"location":"reference/monitor/queue/#configuration","text":"field type required default description period time.Duration false 10s The interval at which listener consumes and processes data.","title":"Configuration"},{"location":"reference/monitor/queue/#metrics","text":"LABELS: pipeline: Indicates the name of the pipeline where it is located. type: the type of queue","title":"Metrics"},{"location":"reference/monitor/queue/#capacity","text":"# HELP queue capacity # TYPE loggie_queue_capacity gauge loggie_queue_capacity{pipeline=\"xxx\", type=\"channel\"} 2048 HELP: The capacity of the current queue TYPE: gauge","title":"capacity"},{"location":"reference/monitor/queue/#size","text":"# HELP queue size # TYPE loggie_queue_size gauge loggie_queue_size{pipeline=\"xxx\", type=\"channel\"} 2048 HELP: The length of the current queue being used TYPE: gauge","title":"size"},{"location":"reference/monitor/queue/#fill_percentage","text":"# HELP how full is queue # TYPE loggie_queue_fill_percentage gauge loggie_queue_fill_percentage{pipeline=\"xxx\", type=\"channel\"} 50 HELP: The percentage used by the current queue TYPE: gauge","title":"fill_percentage"},{"location":"reference/monitor/reload/","text":"reload listener \u00b6 Reload indicator, the total number of reloads. Metrics \u00b6 total \u00b6 # HELP Loggie reload total count # TYPE loggie_reload_total gauge loggie_reload_total{} 10 HELP: Indicates the total number of current reloads. TYPE: counter","title":"reload"},{"location":"reference/monitor/reload/#reload-listener","text":"Reload indicator, the total number of reloads.","title":"reload listener"},{"location":"reference/monitor/reload/#metrics","text":"","title":"Metrics"},{"location":"reference/monitor/reload/#total","text":"# HELP Loggie reload total count # TYPE loggie_reload_total gauge loggie_reload_total{} 10 HELP: Indicates the total number of current reloads. TYPE: counter","title":"total"},{"location":"reference/monitor/sink/","text":"sink listener \u00b6 The monitoring of sink indicates the count of successful and failed events and event QPS, etc. Configuration \u00b6 field type required default description period time.Duration false 10s The interval at which listener consumes and processes data. Metrics \u00b6 LABELS: pipeline: Indicates the name of the pipeline where it is located. source: Indicates the name of the source where it is located. success_event \u00b6 # HELP send event success count # TYPE loggie_sink_success_event gauge loggie_sink_success_event{pipeline=\"xxx\", source=\"access\"} 2048 HELP: Indicates count of successful event sent during the past period of time. TYPE: gauge failed_event \u00b6 # HELP send event failed count # TYPE loggie_sink_failed_event gauge loggie_sink_failed_event{pipeline=\"xxx\", source=\"access\"} 2048 HELP: Indicates count of failed event sent during the past period of time. TYPE: gauge event_qps \u00b6 # HELP send success event failed count # TYPE loggie_sink_event_qps gauge loggie_sink_event_qps{pipeline=\"xxx\", source=\"access\"} 2048 HELP: Indicates count of event QPS during the past period of time. TYPE: gauge","title":"sink"},{"location":"reference/monitor/sink/#sink-listener","text":"The monitoring of sink indicates the count of successful and failed events and event QPS, etc.","title":"sink listener"},{"location":"reference/monitor/sink/#configuration","text":"field type required default description period time.Duration false 10s The interval at which listener consumes and processes data.","title":"Configuration"},{"location":"reference/monitor/sink/#metrics","text":"LABELS: pipeline: Indicates the name of the pipeline where it is located. source: Indicates the name of the source where it is located.","title":"Metrics"},{"location":"reference/monitor/sink/#success_event","text":"# HELP send event success count # TYPE loggie_sink_success_event gauge loggie_sink_success_event{pipeline=\"xxx\", source=\"access\"} 2048 HELP: Indicates count of successful event sent during the past period of time. TYPE: gauge","title":"success_event"},{"location":"reference/monitor/sink/#failed_event","text":"# HELP send event failed count # TYPE loggie_sink_failed_event gauge loggie_sink_failed_event{pipeline=\"xxx\", source=\"access\"} 2048 HELP: Indicates count of failed event sent during the past period of time. TYPE: gauge","title":"failed_event"},{"location":"reference/monitor/sink/#event_qps","text":"# HELP send success event failed count # TYPE loggie_sink_event_qps gauge loggie_sink_event_qps{pipeline=\"xxx\", source=\"access\"} 2048 HELP: Indicates count of event QPS during the past period of time. TYPE: gauge","title":"event_qps"},{"location":"reference/monitor/sys/","text":"sys listener \u00b6 Loggie's own CPU and Memory metrics. Configuration \u00b6 field type required default description period time.Duration false 10s The interval at which listener consumes and processes data. Metrics \u00b6 cpu_percent \u00b6 # HELP loggie_sys_cpu_percent Loggie cpu percent # TYPE loggie_sys_cpu_percent gauge loggie_sys_cpu_percent 0.37 HELP: Loggie's CPU usage. TYPE: gauge mem_rss \u00b6 # HELP loggie_sys_mem_rss Loggie memory rss bytes # TYPE loggie_sys_mem_rss gauge loggie_sys_mem_rss 2.5853952e+07 HELP: Loggie's memory usage in bytes. TYPE: gauge","title":"sys"},{"location":"reference/monitor/sys/#sys-listener","text":"Loggie's own CPU and Memory metrics.","title":"sys listener"},{"location":"reference/monitor/sys/#configuration","text":"field type required default description period time.Duration false 10s The interval at which listener consumes and processes data.","title":"Configuration"},{"location":"reference/monitor/sys/#metrics","text":"","title":"Metrics"},{"location":"reference/monitor/sys/#cpu_percent","text":"# HELP loggie_sys_cpu_percent Loggie cpu percent # TYPE loggie_sys_cpu_percent gauge loggie_sys_cpu_percent 0.37 HELP: Loggie's CPU usage. TYPE: gauge","title":"cpu_percent"},{"location":"reference/monitor/sys/#mem_rss","text":"# HELP loggie_sys_mem_rss Loggie memory rss bytes # TYPE loggie_sys_mem_rss gauge loggie_sys_mem_rss 2.5853952e+07 HELP: Loggie's memory usage in bytes. TYPE: gauge","title":"mem_rss"},{"location":"reference/pipelines/interceptor/addk8smeta/","text":"addK8sMeta \u00b6 Used to get from some fields in the event (such as the path of the log file). Source interceptor. pod.uid namespace and pod.name container.id With any one of the above three kinds of index information, Loggie can query the specific Pod according to the index, and add additional kubernetes meta-information such as ${node.name} , ${namespace} , ${pod.uid} , ${pod.name} , etc. to the event for subsequent analysis and processing. Example interceptors : - type : addK8sMeta pattern : \"/var/log/${pod.uid}/${pod.name}/\" addFields : nodename : \"${node.name}\" namespace : \"${namespace}\" podname : \"${pod.name}\" pattern \u00b6 \u5b57\u6bb5 \u7c7b\u578b \u662f\u5426\u5fc5\u586b \u9ed8\u8ba4\u503c \u542b\u4e49 pattern string true Matching model for extracting fields Must contain one of: pod.uid namespace and pod.name container.id For example: /var/log/${pod.uid}/${pod.name}/ patternFields \u00b6 \u5b57\u6bb5 \u7c7b\u578b \u662f\u5426\u5fc5\u586b \u9ed8\u8ba4\u503c \u542b\u4e49 patternFields string false By default, the filename in the system field will be obtained from the event. In this case, you need to use the file source Fields for the extracting pattern from the event fieldsName \u00b6 \u5b57\u6bb5 \u7c7b\u578b \u662f\u5426\u5fc5\u586b \u9ed8\u8ba4\u503c \u542b\u4e49 fieldsName string false kubernetes Fields to add meta information addFields \u00b6 \u5b57\u6bb5 \u7c7b\u578b \u662f\u5426\u5fc5\u586b \u9ed8\u8ba4\u503c \u542b\u4e49 addFields map false Meta information to be added The currently supported meta-information fields are: ${cluster} : Cluster information. discovery.kubernetes.cluster in system configuration. ${node.name} ${namespace} ${workload.kind} : Deployment/StatefulSet/DaemonSet/Job, etc. ${workload.name} : the name of the workload ${pod.uid} ${pod.name}","title":"addK8sMeta"},{"location":"reference/pipelines/interceptor/addk8smeta/#addk8smeta","text":"Used to get from some fields in the event (such as the path of the log file). Source interceptor. pod.uid namespace and pod.name container.id With any one of the above three kinds of index information, Loggie can query the specific Pod according to the index, and add additional kubernetes meta-information such as ${node.name} , ${namespace} , ${pod.uid} , ${pod.name} , etc. to the event for subsequent analysis and processing. Example interceptors : - type : addK8sMeta pattern : \"/var/log/${pod.uid}/${pod.name}/\" addFields : nodename : \"${node.name}\" namespace : \"${namespace}\" podname : \"${pod.name}\"","title":"addK8sMeta"},{"location":"reference/pipelines/interceptor/addk8smeta/#pattern","text":"\u5b57\u6bb5 \u7c7b\u578b \u662f\u5426\u5fc5\u586b \u9ed8\u8ba4\u503c \u542b\u4e49 pattern string true Matching model for extracting fields Must contain one of: pod.uid namespace and pod.name container.id For example: /var/log/${pod.uid}/${pod.name}/","title":"pattern"},{"location":"reference/pipelines/interceptor/addk8smeta/#patternfields","text":"\u5b57\u6bb5 \u7c7b\u578b \u662f\u5426\u5fc5\u586b \u9ed8\u8ba4\u503c \u542b\u4e49 patternFields string false By default, the filename in the system field will be obtained from the event. In this case, you need to use the file source Fields for the extracting pattern from the event","title":"patternFields"},{"location":"reference/pipelines/interceptor/addk8smeta/#fieldsname","text":"\u5b57\u6bb5 \u7c7b\u578b \u662f\u5426\u5fc5\u586b \u9ed8\u8ba4\u503c \u542b\u4e49 fieldsName string false kubernetes Fields to add meta information","title":"fieldsName"},{"location":"reference/pipelines/interceptor/addk8smeta/#addfields","text":"\u5b57\u6bb5 \u7c7b\u578b \u662f\u5426\u5fc5\u586b \u9ed8\u8ba4\u503c \u542b\u4e49 addFields map false Meta information to be added The currently supported meta-information fields are: ${cluster} : Cluster information. discovery.kubernetes.cluster in system configuration. ${node.name} ${namespace} ${workload.kind} : Deployment/StatefulSet/DaemonSet/Job, etc. ${workload.name} : the name of the workload ${pod.uid} ${pod.name}","title":"addFields"},{"location":"reference/pipelines/interceptor/limit/","text":"rateLimit \u00b6 Used for log current limiting. Source interceptor\u3002 Example interceptors : - type : rateLimit qps : 4000 qps \u00b6 field type required default description qps int false 2048 Current limiting qps","title":"rateLimit"},{"location":"reference/pipelines/interceptor/limit/#ratelimit","text":"Used for log current limiting. Source interceptor\u3002 Example interceptors : - type : rateLimit qps : 4000","title":"rateLimit"},{"location":"reference/pipelines/interceptor/limit/#qps","text":"field type required default description qps int false 2048 Current limiting qps","title":"qps"},{"location":"reference/pipelines/interceptor/logalert/","text":"logAlert \u00b6 Used for log detection. Source interceptor. Please refer to Log Alarm for usage examples. Example interceptors : - type : logAlert matcher : contains : [ \"error\" , \"err\" ] matcher \u00b6 field type required default description matcher.contains string array false check whether log contains string matcher.regexp string array false check whether log matches regexp pattern matcher.target string false body the field of log data to check labels \u00b6 field type required default description labels.from string array false additional labels from the header. Fill in the specific key name in the header.","title":"logAlert"},{"location":"reference/pipelines/interceptor/logalert/#logalert","text":"Used for log detection. Source interceptor. Please refer to Log Alarm for usage examples. Example interceptors : - type : logAlert matcher : contains : [ \"error\" , \"err\" ]","title":"logAlert"},{"location":"reference/pipelines/interceptor/logalert/#matcher","text":"field type required default description matcher.contains string array false check whether log contains string matcher.regexp string array false check whether log matches regexp pattern matcher.target string false body the field of log data to check","title":"matcher"},{"location":"reference/pipelines/interceptor/logalert/#labels","text":"field type required default description labels.from string array false additional labels from the header. Fill in the specific key name in the header.","title":"labels"},{"location":"reference/pipelines/interceptor/maxbytes/","text":"maxbytes \u00b6 Used to limit the size of the original single-line log to prevent the large amount of single-line log data from affecting the memory and stability of Loggie. Source interceptor which is Built-in and loaded by default. Example interceptors : - type : maxbytes maxBytes : 102400 maxBytes \u00b6 field type required default description maxBytes int false The maximum number of bytes in a single line. The excess part will be discarded.","title":"maxbytes"},{"location":"reference/pipelines/interceptor/maxbytes/#maxbytes","text":"Used to limit the size of the original single-line log to prevent the large amount of single-line log data from affecting the memory and stability of Loggie. Source interceptor which is Built-in and loaded by default. Example interceptors : - type : maxbytes maxBytes : 102400","title":"maxbytes"},{"location":"reference/pipelines/interceptor/maxbytes/#maxbytes_1","text":"field type required default description maxBytes int false The maximum number of bytes in a single line. The excess part will be discarded.","title":"maxBytes"},{"location":"reference/pipelines/interceptor/metrics/","text":"metric \u00b6 Used to sample metics for data transmission which are comsumed by listeners in monitor eventbus. Sink interceptor which is Built-in and loaded by default. Example interceptors : - type : metric","title":"metrics"},{"location":"reference/pipelines/interceptor/metrics/#metric","text":"Used to sample metics for data transmission which are comsumed by listeners in monitor eventbus. Sink interceptor which is Built-in and loaded by default. Example interceptors : - type : metric","title":"metric"},{"location":"reference/pipelines/interceptor/normalize/","text":"normalize \u00b6 (The interceptor will no longer be maintained in the future, and it is recommended to replace it with a transformer) Used for log segmentation. Source interceptor. Can be specified to be used only by certain sources. processors \u00b6 field type required default description processors array true none List of all processors The configured processors will be executed in order. Tips Loggie supports using a.b to refer to nested fields. For example: { \"fields\" : { \"hello\" : \"world\" } } The following processor can use fields.hello to specific hello: world inside fields . addMeta \u00b6 By default, Loggie does not add any internal system information to the raw data. By addMeta, the built-in fields of the system can be added and sent to the downstream. Note Please note that configuring addMeta in a pipeline will only affect all data sent by the pipeline. If you need the interceptor to take effect globally, please configure normalize.addMeta in defaults . loggie : defaults : interceptors : - type : normalize name : global processors : - addMeta : ~ field type required default description target string false meta The field name of the system built-in field added to the event regex \u00b6 Regularly extract the specified field. field type required default description regex.pattern string true none regular parsing rules regex.target string false body target field of regular parsing regex.ignoreError bool false false whether to ignore errors Example interceptors : - type : normalize processors : - regex : pattern : '(?<ip>\\S+) (?<id>\\S+) (?<u>\\S+) (?<time>\\[.*?\\]) (?<url>\\\".*?\\\") (?<status>\\S+) (?<size>\\S+)' Using the above regular expression, the log of the following example can be converted from: 10.244.0.1 - - [13/Dec/2021:12:40:48 +0000] \"GET / HTTP/1.1\" 404 683 to: \"ip\": \"10.244.0.1\", \"id\": \"-\", \"u\": \"-\", \"time\": \"[13/Dec/2021:12:40:48 +0000]\", \"url\": \"\\\"GET / HTTP/1.1\\\"\", \"status\": \"404\", \"size\": \"683\" When configuring the specific configuration, it is recommended to use some regular debugging tools ( https://regex101.com/ ) to verify whether it works. jsonDecode \u00b6 Parse the specified field json. field type required default description jsonDecode.target string false body The target field of json decode jsonDecode.ignoreError bool false false whether to ignore errors Example interceptors : - type : normalize processors : - jsonDecode : ~ split \u00b6 Extract the specified field with a delimiter. field type required default description split.target string false body target field for split split.separator string true none delimiter split.max int false -1 The maximum number of fields obtained by dividing by the delimiter split.keys string array true none The key corresponding to the segmented field split.ignoreError bool false false whether to ignore errors Example base interceptors : - type : normalize processors : - split : separator : '|' keys : [ \"time\" , \"order\" , \"service\" , \"price\" ] Using the above split configuration can convert the log from: 2021-08-08|U12345|storeCenter|13.14 to: \"time\": \"2021-08-08\" \"order\": \"U12345\" \"service\": \"storeCenter\" \"price\": 13.14 max interceptors : - type : normalize processors : - split : separator : ' ' max : 2 keys : [ \"time\" , \"content\" ] By adding max , you can control the fields that are split at most. For example the following log: 2021-08-08 U12345 storeCenter 13.14 It can be converted by the above configuration as: \"time\": \"2021-08-08\" \"content\": \"U12345 storeCenter 13.14\" drop \u00b6 Discards the specified field. field type required default description drop.targets string array true none field dropped Example interceptors : - type : normalize processors : - drop : targets : [ \"id\" , \"body\" ] rename \u00b6 Rename the specified field. field type required default description rename.convert array true none rename.convert[n].from string true none target of rename rename.convert[n].to string true none new name Example interceptors : - type : normalize processors : - rename : convert : - from : \"hello\" to : \"world\" add \u00b6 Add fields. field type required default description add.fields map true true new key:value Example interceptors : - type : normalize processors : - add : fields : hello : world convert \u00b6 Field type conversion. field type required default description convert.convert array true none convert.convert[n].from string true none field to be converted convert.convert[n].to string true none The converted type, which can be: \"bool\", \"integer\", \"float\" Example interceptors : - type : normalize processors : - convert : convert : - from : count to : float copy \u00b6 Field copy. field type required default description copy.convert array true none copy.convert[n].from string true none field to be copied copy.convert[n].to string true none new name Example interceptors : - type : normalize processors : - copy : convert : - from : hello to : world underRoot \u00b6 Put all the fields in the key:value outermost layer of the event. field type required default description underRoot.keys string array true none field that requires underRoot Example interceptors : - type : normalize processors : - underRoot : keys : [ \"fields\" ] timestamp \u00b6 Convert time format. field type required default description timestamp.convert array true none timestamp.convert[n].from string true none field to be converted timestamp.convert[n].fromLayout string true none specify the time format of the field (golang form) timestamp.convert[n].toLayout string true none the converted time format (golang form). can also be unix and unix_ms . timestamp.convert[n].toType string false none field type of converted time timestamp.convert[n].local bool false false whether to convert the parsed time to the current time zone Example interceptors : - type : normalize processors : - timestamp : convert : - from : logtime fromLayout : \"2006-01-02T15:04:05Z07:00\" toLayout : \"unix\" For time in golang form, please refer to: const ( Layout = \"01/02 03:04:05PM '06 -0700\" // The reference time, in numerical order. ANSIC = \"Mon Jan _2 15:04:05 2006\" UnixDate = \"Mon Jan _2 15:04:05 MST 2006\" RubyDate = \"Mon Jan 02 15:04:05 -0700 2006\" RFC822 = \"02 Jan 06 15:04 MST\" RFC822Z = \"02 Jan 06 15:04 -0700\" // RFC822 with numeric zone RFC850 = \"Monday, 02-Jan-06 15:04:05 MST\" RFC1123 = \"Mon, 02 Jan 2006 15:04:05 MST\" RFC1123Z = \"Mon, 02 Jan 2006 15:04:05 -0700\" // RFC1123 with numeric zone RFC3339 = \"2006-01-02T15:04:05Z07:00\" RFC3339Nano = \"2006-01-02T15:04:05.999999999Z07:00\" Kitchen = \"3:04PM\" // Handy time stamps. Stamp = \"Jan _2 15:04:05\" StampMilli = \"Jan _2 15:04:05.000\" StampMicro = \"Jan _2 15:04:05.000000\" StampNano = \"Jan _2 15:04:05.000000000\" ) It can also be modified according to requirements. fmt \u00b6 Reformat field contents. Combination and formatting according to other field contents are supported. field type required default description fmt.fields map true none The key represents the field name that needs to be formatted, and the value is the content that needs to be formatted. ${} can be used to express the value of a field. Example interceptors : - type : normalize processors : - fmt : fields : d : new-${a.b}-${c}","title":"normalize"},{"location":"reference/pipelines/interceptor/normalize/#normalize","text":"(The interceptor will no longer be maintained in the future, and it is recommended to replace it with a transformer) Used for log segmentation. Source interceptor. Can be specified to be used only by certain sources.","title":"normalize"},{"location":"reference/pipelines/interceptor/normalize/#processors","text":"field type required default description processors array true none List of all processors The configured processors will be executed in order. Tips Loggie supports using a.b to refer to nested fields. For example: { \"fields\" : { \"hello\" : \"world\" } } The following processor can use fields.hello to specific hello: world inside fields .","title":"processors"},{"location":"reference/pipelines/interceptor/normalize/#addmeta","text":"By default, Loggie does not add any internal system information to the raw data. By addMeta, the built-in fields of the system can be added and sent to the downstream. Note Please note that configuring addMeta in a pipeline will only affect all data sent by the pipeline. If you need the interceptor to take effect globally, please configure normalize.addMeta in defaults . loggie : defaults : interceptors : - type : normalize name : global processors : - addMeta : ~ field type required default description target string false meta The field name of the system built-in field added to the event","title":"addMeta"},{"location":"reference/pipelines/interceptor/normalize/#regex","text":"Regularly extract the specified field. field type required default description regex.pattern string true none regular parsing rules regex.target string false body target field of regular parsing regex.ignoreError bool false false whether to ignore errors Example interceptors : - type : normalize processors : - regex : pattern : '(?<ip>\\S+) (?<id>\\S+) (?<u>\\S+) (?<time>\\[.*?\\]) (?<url>\\\".*?\\\") (?<status>\\S+) (?<size>\\S+)' Using the above regular expression, the log of the following example can be converted from: 10.244.0.1 - - [13/Dec/2021:12:40:48 +0000] \"GET / HTTP/1.1\" 404 683 to: \"ip\": \"10.244.0.1\", \"id\": \"-\", \"u\": \"-\", \"time\": \"[13/Dec/2021:12:40:48 +0000]\", \"url\": \"\\\"GET / HTTP/1.1\\\"\", \"status\": \"404\", \"size\": \"683\" When configuring the specific configuration, it is recommended to use some regular debugging tools ( https://regex101.com/ ) to verify whether it works.","title":"regex"},{"location":"reference/pipelines/interceptor/normalize/#jsondecode","text":"Parse the specified field json. field type required default description jsonDecode.target string false body The target field of json decode jsonDecode.ignoreError bool false false whether to ignore errors Example interceptors : - type : normalize processors : - jsonDecode : ~","title":"jsonDecode"},{"location":"reference/pipelines/interceptor/normalize/#split","text":"Extract the specified field with a delimiter. field type required default description split.target string false body target field for split split.separator string true none delimiter split.max int false -1 The maximum number of fields obtained by dividing by the delimiter split.keys string array true none The key corresponding to the segmented field split.ignoreError bool false false whether to ignore errors Example base interceptors : - type : normalize processors : - split : separator : '|' keys : [ \"time\" , \"order\" , \"service\" , \"price\" ] Using the above split configuration can convert the log from: 2021-08-08|U12345|storeCenter|13.14 to: \"time\": \"2021-08-08\" \"order\": \"U12345\" \"service\": \"storeCenter\" \"price\": 13.14 max interceptors : - type : normalize processors : - split : separator : ' ' max : 2 keys : [ \"time\" , \"content\" ] By adding max , you can control the fields that are split at most. For example the following log: 2021-08-08 U12345 storeCenter 13.14 It can be converted by the above configuration as: \"time\": \"2021-08-08\" \"content\": \"U12345 storeCenter 13.14\"","title":"split"},{"location":"reference/pipelines/interceptor/normalize/#drop","text":"Discards the specified field. field type required default description drop.targets string array true none field dropped Example interceptors : - type : normalize processors : - drop : targets : [ \"id\" , \"body\" ]","title":"drop"},{"location":"reference/pipelines/interceptor/normalize/#rename","text":"Rename the specified field. field type required default description rename.convert array true none rename.convert[n].from string true none target of rename rename.convert[n].to string true none new name Example interceptors : - type : normalize processors : - rename : convert : - from : \"hello\" to : \"world\"","title":"rename"},{"location":"reference/pipelines/interceptor/normalize/#add","text":"Add fields. field type required default description add.fields map true true new key:value Example interceptors : - type : normalize processors : - add : fields : hello : world","title":"add"},{"location":"reference/pipelines/interceptor/normalize/#convert","text":"Field type conversion. field type required default description convert.convert array true none convert.convert[n].from string true none field to be converted convert.convert[n].to string true none The converted type, which can be: \"bool\", \"integer\", \"float\" Example interceptors : - type : normalize processors : - convert : convert : - from : count to : float","title":"convert"},{"location":"reference/pipelines/interceptor/normalize/#copy","text":"Field copy. field type required default description copy.convert array true none copy.convert[n].from string true none field to be copied copy.convert[n].to string true none new name Example interceptors : - type : normalize processors : - copy : convert : - from : hello to : world","title":"copy"},{"location":"reference/pipelines/interceptor/normalize/#underroot","text":"Put all the fields in the key:value outermost layer of the event. field type required default description underRoot.keys string array true none field that requires underRoot Example interceptors : - type : normalize processors : - underRoot : keys : [ \"fields\" ]","title":"underRoot"},{"location":"reference/pipelines/interceptor/normalize/#timestamp","text":"Convert time format. field type required default description timestamp.convert array true none timestamp.convert[n].from string true none field to be converted timestamp.convert[n].fromLayout string true none specify the time format of the field (golang form) timestamp.convert[n].toLayout string true none the converted time format (golang form). can also be unix and unix_ms . timestamp.convert[n].toType string false none field type of converted time timestamp.convert[n].local bool false false whether to convert the parsed time to the current time zone Example interceptors : - type : normalize processors : - timestamp : convert : - from : logtime fromLayout : \"2006-01-02T15:04:05Z07:00\" toLayout : \"unix\" For time in golang form, please refer to: const ( Layout = \"01/02 03:04:05PM '06 -0700\" // The reference time, in numerical order. ANSIC = \"Mon Jan _2 15:04:05 2006\" UnixDate = \"Mon Jan _2 15:04:05 MST 2006\" RubyDate = \"Mon Jan 02 15:04:05 -0700 2006\" RFC822 = \"02 Jan 06 15:04 MST\" RFC822Z = \"02 Jan 06 15:04 -0700\" // RFC822 with numeric zone RFC850 = \"Monday, 02-Jan-06 15:04:05 MST\" RFC1123 = \"Mon, 02 Jan 2006 15:04:05 MST\" RFC1123Z = \"Mon, 02 Jan 2006 15:04:05 -0700\" // RFC1123 with numeric zone RFC3339 = \"2006-01-02T15:04:05Z07:00\" RFC3339Nano = \"2006-01-02T15:04:05.999999999Z07:00\" Kitchen = \"3:04PM\" // Handy time stamps. Stamp = \"Jan _2 15:04:05\" StampMilli = \"Jan _2 15:04:05.000\" StampMicro = \"Jan _2 15:04:05.000000\" StampNano = \"Jan _2 15:04:05.000000000\" ) It can also be modified according to requirements.","title":"timestamp"},{"location":"reference/pipelines/interceptor/normalize/#fmt","text":"Reformat field contents. Combination and formatting according to other field contents are supported. field type required default description fmt.fields map true none The key represents the field name that needs to be formatted, and the value is the content that needs to be formatted. ${} can be used to express the value of a field. Example interceptors : - type : normalize processors : - fmt : fields : d : new-${a.b}-${c}","title":"fmt"},{"location":"reference/pipelines/interceptor/overview/","text":"Overview \u00b6 The interceptors field is an array. A Pipeline can have multiple interceptor configurations. Currently, interceptors are divided into two types: source interceptor: running in the process of source sending data to queue source -> source interceptor -> queue . sink interceptor: running in the process of queue to sink queue -> sink interceptor -> sink . An interceptor belongs to only one of them. Most of the components are source interceptor, which supports configuring belongTo to be used by some sources. A few general properties such as retry interceptor are sink interceptor. Interceptor Common Configuration \u00b6 enabled \u00b6 field type required default description enabled bool false true whether to enable the interceptor name \u00b6 field type required default description name string false Indicates the name of the interceptor. When the same type of interceptor is configured in the pipeline, it is required to distinguish the identifier. belongTo \u00b6 field type required default description belongTo string array false Only the source interceptor can use to specify which sources the interceptor is used by. order \u00b6 field type required default description order int false The order of the interceptor","title":"Overview"},{"location":"reference/pipelines/interceptor/overview/#overview","text":"The interceptors field is an array. A Pipeline can have multiple interceptor configurations. Currently, interceptors are divided into two types: source interceptor: running in the process of source sending data to queue source -> source interceptor -> queue . sink interceptor: running in the process of queue to sink queue -> sink interceptor -> sink . An interceptor belongs to only one of them. Most of the components are source interceptor, which supports configuring belongTo to be used by some sources. A few general properties such as retry interceptor are sink interceptor.","title":"Overview"},{"location":"reference/pipelines/interceptor/overview/#interceptor-common-configuration","text":"","title":"Interceptor Common Configuration"},{"location":"reference/pipelines/interceptor/overview/#enabled","text":"field type required default description enabled bool false true whether to enable the interceptor","title":"enabled"},{"location":"reference/pipelines/interceptor/overview/#name","text":"field type required default description name string false Indicates the name of the interceptor. When the same type of interceptor is configured in the pipeline, it is required to distinguish the identifier.","title":"name"},{"location":"reference/pipelines/interceptor/overview/#belongto","text":"field type required default description belongTo string array false Only the source interceptor can use to specify which sources the interceptor is used by.","title":"belongTo"},{"location":"reference/pipelines/interceptor/overview/#order","text":"field type required default description order int false The order of the interceptor","title":"order"},{"location":"reference/pipelines/interceptor/retry/","text":"retry \u00b6 Used to retry when sending to downstream fails. Sink interceptor which is Built-in and loaded by default. Example interceptors : - type : retry retryMaxCount \u00b6 field type required default description retryMaxCount int false 0 maximum number of retries","title":"retry"},{"location":"reference/pipelines/interceptor/retry/#retry","text":"Used to retry when sending to downstream fails. Sink interceptor which is Built-in and loaded by default. Example interceptors : - type : retry","title":"retry"},{"location":"reference/pipelines/interceptor/retry/#retrymaxcount","text":"field type required default description retryMaxCount int false 0 maximum number of retries","title":"retryMaxCount"},{"location":"reference/pipelines/interceptor/schema/","text":"schema \u00b6 Used to convert log format. Source interceptor. Use Cases \u00b6 For some log scenarios, there may be some differences in the log format we require. These differences are mainly in the time field, body field, and so on. By default, Loggie will only put the raw data collected or received by the source into the body field and send it in the simplest way: { \"body\" : \"this is raw data\" } But, sometimes, we need to: add time field modify body and other fields Example add @timestamp , modify body to message interceptors : - type : schema addMeta : timestamp : key : \"@timestamp\" remap : body : key : message event converted: { \"message\" : \"this is raw data\" \"@timestamp\" : \"2022-08-30T06:58:49.545Z\" , } add timestamp \uff0cmodify timezone and format, modift body to log interceptors : - type : schema addMeta : timestamp : key : \"_timestamp_\" location : Local layout : 2006-01-02T15:04:05Z07:00 remap : body : key : _log_ Configuration \u00b6 addMeta \u00b6 field type required default description addMeta false add meta info timestamp \u00b6 field type required default description addMeta.timestamp false add time field\uff08time when log is collected\uff09 addMeta.timestamp.key string true key for system time addMeta.timestamp.location string false UTC add time zone. Local supported addMeta.timestamp.layout string false \"2006-01-02T15:04:05.000Z\" golang time format layout, refer to https://go.dev/src/time/format.go pipelineName \u00b6 field type required default description addMeta.pipelineName false add pipelineName into event addMeta.pipelineName.key string true field key for pipelineName Example interceptors : - type : schema addMeta : pipelineName : key : pipeline event converted: { \"pipeline\" : \"demo\" ... } sourceName \u00b6 field type required default description addMeta.sourceName false add sourceName into event addMeta.sourceName.key string true field key for sourceName Example interceptors : - type : schema addMeta : sourceName : key : source event converted: { \"source\" : \"local\" ... } remap \u00b6 field type required default description remap map false convert field. rename supported currently. remap.[originKey] string false original key remap.[originKey].key string false converted key Example interceptors : - type : schema remap : body : key : msg state : key : meta original event: { \"body\" : \"this is log\" \"state\" : \"ok\" , } event converted: { \"msg\" : \"this is log\" \"meta\" : \"ok\" , }","title":"schema"},{"location":"reference/pipelines/interceptor/schema/#schema","text":"Used to convert log format. Source interceptor.","title":"schema"},{"location":"reference/pipelines/interceptor/schema/#use-cases","text":"For some log scenarios, there may be some differences in the log format we require. These differences are mainly in the time field, body field, and so on. By default, Loggie will only put the raw data collected or received by the source into the body field and send it in the simplest way: { \"body\" : \"this is raw data\" } But, sometimes, we need to: add time field modify body and other fields Example add @timestamp , modify body to message interceptors : - type : schema addMeta : timestamp : key : \"@timestamp\" remap : body : key : message event converted: { \"message\" : \"this is raw data\" \"@timestamp\" : \"2022-08-30T06:58:49.545Z\" , } add timestamp \uff0cmodify timezone and format, modift body to log interceptors : - type : schema addMeta : timestamp : key : \"_timestamp_\" location : Local layout : 2006-01-02T15:04:05Z07:00 remap : body : key : _log_","title":"Use Cases"},{"location":"reference/pipelines/interceptor/schema/#configuration","text":"","title":"Configuration"},{"location":"reference/pipelines/interceptor/schema/#addmeta","text":"field type required default description addMeta false add meta info","title":"addMeta"},{"location":"reference/pipelines/interceptor/schema/#timestamp","text":"field type required default description addMeta.timestamp false add time field\uff08time when log is collected\uff09 addMeta.timestamp.key string true key for system time addMeta.timestamp.location string false UTC add time zone. Local supported addMeta.timestamp.layout string false \"2006-01-02T15:04:05.000Z\" golang time format layout, refer to https://go.dev/src/time/format.go","title":"timestamp"},{"location":"reference/pipelines/interceptor/schema/#pipelinename","text":"field type required default description addMeta.pipelineName false add pipelineName into event addMeta.pipelineName.key string true field key for pipelineName Example interceptors : - type : schema addMeta : pipelineName : key : pipeline event converted: { \"pipeline\" : \"demo\" ... }","title":"pipelineName"},{"location":"reference/pipelines/interceptor/schema/#sourcename","text":"field type required default description addMeta.sourceName false add sourceName into event addMeta.sourceName.key string true field key for sourceName Example interceptors : - type : schema addMeta : sourceName : key : source event converted: { \"source\" : \"local\" ... }","title":"sourceName"},{"location":"reference/pipelines/interceptor/schema/#remap","text":"field type required default description remap map false convert field. rename supported currently. remap.[originKey] string false original key remap.[originKey].key string false converted key Example interceptors : - type : schema remap : body : key : msg state : key : meta original event: { \"body\" : \"this is log\" \"state\" : \"ok\" , } event converted: { \"msg\" : \"this is log\" \"meta\" : \"ok\" , }","title":"remap"},{"location":"reference/pipelines/interceptor/transformer/","text":"transformer \u00b6 Used to perform functional data processing conditional judgement. Source interceptor. Use Cases \u00b6 Extracts the log level and drops the DEBUG log. Check whether the log is in json form when logs in both json and plain form are mixed. Add different topic topics according to the status code in log. ... See reference . Usage \u00b6 The transformer will execute all actions in order in the configured actions. The action is similar to the function, and parameters can be written. The parameters are generally fields in the event. Each action may also include additional fields. Take the following regex(body) for example, body is the parameter of regex, and pattern is an extra field. interceptors : - type : transformer actions : - action : regex(body) pattern : ^(?P<time>[^ ^Z]+Z) (?P<level>[^ ]*) (?P<log>.*)$ - action : add(topic, common) In addition, action also supports if-then-else : - if : <condition> then : - action : funcA() else : - action : funcB() Condition is also in the form of a function. interceptors : - type : transformer actions : - if : equal(status, 404) then : - action : add(topic, not_found) - action : return() action \u00b6 Common fields \u00b6 ignoreError: Indicates whether to ignore and print errors during the processing of this action. Example - type : transformer actions : - action : regex(body) pattern : (?<ip>\\S+) (?<id>\\S+) (?<u>\\S+) (?<time>\\[.*?\\]) (?<url>\\\".*?\\\") (?<status>\\S+) (?<size>\\S+) ignoreError : true The ignoreError here is set to true, which means that the regular matching error will be ignored, and subsequent actions will continue to be executed. add(key, value) \u00b6 Add additional key:value to the event. Example - action : add(topic, loggie) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"topic\" : \"loggie\" } copy(from, to) \u00b6 Copy the fields in the event. parameter: from: original key to: new key Example - action : copy(foo, bar) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" , \"bar\" : \"loggie\" } move(from, to) \u00b6 Move/rename fields. parameter: from: original key to: new key Example - action : move(foo, bar) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"bar\" : \"loggie\" } set(key, value) \u00b6 Update the value of key to be value. parameter: key: the field to be updated value: new value Example - action : set(foo, test) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"test\" } del(key1, key2...) \u00b6 Delete fields. Multiple field keys can be filled in. Example - action : del(foo) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , } underRoot(key) \u00b6 Put nested fields at the root (outermost). Example - action : underRoot(state) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"state\" : { \"node\" : \"127.0.0.1\" , \"phase\" : \"running\" } } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"node\" : \"127.0.0.1\" , \"phase\" : \"running\" } fmt(key) \u00b6 Generate a value based on other field values. If the key does not exist, the field will be added. Extra fields: pattern: Required, indicating formatting rules, eg ${state.node}-${state.phase} . If pattern is a fixed value, the action is similar to set(key, value). Example - action : fmt(status) pattern : ${state.node} is ${state.phase} input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"state\" : { \"node\" : \"127.0.0.1\" , \"phase\" : \"running\" } } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"state\" : { \"node\" : \"127.0.0.1\" , \"phase\" : \"running\" } \uff0c \"status\" : \"127.0.0.1 is running\" } timestamp(key) \u00b6 Time format conversion. Extra fields: fromLayout: required, specifies the time format of the field (golang form), and can also be unix and unix_ms . fromLocation: specifies the time zone of the field. UTC and Local also supported. If empty, UTC is used. toLayout: required, the converted time format (golang form), and can also be unix and unix_ms . toLocation: the converted time zone. UTC and Local also supported. If empty, UTC is used. Example - action : timestamp(time) fromLayout : \"2006-01-02 15:04:05\" fromLocation : Asia/Shanghai toLayout : unix_ms toLocation : Local input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"time\" : \"2022-06-28 11:24:35\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"time\" : 1656386675000 } For time in golang form, please refer to: const ( Layout = \"01/02 03:04:05PM '06 -0700\" // The reference time, in numerical order. ANSIC = \"Mon Jan _2 15:04:05 2006\" UnixDate = \"Mon Jan _2 15:04:05 MST 2006\" RubyDate = \"Mon Jan 02 15:04:05 -0700 2006\" RFC822 = \"02 Jan 06 15:04 MST\" RFC822Z = \"02 Jan 06 15:04 -0700\" // RFC822 with numeric zone RFC850 = \"Monday, 02-Jan-06 15:04:05 MST\" RFC1123 = \"Mon, 02 Jan 2006 15:04:05 MST\" RFC1123Z = \"Mon, 02 Jan 2006 15:04:05 -0700\" // RFC1123 with numeric zone RFC3339 = \"2006-01-02T15:04:05Z07:00\" RFC3339Nano = \"2006-01-02T15:04:05.999999999Z07:00\" Kitchen = \"3:04PM\" // Handy time stamps. Stamp = \"Jan _2 15:04:05\" StampMilli = \"Jan _2 15:04:05.000\" StampMicro = \"Jan _2 15:04:05.000000\" StampNano = \"Jan _2 15:04:05.000000000\" ) It can also be modified according to requirements. regex(key) \u00b6 Use regular pattern to segment logs and extract fields. It can also be regex(key, to). parameter: key: required, the field to be extracted regularly. to: optional, the key to which all fields are placed after extraction. The default is empty, which means to extract the field to the root. Extra fields: pattern: required, regular expression. Example - action : regex(body) pattern : (?<ip>\\S+) (?<id>\\S+) (?<u>\\S+) (?<time>\\[.*?\\]) (?<url>\\\".*?\\\") (?<status>\\S+) (?<size>\\S+) input: { \"body\" : \"10.244.0.1 - - [13/Dec/2021:12:40:48 +0000] 'GET / HTTP/1.1' 404 683\" , } output: { \"ip\" : \"10.244.0.1\" , \"id\" : \"-\" , \"u\" : \"-\" , \"time\" : \"[13/Dec/2021:12:40:48 +0000]\" , \"url\" : \"GET / HTTP/1.1\" , \"status\" : \"404\" , \"size\" : \"683\" , } jsonDecode(key) \u00b6 Deserialize json text. Can also be jsonDecode(key, to). parameter: key: required, the corresponding field key. to: optional, the key to which all fields are placed after extraction. The default is empty, which means to extract the field to the root. Example - action : jsonDecode(body) input: { \"body\" : ` { \"log\" : \"I0610 08:29:07.698664 Waiting for caches to sync\" , \"stream\" : \"stderr\" , \"time\" : \"2021-06-10T08:29:07.698731204Z\" } ` , } output: { \"log\" : \"I0610 08:29:07.698664 Waiting for caches to sync\" , \"stream\" : \"stderr\" , \"time\" : \"2021-06-10T08:29:07.698731204Z\" } strconv(key, type) \u00b6 Value type conversion. parameter: key: target field type: the converted type, which can be bool , int , float Example - action : strconv(code, int) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"code\" : \"200\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"code\" : 200 } print() \u00b6 Print event. Generally used in the debugging phase. return() \u00b6 Control function. Interceptor returns when executing return(), and does not continue to execute the following action. dropEvent() \u00b6 Control function. Interceptor drops event when executing dropEvent(), which means that the piece of data will be lost and will not continue to be processed and consumed by subsequent interceptors or sinks. Example interceptors : - type : transformer actions : - action : regex(body) pattern : ^(?P<time>[^ ^Z]+Z) (?P<level>[^ ]*) (?P<log>.*)$ - if : equal(level, DEBUG) then : - action : dropEvent() Assuming that the log is: 2021-02-16T09:21:20.545525544Z DEBUG this is log body .If the level field is DEBUG, the log will be discarded directly. condition \u00b6 Conditional function. operator \u00b6 AND: Indicates the and result of two conditions Example interceptors : - type : transformer actions : - if : equal(level, DEBUG) AND equal(code, 200) then : - action : dropEvent() OR\uff1aIndicates the or result of two conditions Example interceptors : - type : transformer actions : - if : equal(level, DEBUG) OR equal(level, INFO) then : - action : dropEvent() NOT: Indicates the not result of the condition Example interceptors : - type : transformer actions : - if : NOT equal(level, DEBUG) then : - action : dropEvent() equal(key, target) \u00b6 Whether the value of field is equal to target. contain(key, target) \u00b6 Whether the value of field contains target. exist(key) \u00b6 Whether the field exists or is empty. greater(key, target) \u00b6 Whether the value of field is greater than targrt. less(key, target) \u00b6 Whether the value of field is less than targrt. hasPrefix(key, target) \u00b6 Whether the value of field contains the target prefix. match(key, regex) \u00b6 Whether the value of field matches regex. oneOf(key, value1, value2...) \u00b6 Whether the value of field is one of the value1 \u200b\u200bvalue2...","title":"transformer"},{"location":"reference/pipelines/interceptor/transformer/#transformer","text":"Used to perform functional data processing conditional judgement. Source interceptor.","title":"transformer"},{"location":"reference/pipelines/interceptor/transformer/#use-cases","text":"Extracts the log level and drops the DEBUG log. Check whether the log is in json form when logs in both json and plain form are mixed. Add different topic topics according to the status code in log. ... See reference .","title":"Use Cases"},{"location":"reference/pipelines/interceptor/transformer/#usage","text":"The transformer will execute all actions in order in the configured actions. The action is similar to the function, and parameters can be written. The parameters are generally fields in the event. Each action may also include additional fields. Take the following regex(body) for example, body is the parameter of regex, and pattern is an extra field. interceptors : - type : transformer actions : - action : regex(body) pattern : ^(?P<time>[^ ^Z]+Z) (?P<level>[^ ]*) (?P<log>.*)$ - action : add(topic, common) In addition, action also supports if-then-else : - if : <condition> then : - action : funcA() else : - action : funcB() Condition is also in the form of a function. interceptors : - type : transformer actions : - if : equal(status, 404) then : - action : add(topic, not_found) - action : return()","title":"Usage"},{"location":"reference/pipelines/interceptor/transformer/#action","text":"","title":"action"},{"location":"reference/pipelines/interceptor/transformer/#common-fields","text":"ignoreError: Indicates whether to ignore and print errors during the processing of this action. Example - type : transformer actions : - action : regex(body) pattern : (?<ip>\\S+) (?<id>\\S+) (?<u>\\S+) (?<time>\\[.*?\\]) (?<url>\\\".*?\\\") (?<status>\\S+) (?<size>\\S+) ignoreError : true The ignoreError here is set to true, which means that the regular matching error will be ignored, and subsequent actions will continue to be executed.","title":"Common fields"},{"location":"reference/pipelines/interceptor/transformer/#addkey-value","text":"Add additional key:value to the event. Example - action : add(topic, loggie) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"topic\" : \"loggie\" }","title":"add(key, value)"},{"location":"reference/pipelines/interceptor/transformer/#copyfrom-to","text":"Copy the fields in the event. parameter: from: original key to: new key Example - action : copy(foo, bar) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" , \"bar\" : \"loggie\" }","title":"copy(from, to)"},{"location":"reference/pipelines/interceptor/transformer/#movefrom-to","text":"Move/rename fields. parameter: from: original key to: new key Example - action : move(foo, bar) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"bar\" : \"loggie\" }","title":"move(from, to)"},{"location":"reference/pipelines/interceptor/transformer/#setkey-value","text":"Update the value of key to be value. parameter: key: the field to be updated value: new value Example - action : set(foo, test) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"test\" }","title":"set(key, value)"},{"location":"reference/pipelines/interceptor/transformer/#delkey1-key2","text":"Delete fields. Multiple field keys can be filled in. Example - action : del(foo) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"foo\" : \"loggie\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , }","title":"del(key1, key2...)"},{"location":"reference/pipelines/interceptor/transformer/#underrootkey","text":"Put nested fields at the root (outermost). Example - action : underRoot(state) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"state\" : { \"node\" : \"127.0.0.1\" , \"phase\" : \"running\" } } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"node\" : \"127.0.0.1\" , \"phase\" : \"running\" }","title":"underRoot(key)"},{"location":"reference/pipelines/interceptor/transformer/#fmtkey","text":"Generate a value based on other field values. If the key does not exist, the field will be added. Extra fields: pattern: Required, indicating formatting rules, eg ${state.node}-${state.phase} . If pattern is a fixed value, the action is similar to set(key, value). Example - action : fmt(status) pattern : ${state.node} is ${state.phase} input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"state\" : { \"node\" : \"127.0.0.1\" , \"phase\" : \"running\" } } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"state\" : { \"node\" : \"127.0.0.1\" , \"phase\" : \"running\" } \uff0c \"status\" : \"127.0.0.1 is running\" }","title":"fmt(key)"},{"location":"reference/pipelines/interceptor/transformer/#timestampkey","text":"Time format conversion. Extra fields: fromLayout: required, specifies the time format of the field (golang form), and can also be unix and unix_ms . fromLocation: specifies the time zone of the field. UTC and Local also supported. If empty, UTC is used. toLayout: required, the converted time format (golang form), and can also be unix and unix_ms . toLocation: the converted time zone. UTC and Local also supported. If empty, UTC is used. Example - action : timestamp(time) fromLayout : \"2006-01-02 15:04:05\" fromLocation : Asia/Shanghai toLayout : unix_ms toLocation : Local input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"time\" : \"2022-06-28 11:24:35\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"time\" : 1656386675000 } For time in golang form, please refer to: const ( Layout = \"01/02 03:04:05PM '06 -0700\" // The reference time, in numerical order. ANSIC = \"Mon Jan _2 15:04:05 2006\" UnixDate = \"Mon Jan _2 15:04:05 MST 2006\" RubyDate = \"Mon Jan 02 15:04:05 -0700 2006\" RFC822 = \"02 Jan 06 15:04 MST\" RFC822Z = \"02 Jan 06 15:04 -0700\" // RFC822 with numeric zone RFC850 = \"Monday, 02-Jan-06 15:04:05 MST\" RFC1123 = \"Mon, 02 Jan 2006 15:04:05 MST\" RFC1123Z = \"Mon, 02 Jan 2006 15:04:05 -0700\" // RFC1123 with numeric zone RFC3339 = \"2006-01-02T15:04:05Z07:00\" RFC3339Nano = \"2006-01-02T15:04:05.999999999Z07:00\" Kitchen = \"3:04PM\" // Handy time stamps. Stamp = \"Jan _2 15:04:05\" StampMilli = \"Jan _2 15:04:05.000\" StampMicro = \"Jan _2 15:04:05.000000\" StampNano = \"Jan _2 15:04:05.000000000\" ) It can also be modified according to requirements.","title":"timestamp(key)"},{"location":"reference/pipelines/interceptor/transformer/#regexkey","text":"Use regular pattern to segment logs and extract fields. It can also be regex(key, to). parameter: key: required, the field to be extracted regularly. to: optional, the key to which all fields are placed after extraction. The default is empty, which means to extract the field to the root. Extra fields: pattern: required, regular expression. Example - action : regex(body) pattern : (?<ip>\\S+) (?<id>\\S+) (?<u>\\S+) (?<time>\\[.*?\\]) (?<url>\\\".*?\\\") (?<status>\\S+) (?<size>\\S+) input: { \"body\" : \"10.244.0.1 - - [13/Dec/2021:12:40:48 +0000] 'GET / HTTP/1.1' 404 683\" , } output: { \"ip\" : \"10.244.0.1\" , \"id\" : \"-\" , \"u\" : \"-\" , \"time\" : \"[13/Dec/2021:12:40:48 +0000]\" , \"url\" : \"GET / HTTP/1.1\" , \"status\" : \"404\" , \"size\" : \"683\" , }","title":"regex(key)"},{"location":"reference/pipelines/interceptor/transformer/#jsondecodekey","text":"Deserialize json text. Can also be jsonDecode(key, to). parameter: key: required, the corresponding field key. to: optional, the key to which all fields are placed after extraction. The default is empty, which means to extract the field to the root. Example - action : jsonDecode(body) input: { \"body\" : ` { \"log\" : \"I0610 08:29:07.698664 Waiting for caches to sync\" , \"stream\" : \"stderr\" , \"time\" : \"2021-06-10T08:29:07.698731204Z\" } ` , } output: { \"log\" : \"I0610 08:29:07.698664 Waiting for caches to sync\" , \"stream\" : \"stderr\" , \"time\" : \"2021-06-10T08:29:07.698731204Z\" }","title":"jsonDecode(key)"},{"location":"reference/pipelines/interceptor/transformer/#strconvkey-type","text":"Value type conversion. parameter: key: target field type: the converted type, which can be bool , int , float Example - action : strconv(code, int) input: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"code\" : \"200\" } output: { \"body\" : \"2021-02-16T09:21:20.545525544Z DEBUG this is log body\" , \"code\" : 200 }","title":"strconv(key, type)"},{"location":"reference/pipelines/interceptor/transformer/#print","text":"Print event. Generally used in the debugging phase.","title":"print()"},{"location":"reference/pipelines/interceptor/transformer/#return","text":"Control function. Interceptor returns when executing return(), and does not continue to execute the following action.","title":"return()"},{"location":"reference/pipelines/interceptor/transformer/#dropevent","text":"Control function. Interceptor drops event when executing dropEvent(), which means that the piece of data will be lost and will not continue to be processed and consumed by subsequent interceptors or sinks. Example interceptors : - type : transformer actions : - action : regex(body) pattern : ^(?P<time>[^ ^Z]+Z) (?P<level>[^ ]*) (?P<log>.*)$ - if : equal(level, DEBUG) then : - action : dropEvent() Assuming that the log is: 2021-02-16T09:21:20.545525544Z DEBUG this is log body .If the level field is DEBUG, the log will be discarded directly.","title":"dropEvent()"},{"location":"reference/pipelines/interceptor/transformer/#condition","text":"Conditional function.","title":"condition"},{"location":"reference/pipelines/interceptor/transformer/#operator","text":"AND: Indicates the and result of two conditions Example interceptors : - type : transformer actions : - if : equal(level, DEBUG) AND equal(code, 200) then : - action : dropEvent() OR\uff1aIndicates the or result of two conditions Example interceptors : - type : transformer actions : - if : equal(level, DEBUG) OR equal(level, INFO) then : - action : dropEvent() NOT: Indicates the not result of the condition Example interceptors : - type : transformer actions : - if : NOT equal(level, DEBUG) then : - action : dropEvent()","title":"operator"},{"location":"reference/pipelines/interceptor/transformer/#equalkey-target","text":"Whether the value of field is equal to target.","title":"equal(key, target)"},{"location":"reference/pipelines/interceptor/transformer/#containkey-target","text":"Whether the value of field contains target.","title":"contain(key, target)"},{"location":"reference/pipelines/interceptor/transformer/#existkey","text":"Whether the field exists or is empty.","title":"exist(key)"},{"location":"reference/pipelines/interceptor/transformer/#greaterkey-target","text":"Whether the value of field is greater than targrt.","title":"greater(key, target)"},{"location":"reference/pipelines/interceptor/transformer/#lesskey-target","text":"Whether the value of field is less than targrt.","title":"less(key, target)"},{"location":"reference/pipelines/interceptor/transformer/#hasprefixkey-target","text":"Whether the value of field contains the target prefix.","title":"hasPrefix(key, target)"},{"location":"reference/pipelines/interceptor/transformer/#matchkey-regex","text":"Whether the value of field matches regex.","title":"match(key, regex)"},{"location":"reference/pipelines/interceptor/transformer/#oneofkey-value1-value2","text":"Whether the value of field is one of the value1 \u200b\u200bvalue2...","title":"oneOf(key, value1, value2...)"},{"location":"reference/pipelines/queue/channel/","text":"channel \u00b6 Channel queue, is a memory buffer queue implemented based on go chan. Example queue : type : channel batchSize \u00b6 field type required default description batchSize int false 2048 The number of events contained in a batch. batchBytes \u00b6 field type required default description batchBytes int64 false 33554432(32MB) The maximum number of bytes of data contained in a batch. batchAggTimeout \u00b6 field type required default description batchAggTimeout time.Duration false 1s Timeout time for Assembling and aggregating multiple events into one batch.","title":"channel"},{"location":"reference/pipelines/queue/channel/#channel","text":"Channel queue, is a memory buffer queue implemented based on go chan. Example queue : type : channel","title":"channel"},{"location":"reference/pipelines/queue/channel/#batchsize","text":"field type required default description batchSize int false 2048 The number of events contained in a batch.","title":"batchSize"},{"location":"reference/pipelines/queue/channel/#batchbytes","text":"field type required default description batchBytes int64 false 33554432(32MB) The maximum number of bytes of data contained in a batch.","title":"batchBytes"},{"location":"reference/pipelines/queue/channel/#batchaggtimeout","text":"field type required default description batchAggTimeout time.Duration false 1s Timeout time for Assembling and aggregating multiple events into one batch.","title":"batchAggTimeout"},{"location":"reference/pipelines/queue/memory/","text":"memory \u00b6 Memory queue, is a memory buffer queue implemented based on go-disruptor . Note Since the version of go-disruptor has not been released, the memory queue is still in the experimental stage. Not recommended for production environments! Example queue : type : memory batchSize \u00b6 field type required default description batchSize int false 2048 The number of events contained in a batch. batchBufferFactor \u00b6 field type required default description batchBufferFactor int false 2 The size of the queue buffer (the capacity of the channel) = batchSize*batchBufferFactor batchBytes \u00b6 field type required default description batchBytes int64 false 33554432(32MB) The maximum number of bytes of data contained in a batch. batchAggTimeout \u00b6 field type required default description batchAggTimeout time.Duration false 1s Timeout time for Assembling and aggregating multiple events into one batch.","title":"memory"},{"location":"reference/pipelines/queue/memory/#memory","text":"Memory queue, is a memory buffer queue implemented based on go-disruptor . Note Since the version of go-disruptor has not been released, the memory queue is still in the experimental stage. Not recommended for production environments! Example queue : type : memory","title":"memory"},{"location":"reference/pipelines/queue/memory/#batchsize","text":"field type required default description batchSize int false 2048 The number of events contained in a batch.","title":"batchSize"},{"location":"reference/pipelines/queue/memory/#batchbufferfactor","text":"field type required default description batchBufferFactor int false 2 The size of the queue buffer (the capacity of the channel) = batchSize*batchBufferFactor","title":"batchBufferFactor"},{"location":"reference/pipelines/queue/memory/#batchbytes","text":"field type required default description batchBytes int64 false 33554432(32MB) The maximum number of bytes of data contained in a batch.","title":"batchBytes"},{"location":"reference/pipelines/queue/memory/#batchaggtimeout","text":"field type required default description batchAggTimeout time.Duration false 1s Timeout time for Assembling and aggregating multiple events into one batch.","title":"batchAggTimeout"},{"location":"reference/pipelines/sink/dev/","text":"dev \u00b6 The dev sink prints log data to the console, which can generally be used for debugging or troubleshooting. After configuring the dev sink, you can set printEvents=true to view the log data sent to the sink in Loggie. In addition to the original logs received or collected by the source, the data generally contains other meta information. Example sink : type : dev printEvents : true codec : type : json pretty : true printEvents \u00b6 field type required default description printEvents bool false false Whether to print the collected logs By default, the log of Loggie is printed in json format, and the CMD arguments can be configured -log.jsonFormat=false to facilitate viewing the output results on the log of Loggie.","title":"dev"},{"location":"reference/pipelines/sink/dev/#dev","text":"The dev sink prints log data to the console, which can generally be used for debugging or troubleshooting. After configuring the dev sink, you can set printEvents=true to view the log data sent to the sink in Loggie. In addition to the original logs received or collected by the source, the data generally contains other meta information. Example sink : type : dev printEvents : true codec : type : json pretty : true","title":"dev"},{"location":"reference/pipelines/sink/dev/#printevents","text":"field type required default description printEvents bool false false Whether to print the collected logs By default, the log of Loggie is printed in json format, and the CMD arguments can be configured -log.jsonFormat=false to facilitate viewing the output results on the log of Loggie.","title":"printEvents"},{"location":"reference/pipelines/sink/elasticsearch/","text":"elasticsearch \u00b6 Use Elasticsearch sink to send data to Elasticsearch cluster. Example sink : type : elasticsearch hosts : [ \"elasticsearch1:9200\" , \"elasticsearch2:9200\" , \"elasticsearch3:9200\" ] index : \"log-${fields.service}-${+YYYY.MM.DD}\" hosts \u00b6 field type required default description hosts string array true none elasticsearch addresses index \u00b6 field type required default description index string true none the index of elasticsearch storage data You can use ${a.b} to obtain fields in the log data, or add ${+YYYY.MM.DD.hh} timestamps to dynamically generate indexes. username \u00b6 field type required default description username string false none If Elasticsearch is configured with username and password authentication, you need to fill in the requested username. password \u00b6 field type required default description password string false none If Elasticsearch is configured with username and password authentication, you need to fill in the requested password. schema \u00b6 field type required default description schema string false http used for client sniffing sniff \u00b6 field type required default description sniff bool false false whether to enable sniffer gzip \u00b6 field type required default description gzip bool false false whether to enable gzip compression for sending data documentId \u00b6 field type required default description documentId string false The id value sent to elasticsearch, which can be extracted from a field by ${} .","title":"elasticsearch"},{"location":"reference/pipelines/sink/elasticsearch/#elasticsearch","text":"Use Elasticsearch sink to send data to Elasticsearch cluster. Example sink : type : elasticsearch hosts : [ \"elasticsearch1:9200\" , \"elasticsearch2:9200\" , \"elasticsearch3:9200\" ] index : \"log-${fields.service}-${+YYYY.MM.DD}\"","title":"elasticsearch"},{"location":"reference/pipelines/sink/elasticsearch/#hosts","text":"field type required default description hosts string array true none elasticsearch addresses","title":"hosts"},{"location":"reference/pipelines/sink/elasticsearch/#index","text":"field type required default description index string true none the index of elasticsearch storage data You can use ${a.b} to obtain fields in the log data, or add ${+YYYY.MM.DD.hh} timestamps to dynamically generate indexes.","title":"index"},{"location":"reference/pipelines/sink/elasticsearch/#username","text":"field type required default description username string false none If Elasticsearch is configured with username and password authentication, you need to fill in the requested username.","title":"username"},{"location":"reference/pipelines/sink/elasticsearch/#password","text":"field type required default description password string false none If Elasticsearch is configured with username and password authentication, you need to fill in the requested password.","title":"password"},{"location":"reference/pipelines/sink/elasticsearch/#schema","text":"field type required default description schema string false http used for client sniffing","title":"schema"},{"location":"reference/pipelines/sink/elasticsearch/#sniff","text":"field type required default description sniff bool false false whether to enable sniffer","title":"sniff"},{"location":"reference/pipelines/sink/elasticsearch/#gzip","text":"field type required default description gzip bool false false whether to enable gzip compression for sending data","title":"gzip"},{"location":"reference/pipelines/sink/elasticsearch/#documentid","text":"field type required default description documentId string false The id value sent to elasticsearch, which can be extracted from a field by ${} .","title":"documentId"},{"location":"reference/pipelines/sink/file/","text":"file \u00b6 Write the received data to the local in the form of a file. Example sink : type : file workerCount : 1024 baseDirs : - /data0 - /data1 - /data2 dirHashKey : ${namespace}-${deployName} filename : /${namespace}/${deployName}/${podName}/${filename} maxSize : 500 maxAge : 7 maxBackups : 50 compress : true workerCount \u00b6 field type required default description workerCount int false 1 concurrency of writing to file baseDirs \u00b6 field type required default description baseDirs string array false The base directory of the file, which can be hashed according to a key. And data can be stored in the corresponding base directory. dirHashKey \u00b6 field type required default description baseDirs string false Hash according to the specified Key. Variables supported. filename \u00b6 field type required default description filename string true filename. Variables supported. maxSize \u00b6 field type required default description maxSize int false 1 File size in MiB maxAge \u00b6 field type required default description maxAge int false The number of days to keep old files. The unit is \"days\". The default is not to delete. maxBackups \u00b6 field type required default description maxBackups int false 1 The maximum number of backup files to retain. Not deleted by default. (if maxAge is configured, the files will be deleted) localTime \u00b6 field type required default description localTime bool false false Whether to format backup files in local time. UTC time is used by default. compress \u00b6 field type required default description compress bool false false Whether to compress. Gzip used. No compression by default.","title":"file"},{"location":"reference/pipelines/sink/file/#file","text":"Write the received data to the local in the form of a file. Example sink : type : file workerCount : 1024 baseDirs : - /data0 - /data1 - /data2 dirHashKey : ${namespace}-${deployName} filename : /${namespace}/${deployName}/${podName}/${filename} maxSize : 500 maxAge : 7 maxBackups : 50 compress : true","title":"file"},{"location":"reference/pipelines/sink/file/#workercount","text":"field type required default description workerCount int false 1 concurrency of writing to file","title":"workerCount"},{"location":"reference/pipelines/sink/file/#basedirs","text":"field type required default description baseDirs string array false The base directory of the file, which can be hashed according to a key. And data can be stored in the corresponding base directory.","title":"baseDirs"},{"location":"reference/pipelines/sink/file/#dirhashkey","text":"field type required default description baseDirs string false Hash according to the specified Key. Variables supported.","title":"dirHashKey"},{"location":"reference/pipelines/sink/file/#filename","text":"field type required default description filename string true filename. Variables supported.","title":"filename"},{"location":"reference/pipelines/sink/file/#maxsize","text":"field type required default description maxSize int false 1 File size in MiB","title":"maxSize"},{"location":"reference/pipelines/sink/file/#maxage","text":"field type required default description maxAge int false The number of days to keep old files. The unit is \"days\". The default is not to delete.","title":"maxAge"},{"location":"reference/pipelines/sink/file/#maxbackups","text":"field type required default description maxBackups int false 1 The maximum number of backup files to retain. Not deleted by default. (if maxAge is configured, the files will be deleted)","title":"maxBackups"},{"location":"reference/pipelines/sink/file/#localtime","text":"field type required default description localTime bool false false Whether to format backup files in local time. UTC time is used by default.","title":"localTime"},{"location":"reference/pipelines/sink/file/#compress","text":"field type required default description compress bool false false Whether to compress. Gzip used. No compression by default.","title":"compress"},{"location":"reference/pipelines/sink/grpc/","text":"grpc \u00b6 Use grpc sink to send to the downstream. The downstream needs to support the grpc protocol in the same format. You can use this sink to send to the grpc source of Loggie Aggregator. Example sink : type : grpc host : \"loggie-aggregator.loggie-aggregator:6166\" host \u00b6 field type required default description host string true none downstream host address. Multiple ips are separated by , commas. loadBalance \u00b6 field type required default description loadBalance string false round_robin grpc load balancing strategy timeout \u00b6 field type required default description timeout time.Duration false 30s send timeout grpcHeaderKey \u00b6 field type required default description grpcHeaderKey string false none","title":"grpc"},{"location":"reference/pipelines/sink/grpc/#grpc","text":"Use grpc sink to send to the downstream. The downstream needs to support the grpc protocol in the same format. You can use this sink to send to the grpc source of Loggie Aggregator. Example sink : type : grpc host : \"loggie-aggregator.loggie-aggregator:6166\"","title":"grpc"},{"location":"reference/pipelines/sink/grpc/#host","text":"field type required default description host string true none downstream host address. Multiple ips are separated by , commas.","title":"host"},{"location":"reference/pipelines/sink/grpc/#loadbalance","text":"field type required default description loadBalance string false round_robin grpc load balancing strategy","title":"loadBalance"},{"location":"reference/pipelines/sink/grpc/#timeout","text":"field type required default description timeout time.Duration false 30s send timeout","title":"timeout"},{"location":"reference/pipelines/sink/grpc/#grpcheaderkey","text":"field type required default description grpcHeaderKey string false none","title":"grpcHeaderKey"},{"location":"reference/pipelines/sink/kafka/","text":"kafka \u00b6 Use sink kafka to send log data to downstream Kafka. Example sink : type : kafka brokers : [ \"127.0.0.1:6400\" ] topic : \"log-${fields.topic}\" brokers \u00b6 field type required default description brokers string array true none Brokers address for sending logs to Kafka topic \u00b6 field type required default description topic string false loggie Kafka topic Use ${a.b} to get the field value in the event as the specific topic name. For example, an event is: { \"topic\" : \"loggie\" , \"hello\" : \"world\" } Configure topic: ${topic} . At this time, the topic of Kafka is \"loggie\". Also nested selection is supported: { \"fields\" : { \"topic\" : \"loggie\" }, \"hello\" : \"world\" } Configure topic: ${fields.topic} , and the topic of Kafka is \"loggie\". balance \u00b6 field type required default description balance string false roundRobin Load balancing strategy, which can be hash , roundRobin , leastBytes compression \u00b6 field type required default description compression string false gzip Compression strategy for sending logs to Kafka, which can be gzip , snappy , lz4 , zstd maxAttempts \u00b6 field type required default description maxAttempts int false 10 max retries batchSize \u00b6 field type required default description batchSize int false 100 The maximum number of data contained in each batch when sending batchBytes \u00b6 field type required default description batchBytes int false 1048576 Maximum number of bytes included in each send request batchTimeout \u00b6 field type required default description batchTimeout time.Duration false 1s Maximum time to form each send batch readTimeout \u00b6 field type required default description readTimeout time.Duration false 10s read timeout writeTimeout \u00b6 field type required default description writeTimeout time.Duration false 10s write timeout requiredAcks \u00b6 field type required default description requiredAcks int false 0 ack waiting parameter, which can be 0 , 1 , -1 0 : don't ask for ack 1 : wait for leader partition ack -1 : wait for all replica acks in the ISR sasl \u00b6 field type required default description sasl false SASL authentication sasl.type string false SASL type, which can be plain , scram sasl.userName string false username sasl.password string false password sasl.algorithm string true when type=scram Algorithm to use when type=scram. sha256 or sha512 partitionKey \u00b6 field type required default description partitionKey string false controller which partition of topic to send message Similar with topic. Use ${a.b} to get the field value in the event as the specific partition key.","title":"kafka"},{"location":"reference/pipelines/sink/kafka/#kafka","text":"Use sink kafka to send log data to downstream Kafka. Example sink : type : kafka brokers : [ \"127.0.0.1:6400\" ] topic : \"log-${fields.topic}\"","title":"kafka"},{"location":"reference/pipelines/sink/kafka/#brokers","text":"field type required default description brokers string array true none Brokers address for sending logs to Kafka","title":"brokers"},{"location":"reference/pipelines/sink/kafka/#topic","text":"field type required default description topic string false loggie Kafka topic Use ${a.b} to get the field value in the event as the specific topic name. For example, an event is: { \"topic\" : \"loggie\" , \"hello\" : \"world\" } Configure topic: ${topic} . At this time, the topic of Kafka is \"loggie\". Also nested selection is supported: { \"fields\" : { \"topic\" : \"loggie\" }, \"hello\" : \"world\" } Configure topic: ${fields.topic} , and the topic of Kafka is \"loggie\".","title":"topic"},{"location":"reference/pipelines/sink/kafka/#balance","text":"field type required default description balance string false roundRobin Load balancing strategy, which can be hash , roundRobin , leastBytes","title":"balance"},{"location":"reference/pipelines/sink/kafka/#compression","text":"field type required default description compression string false gzip Compression strategy for sending logs to Kafka, which can be gzip , snappy , lz4 , zstd","title":"compression"},{"location":"reference/pipelines/sink/kafka/#maxattempts","text":"field type required default description maxAttempts int false 10 max retries","title":"maxAttempts"},{"location":"reference/pipelines/sink/kafka/#batchsize","text":"field type required default description batchSize int false 100 The maximum number of data contained in each batch when sending","title":"batchSize"},{"location":"reference/pipelines/sink/kafka/#batchbytes","text":"field type required default description batchBytes int false 1048576 Maximum number of bytes included in each send request","title":"batchBytes"},{"location":"reference/pipelines/sink/kafka/#batchtimeout","text":"field type required default description batchTimeout time.Duration false 1s Maximum time to form each send batch","title":"batchTimeout"},{"location":"reference/pipelines/sink/kafka/#readtimeout","text":"field type required default description readTimeout time.Duration false 10s read timeout","title":"readTimeout"},{"location":"reference/pipelines/sink/kafka/#writetimeout","text":"field type required default description writeTimeout time.Duration false 10s write timeout","title":"writeTimeout"},{"location":"reference/pipelines/sink/kafka/#requiredacks","text":"field type required default description requiredAcks int false 0 ack waiting parameter, which can be 0 , 1 , -1 0 : don't ask for ack 1 : wait for leader partition ack -1 : wait for all replica acks in the ISR","title":"requiredAcks"},{"location":"reference/pipelines/sink/kafka/#sasl","text":"field type required default description sasl false SASL authentication sasl.type string false SASL type, which can be plain , scram sasl.userName string false username sasl.password string false password sasl.algorithm string true when type=scram Algorithm to use when type=scram. sha256 or sha512","title":"sasl"},{"location":"reference/pipelines/sink/kafka/#partitionkey","text":"field type required default description partitionKey string false controller which partition of topic to send message Similar with topic. Use ${a.b} to get the field value in the event as the specific partition key.","title":"partitionKey"},{"location":"reference/pipelines/sink/loki/","text":"loki \u00b6 loki sink is used to send data to Loki storage. Loki documentation can be found here \u3002 Example sink : type : loki url : \"http://localhost:3100/loki/api/v1/push\" url \u00b6 field type required default description url string true api for pushing loki tenantId \u00b6 field type required default description tenantId string false tenant name used timeout \u00b6 field type required default description timeout time.Duration false 30s send timeout entryLine \u00b6 field type required default description entryLine string false The log content sent to Loki. The default is the body of the Loggie event Loki's log data structure is roughly divided into label and main data. By default, Loggie will convert the meta-information field in the header into a label connected with _ . In addition, it should be noted that since loki's labels key does not support . , / , - , the keys containing these symbols in the header will be automatically converted into _ form.","title":"loki"},{"location":"reference/pipelines/sink/loki/#loki","text":"loki sink is used to send data to Loki storage. Loki documentation can be found here \u3002 Example sink : type : loki url : \"http://localhost:3100/loki/api/v1/push\"","title":"loki"},{"location":"reference/pipelines/sink/loki/#url","text":"field type required default description url string true api for pushing loki","title":"url"},{"location":"reference/pipelines/sink/loki/#tenantid","text":"field type required default description tenantId string false tenant name used","title":"tenantId"},{"location":"reference/pipelines/sink/loki/#timeout","text":"field type required default description timeout time.Duration false 30s send timeout","title":"timeout"},{"location":"reference/pipelines/sink/loki/#entryline","text":"field type required default description entryLine string false The log content sent to Loki. The default is the body of the Loggie event Loki's log data structure is roughly divided into label and main data. By default, Loggie will convert the meta-information field in the header into a label connected with _ . In addition, it should be noted that since loki's labels key does not support . , / , - , the keys containing these symbols in the header will be automatically converted into _ form.","title":"entryLine"},{"location":"reference/pipelines/sink/overview/","text":"Overview \u00b6 A Pipeline has a Sink. Sink Common Configuration \u00b6 Example sink : type : \"dev\" codec : type : json pretty : true parallelism \u00b6 field type required default description parallelism int false 1 The concurrency of the sink client, which can start multiple clients at the same time to increase the sending throughput. codec \u00b6 field type required default description codec false the format used by the data sent to sink codec.type string false json codec type type: json \u00b6 field type required default description codec.pretty false false Whether to beautify the json format codec.beatsFormat false false The log is converted into a filebeats-like format: a @timestamp field is added, and the body field is named as message . type: raw \u00b6 Used to send the collected raw body data. Example sink : type : dev codec : type : raw","title":"Overview"},{"location":"reference/pipelines/sink/overview/#overview","text":"A Pipeline has a Sink.","title":"Overview"},{"location":"reference/pipelines/sink/overview/#sink-common-configuration","text":"Example sink : type : \"dev\" codec : type : json pretty : true","title":"Sink Common Configuration"},{"location":"reference/pipelines/sink/overview/#parallelism","text":"field type required default description parallelism int false 1 The concurrency of the sink client, which can start multiple clients at the same time to increase the sending throughput.","title":"parallelism"},{"location":"reference/pipelines/sink/overview/#codec","text":"field type required default description codec false the format used by the data sent to sink codec.type string false json codec type","title":"codec"},{"location":"reference/pipelines/sink/overview/#type-json","text":"field type required default description codec.pretty false false Whether to beautify the json format codec.beatsFormat false false The log is converted into a filebeats-like format: a @timestamp field is added, and the body field is named as message .","title":"type: json"},{"location":"reference/pipelines/sink/overview/#type-raw","text":"Used to send the collected raw body data. Example sink : type : dev codec : type : raw","title":"type: raw"},{"location":"reference/pipelines/sink/sls/","text":"sls \u00b6 sls sink is used to send logs to Alibaba Cloud SLS . Example sink : type : sls name : demo endpoint : cn-hangzhou.log.aliyuncs.com accessKeyId : ${id} accessKeySecret : ${secret} project : test logstore : test1 topic : myservice endpoint \u00b6 field type required default description endpoint string true SLS storage domain name You can see it in the project overview on the specific project page. accessKeyId \u00b6 field type required default description accessKeyId string true For accessKeyId\uff0cplease refer to the access credential management of Alibaba Cloud account. It is recommended to use a sub-account of Alibaba Cloud. The sub-account needs to have the corresponding project and logstore permissions. accessKeySecret \u00b6 field type required default description accessKeySecret string true For accessKeySecret\uff0cplease refer to the access credential management of Alibaba Cloud account. project \u00b6 field type required default description project string true project name in SLS logstore \u00b6 field type required default description logstore string true logstore name is SLS topic \u00b6 field type required default description topic string false The log topic is the basic management unit of the log service. You can specify a topic when collecting logs to distinguish logs","title":"sls"},{"location":"reference/pipelines/sink/sls/#sls","text":"sls sink is used to send logs to Alibaba Cloud SLS . Example sink : type : sls name : demo endpoint : cn-hangzhou.log.aliyuncs.com accessKeyId : ${id} accessKeySecret : ${secret} project : test logstore : test1 topic : myservice","title":"sls"},{"location":"reference/pipelines/sink/sls/#endpoint","text":"field type required default description endpoint string true SLS storage domain name You can see it in the project overview on the specific project page.","title":"endpoint"},{"location":"reference/pipelines/sink/sls/#accesskeyid","text":"field type required default description accessKeyId string true For accessKeyId\uff0cplease refer to the access credential management of Alibaba Cloud account. It is recommended to use a sub-account of Alibaba Cloud. The sub-account needs to have the corresponding project and logstore permissions.","title":"accessKeyId"},{"location":"reference/pipelines/sink/sls/#accesskeysecret","text":"field type required default description accessKeySecret string true For accessKeySecret\uff0cplease refer to the access credential management of Alibaba Cloud account.","title":"accessKeySecret"},{"location":"reference/pipelines/sink/sls/#project","text":"field type required default description project string true project name in SLS","title":"project"},{"location":"reference/pipelines/sink/sls/#logstore","text":"field type required default description logstore string true logstore name is SLS","title":"logstore"},{"location":"reference/pipelines/sink/sls/#topic","text":"field type required default description topic string false The log topic is the basic management unit of the log service. You can specify a topic when collecting logs to distinguish logs","title":"topic"},{"location":"reference/pipelines/source/dev/","text":"dev \u00b6 Dev is used to automatically generate data in development or pressure testing. Example sources : - type : dev name : benchmark qps : 100 byteSize : 1024 eventsTotal : 10000 qps \u00b6 field type required default description qps int false 1000 QPS for generating event byteSize \u00b6 field type required default description byteSize int false 1024 The number of bytes of a single event eventsTotal \u00b6 field type required default description eventsTotal int false -1, unlimited The total number of all events sent. After all events sent the source will stop sending.","title":"dev"},{"location":"reference/pipelines/source/dev/#dev","text":"Dev is used to automatically generate data in development or pressure testing. Example sources : - type : dev name : benchmark qps : 100 byteSize : 1024 eventsTotal : 10000","title":"dev"},{"location":"reference/pipelines/source/dev/#qps","text":"field type required default description qps int false 1000 QPS for generating event","title":"qps"},{"location":"reference/pipelines/source/dev/#bytesize","text":"field type required default description byteSize int false 1024 The number of bytes of a single event","title":"byteSize"},{"location":"reference/pipelines/source/dev/#eventstotal","text":"field type required default description eventsTotal int false -1, unlimited The total number of all events sent. After all events sent the source will stop sending.","title":"eventsTotal"},{"location":"reference/pipelines/source/file/","text":"file \u00b6 file source is used for log collection. Example sources : - type : file name : accesslog Tips If you use logconfig/clusterlogconfig to collect container logs, additional fields are added to the file source, please refer to here . paths \u00b6 field type required default description paths string array true none The collected paths are matched using glob expressions. Support glob expansion expressions Brace Expansion and Glob Star Example Object files to be collected: /tmp/loggie/service/order/access.log /tmp/loggie/service/order/access.log.2022-04-11 /tmp/loggie/service/pay/access.log /tmp/loggie/service/pay/access.log.2022-04-11 Corresponding configuration: sources : - type : file paths : - /tmp/loggie/**/access.log{,.[2-9][0-9][0-9][0-9]-[01][0-9]-[0123][0-9]} excludeFiles \u00b6 field type required default description excludeFiles string array false none Exclude collected files regular expression Example sources : - type : file paths : - /tmp/*.log excludeFiles : - \\.gz$ ignoreOlder \u00b6 field type required default description ignoreOlder time.Duration false none for example, 48h, which means to ignore files whose update time is 2 days ago ignoreSymlink \u00b6 field type required default description ignoreSymlink bool false false whether to ignore symbolic links (soft links) files addonMeta \u00b6 field type required default description addonMeta bool false false whether to add the default log collection state meta information event example { \"body\" : \"this is test\" , \"state\" : { \"pipeline\" : \"local\" , \"source\" : \"demo\" , \"filename\" : \"/var/log/a.log\" , \"timestamp\" : \"2006-01-02T15:04:05.000Z\" , \"offset\" : 1024 , \"bytes\" : 4096 , \"hostname\" : \"node-1\" } } state explanation\uff1a pipeline: the name of the pipeline where it is located source: the name of the source where it is located filename: the name of the collected file timestamp: the timestamp of the collection time offset: the offset of the collected data in the file bytes: the number of bytes of data collected hostname: the name of the node where it is located workerCount \u00b6 field type required default description workerCount int false 1 The number of worker threads (goroutines) that read the contents of the file. Consider increasing it when there are more than 100 files on a single node readBufferSize \u00b6 field type required default description readBufferSize int false 65536 The amount of data to read from the file at a time. Default 64K=65536 maxContinueRead \u00b6 field type required default description maxContinueRead int false 16 The number of times the content of the same file is read continuously. Reaching this number of times cause forced switch to the next file to read. The main function is to prevent active files from occupying reading resources all the time, in which case inactive files cannot be read and collected for a long time. maxContinueReadTimeout \u00b6 field type required default description maxContinueReadTimeout time.Duration false 3s The maximum reading time of the same file. If this time is exceeded, the next file will be forced to be read. Similar to maxContinueRead inactiveTimeout \u00b6 field type required default description inactiveTimeout time.Duration false 3s If the file has exceeded inactiveTimeout from the last collection, it is considered that the file has entered an inactive state (that is, the last log has been written), and that the last line of log can be collected safely. firstNBytesForIdentifier \u00b6 field type required default description firstNBytesForIdentifier int false 128 Use the first n characters of the collected target file to generate the file unique code. If the size of the file is less than n, the file will not be collected temporarily. The main purpose is to accurately identify a file in combination with file inode information and to determine whether the file is deleted or renamed. charset \u00b6 Encoding conversion, used to convert different encodings to utf8. Example sources : - type : file name : demo paths : - /tmp/log/*.log fields : topic : \"loggie\" charset : \"gbk\" field type required default description charset string false utf-8 Matching model for extracted fields The currently supported encoding formats for converting to utf-8 are: nop plain utf-8 gbk big5 euc-jp iso2022-jp shift-jis euc-kr iso8859-6e iso8859-6i iso8859-8e iso8859-8i iso8859-1 iso8859-2 iso8859-3 iso8859-4 iso8859-5 iso8859-6 iso8859-7 iso8859-8 iso8859-9 iso8859-10 iso8859-13 iso8859-14 iso8859-15 iso8859-16 cp437 cp850 cp852 cp855 cp858 cp860 cp862 cp863 cp865 cp866 ebcdic-037 ebcdic-1040 ebcdic-1047 koi8r koi8u macintosh macintosh-cyrillic windows1250 windows1251 windows1252 windows1253 windows1254 windows1255 windows1256 windows1257 windows1258 windows874 utf-16be-bom utf-16le-bom lineDelimiter \u00b6 Newline symbol configuration Example sources : - type : file name : demo lineDelimiter : type : carriage_return_line_feed value : \"\\r\\n\" charset : gbk type \u00b6 field type required default description type bool false auto value is only valid when type is custom Currently supported types are: auto line_feed vertical_tab form_feed carriage_return carriage_return_line_feed next_line line_separator paragraph_separator null_terminator The corresponding newline symbols are: ``` auto: {'\\u000A'}, line_feed: {'\\u000A'}, vertical_tab: {'\\u000B'}, form_feed: {'\\u000C'}, carriage_return: {'\\u000D'}, carriage_return_line_feed: []byte(\"\\u000D\\u000A\"), next_line: {'\\u0085'}, line_separator: []byte(\"\\u2028\"), paragraph_separator: []byte(\"\\u2029\"), null_terminator: {'\\u0000'}, ``` value \u00b6 field type required default description value string false \\n newline symbol charset \u00b6 field type required default description charset string false utf-8 newline symbol encoding multi \u00b6 Multi-line collection configuration Example sources : - type : file name : accesslog multi : active : true active \u00b6 field type required default description active bool false false whether to enable multi-line pattern \u00b6 field type required default description pattern string required when multi.active=true false A regular expression that is used to judge whether a line is a brand new log. For example, if it is configured as '^[', it is considered that a line beginning with [ is a new log, otherwise the content of this line is merged into the previous log as part of the previous log. maxLines \u00b6 field type required default description maxLines int false 500 Number of lines a log can contains at most. The default is 500 lines. If the upper limit is exceeded, the current log will be forced to be sent, and the excess will be used as a new log. maxBytes \u00b6 field type required default description maxBytes int64 false 131072 Number of bytes a log can contains at most. The default is 128K. If the upper limit is exceeded, the current log will be forced to be sent, and the excess will be used as a new log. timeout \u00b6 field type required default description timeout time.Duration false 5s How long to wait for a log to be collected as a complete log. The default is 5s. If the upper limit is exceeded, the current log will be sent, and the excess will be used as a new log. ack \u00b6 Configuration related to the confirmation of the source. If you need to make sure at least once , you need to turn on the ack mechanism, but there will be a certain performance loss. Caution This configuration can only be configured in defaults Example defaults : sources : - type : file ack : enable : true enable \u00b6 field type required default description enable bool false true Whether to enable confirmation maintenanceInterval \u00b6 field type required default description maintenanceInterval time.Duration false 20h maintenance cycle. Used to regularly clean up expired confirmation data (such as the ack information of files that are no longer collected) db \u00b6 Use sqlite3 as database. Save the file name, file inode, offset of file collection and other information during the collection process. Used to restore the last collection progress after logie reload or restart. Caution This configuration can only be configured in defaults. Example defaults : sources : - type : file db : file : \"./data/loggie.db\" file \u00b6 field type required default description file string false ./data/loggie.db database file path tableName \u00b6 field type required default description tableName string false registry database table name flushTimeout \u00b6 field type required default description flushTimeout time.Duration false 2s write the collected information to the database regularly bufferSize \u00b6 field type required default description bufferSize int false 2048 The buffer size of the collection information written into the database cleanInactiveTimeout \u00b6 field type required default description cleanInactiveTimeout time.Duration false 504h Clean up outdated data in the database. If the update time of the data exceeds the configured value, the data will be deleted. 21 days by default. cleanScanInterval \u00b6 field type required default description cleanScanInterval time.Duration false 1h Periodically check the database for outdated data. Check every 1 hour by default watcher \u00b6 Configuration for monitoring file changes Caution This configuration can only be configured in defaults Example defaults : sources : - type : file watcher : enableOsWatch : true enableOsWatch \u00b6 field type required default description enableOsWatch bool false true Whether to enable the monitoring notification mechanism of the OS. For example, inotify of linux scanTimeInterval \u00b6 field type required default description scanTimeInterval time.Duration false 10s Periodically check file status changes (such as file creation, deletion, etc.). Check every 10s by default maintenanceInterval \u00b6 field type required default description maintenanceInterval time.Duration false 5m Periodic maintenance work (such as reporting and collecting statistics, cleaning files, etc.) fdHoldTimeoutWhenInactive \u00b6 field type required default description fdHoldTimeoutWhenInactive time.Duration false 5m When the time from the last collection of the file to the present exceeds the limit (the file has not been written for a long time, it is considered that there is a high probability that the content will not be written again), the handle of the file will be released to release system resources fdHoldTimeoutWhenRemove \u00b6 field type required default description fdHoldTimeoutWhenRemove time.Duration false 5m When the file is deleted and the collection is not completed, it will wait for the maximum time to complete the collection. If the limit is exceeded, no matter whether the file is finally collected or not, the handle will be released directly and no longer collected. maxOpenFds \u00b6 field type required default description maxOpenFds int false 512 The maximum number of open file handles. If the limit is exceeded, the files will not be collected temporarily maxEofCount \u00b6 field type required default description maxEofCount int false 3 The maximum number of times EoF is encountered in consecutive reads of a file. If the limit is exceeded, it is considered that the file is temporarily inactive and will enter the \"zombie\" queue to wait for the update event to be activated. cleanWhenRemoved \u00b6 field type required default description cleanWhenRemoved bool false true When the file is deleted, whether to delete the collection-related information in the db synchronously. readFromTail \u00b6 field type required default description readFromTail bool false false Whether to start collecting from the latest line of the file, regardless of writing history. It is suitable for scenarios such as migration of collection systems. taskStopTimeout \u00b6 field type required default description taskStopTimeout time.Duration false 30s The timeout period for the collection task to exit. It is a bottom-up solution when Loggie cannot be reloaded. cleanFiles \u00b6 File clearing related configuration. Expired and collected files will be deleted directly from the disk to free up disk space. maxHistoryDays \u00b6 field type required default description maxHistoryDays int false none Maximum number of days to keep files (after collection). If the limit is exceeded, the file will be deleted directly from the disk. If not configured, the file will never be deleted","title":"file"},{"location":"reference/pipelines/source/file/#file","text":"file source is used for log collection. Example sources : - type : file name : accesslog Tips If you use logconfig/clusterlogconfig to collect container logs, additional fields are added to the file source, please refer to here .","title":"file"},{"location":"reference/pipelines/source/file/#paths","text":"field type required default description paths string array true none The collected paths are matched using glob expressions. Support glob expansion expressions Brace Expansion and Glob Star Example Object files to be collected: /tmp/loggie/service/order/access.log /tmp/loggie/service/order/access.log.2022-04-11 /tmp/loggie/service/pay/access.log /tmp/loggie/service/pay/access.log.2022-04-11 Corresponding configuration: sources : - type : file paths : - /tmp/loggie/**/access.log{,.[2-9][0-9][0-9][0-9]-[01][0-9]-[0123][0-9]}","title":"paths"},{"location":"reference/pipelines/source/file/#excludefiles","text":"field type required default description excludeFiles string array false none Exclude collected files regular expression Example sources : - type : file paths : - /tmp/*.log excludeFiles : - \\.gz$","title":"excludeFiles"},{"location":"reference/pipelines/source/file/#ignoreolder","text":"field type required default description ignoreOlder time.Duration false none for example, 48h, which means to ignore files whose update time is 2 days ago","title":"ignoreOlder"},{"location":"reference/pipelines/source/file/#ignoresymlink","text":"field type required default description ignoreSymlink bool false false whether to ignore symbolic links (soft links) files","title":"ignoreSymlink"},{"location":"reference/pipelines/source/file/#addonmeta","text":"field type required default description addonMeta bool false false whether to add the default log collection state meta information event example { \"body\" : \"this is test\" , \"state\" : { \"pipeline\" : \"local\" , \"source\" : \"demo\" , \"filename\" : \"/var/log/a.log\" , \"timestamp\" : \"2006-01-02T15:04:05.000Z\" , \"offset\" : 1024 , \"bytes\" : 4096 , \"hostname\" : \"node-1\" } } state explanation\uff1a pipeline: the name of the pipeline where it is located source: the name of the source where it is located filename: the name of the collected file timestamp: the timestamp of the collection time offset: the offset of the collected data in the file bytes: the number of bytes of data collected hostname: the name of the node where it is located","title":"addonMeta"},{"location":"reference/pipelines/source/file/#workercount","text":"field type required default description workerCount int false 1 The number of worker threads (goroutines) that read the contents of the file. Consider increasing it when there are more than 100 files on a single node","title":"workerCount"},{"location":"reference/pipelines/source/file/#readbuffersize","text":"field type required default description readBufferSize int false 65536 The amount of data to read from the file at a time. Default 64K=65536","title":"readBufferSize"},{"location":"reference/pipelines/source/file/#maxcontinueread","text":"field type required default description maxContinueRead int false 16 The number of times the content of the same file is read continuously. Reaching this number of times cause forced switch to the next file to read. The main function is to prevent active files from occupying reading resources all the time, in which case inactive files cannot be read and collected for a long time.","title":"maxContinueRead"},{"location":"reference/pipelines/source/file/#maxcontinuereadtimeout","text":"field type required default description maxContinueReadTimeout time.Duration false 3s The maximum reading time of the same file. If this time is exceeded, the next file will be forced to be read. Similar to maxContinueRead","title":"maxContinueReadTimeout"},{"location":"reference/pipelines/source/file/#inactivetimeout","text":"field type required default description inactiveTimeout time.Duration false 3s If the file has exceeded inactiveTimeout from the last collection, it is considered that the file has entered an inactive state (that is, the last log has been written), and that the last line of log can be collected safely.","title":"inactiveTimeout"},{"location":"reference/pipelines/source/file/#firstnbytesforidentifier","text":"field type required default description firstNBytesForIdentifier int false 128 Use the first n characters of the collected target file to generate the file unique code. If the size of the file is less than n, the file will not be collected temporarily. The main purpose is to accurately identify a file in combination with file inode information and to determine whether the file is deleted or renamed.","title":"firstNBytesForIdentifier"},{"location":"reference/pipelines/source/file/#charset","text":"Encoding conversion, used to convert different encodings to utf8. Example sources : - type : file name : demo paths : - /tmp/log/*.log fields : topic : \"loggie\" charset : \"gbk\" field type required default description charset string false utf-8 Matching model for extracted fields The currently supported encoding formats for converting to utf-8 are: nop plain utf-8 gbk big5 euc-jp iso2022-jp shift-jis euc-kr iso8859-6e iso8859-6i iso8859-8e iso8859-8i iso8859-1 iso8859-2 iso8859-3 iso8859-4 iso8859-5 iso8859-6 iso8859-7 iso8859-8 iso8859-9 iso8859-10 iso8859-13 iso8859-14 iso8859-15 iso8859-16 cp437 cp850 cp852 cp855 cp858 cp860 cp862 cp863 cp865 cp866 ebcdic-037 ebcdic-1040 ebcdic-1047 koi8r koi8u macintosh macintosh-cyrillic windows1250 windows1251 windows1252 windows1253 windows1254 windows1255 windows1256 windows1257 windows1258 windows874 utf-16be-bom utf-16le-bom","title":"charset"},{"location":"reference/pipelines/source/file/#linedelimiter","text":"Newline symbol configuration Example sources : - type : file name : demo lineDelimiter : type : carriage_return_line_feed value : \"\\r\\n\" charset : gbk","title":"lineDelimiter"},{"location":"reference/pipelines/source/file/#type","text":"field type required default description type bool false auto value is only valid when type is custom Currently supported types are: auto line_feed vertical_tab form_feed carriage_return carriage_return_line_feed next_line line_separator paragraph_separator null_terminator The corresponding newline symbols are: ``` auto: {'\\u000A'}, line_feed: {'\\u000A'}, vertical_tab: {'\\u000B'}, form_feed: {'\\u000C'}, carriage_return: {'\\u000D'}, carriage_return_line_feed: []byte(\"\\u000D\\u000A\"), next_line: {'\\u0085'}, line_separator: []byte(\"\\u2028\"), paragraph_separator: []byte(\"\\u2029\"), null_terminator: {'\\u0000'}, ```","title":"type"},{"location":"reference/pipelines/source/file/#value","text":"field type required default description value string false \\n newline symbol","title":"value"},{"location":"reference/pipelines/source/file/#charset_1","text":"field type required default description charset string false utf-8 newline symbol encoding","title":"charset"},{"location":"reference/pipelines/source/file/#multi","text":"Multi-line collection configuration Example sources : - type : file name : accesslog multi : active : true","title":"multi"},{"location":"reference/pipelines/source/file/#active","text":"field type required default description active bool false false whether to enable multi-line","title":"active"},{"location":"reference/pipelines/source/file/#pattern","text":"field type required default description pattern string required when multi.active=true false A regular expression that is used to judge whether a line is a brand new log. For example, if it is configured as '^[', it is considered that a line beginning with [ is a new log, otherwise the content of this line is merged into the previous log as part of the previous log.","title":"pattern"},{"location":"reference/pipelines/source/file/#maxlines","text":"field type required default description maxLines int false 500 Number of lines a log can contains at most. The default is 500 lines. If the upper limit is exceeded, the current log will be forced to be sent, and the excess will be used as a new log.","title":"maxLines"},{"location":"reference/pipelines/source/file/#maxbytes","text":"field type required default description maxBytes int64 false 131072 Number of bytes a log can contains at most. The default is 128K. If the upper limit is exceeded, the current log will be forced to be sent, and the excess will be used as a new log.","title":"maxBytes"},{"location":"reference/pipelines/source/file/#timeout","text":"field type required default description timeout time.Duration false 5s How long to wait for a log to be collected as a complete log. The default is 5s. If the upper limit is exceeded, the current log will be sent, and the excess will be used as a new log.","title":"timeout"},{"location":"reference/pipelines/source/file/#ack","text":"Configuration related to the confirmation of the source. If you need to make sure at least once , you need to turn on the ack mechanism, but there will be a certain performance loss. Caution This configuration can only be configured in defaults Example defaults : sources : - type : file ack : enable : true","title":"ack"},{"location":"reference/pipelines/source/file/#enable","text":"field type required default description enable bool false true Whether to enable confirmation","title":"enable"},{"location":"reference/pipelines/source/file/#maintenanceinterval","text":"field type required default description maintenanceInterval time.Duration false 20h maintenance cycle. Used to regularly clean up expired confirmation data (such as the ack information of files that are no longer collected)","title":"maintenanceInterval"},{"location":"reference/pipelines/source/file/#db","text":"Use sqlite3 as database. Save the file name, file inode, offset of file collection and other information during the collection process. Used to restore the last collection progress after logie reload or restart. Caution This configuration can only be configured in defaults. Example defaults : sources : - type : file db : file : \"./data/loggie.db\"","title":"db"},{"location":"reference/pipelines/source/file/#file_1","text":"field type required default description file string false ./data/loggie.db database file path","title":"file"},{"location":"reference/pipelines/source/file/#tablename","text":"field type required default description tableName string false registry database table name","title":"tableName"},{"location":"reference/pipelines/source/file/#flushtimeout","text":"field type required default description flushTimeout time.Duration false 2s write the collected information to the database regularly","title":"flushTimeout"},{"location":"reference/pipelines/source/file/#buffersize","text":"field type required default description bufferSize int false 2048 The buffer size of the collection information written into the database","title":"bufferSize"},{"location":"reference/pipelines/source/file/#cleaninactivetimeout","text":"field type required default description cleanInactiveTimeout time.Duration false 504h Clean up outdated data in the database. If the update time of the data exceeds the configured value, the data will be deleted. 21 days by default.","title":"cleanInactiveTimeout"},{"location":"reference/pipelines/source/file/#cleanscaninterval","text":"field type required default description cleanScanInterval time.Duration false 1h Periodically check the database for outdated data. Check every 1 hour by default","title":"cleanScanInterval"},{"location":"reference/pipelines/source/file/#watcher","text":"Configuration for monitoring file changes Caution This configuration can only be configured in defaults Example defaults : sources : - type : file watcher : enableOsWatch : true","title":"watcher"},{"location":"reference/pipelines/source/file/#enableoswatch","text":"field type required default description enableOsWatch bool false true Whether to enable the monitoring notification mechanism of the OS. For example, inotify of linux","title":"enableOsWatch"},{"location":"reference/pipelines/source/file/#scantimeinterval","text":"field type required default description scanTimeInterval time.Duration false 10s Periodically check file status changes (such as file creation, deletion, etc.). Check every 10s by default","title":"scanTimeInterval"},{"location":"reference/pipelines/source/file/#maintenanceinterval_1","text":"field type required default description maintenanceInterval time.Duration false 5m Periodic maintenance work (such as reporting and collecting statistics, cleaning files, etc.)","title":"maintenanceInterval"},{"location":"reference/pipelines/source/file/#fdholdtimeoutwheninactive","text":"field type required default description fdHoldTimeoutWhenInactive time.Duration false 5m When the time from the last collection of the file to the present exceeds the limit (the file has not been written for a long time, it is considered that there is a high probability that the content will not be written again), the handle of the file will be released to release system resources","title":"fdHoldTimeoutWhenInactive"},{"location":"reference/pipelines/source/file/#fdholdtimeoutwhenremove","text":"field type required default description fdHoldTimeoutWhenRemove time.Duration false 5m When the file is deleted and the collection is not completed, it will wait for the maximum time to complete the collection. If the limit is exceeded, no matter whether the file is finally collected or not, the handle will be released directly and no longer collected.","title":"fdHoldTimeoutWhenRemove"},{"location":"reference/pipelines/source/file/#maxopenfds","text":"field type required default description maxOpenFds int false 512 The maximum number of open file handles. If the limit is exceeded, the files will not be collected temporarily","title":"maxOpenFds"},{"location":"reference/pipelines/source/file/#maxeofcount","text":"field type required default description maxEofCount int false 3 The maximum number of times EoF is encountered in consecutive reads of a file. If the limit is exceeded, it is considered that the file is temporarily inactive and will enter the \"zombie\" queue to wait for the update event to be activated.","title":"maxEofCount"},{"location":"reference/pipelines/source/file/#cleanwhenremoved","text":"field type required default description cleanWhenRemoved bool false true When the file is deleted, whether to delete the collection-related information in the db synchronously.","title":"cleanWhenRemoved"},{"location":"reference/pipelines/source/file/#readfromtail","text":"field type required default description readFromTail bool false false Whether to start collecting from the latest line of the file, regardless of writing history. It is suitable for scenarios such as migration of collection systems.","title":"readFromTail"},{"location":"reference/pipelines/source/file/#taskstoptimeout","text":"field type required default description taskStopTimeout time.Duration false 30s The timeout period for the collection task to exit. It is a bottom-up solution when Loggie cannot be reloaded.","title":"taskStopTimeout"},{"location":"reference/pipelines/source/file/#cleanfiles","text":"File clearing related configuration. Expired and collected files will be deleted directly from the disk to free up disk space.","title":"cleanFiles"},{"location":"reference/pipelines/source/file/#maxhistorydays","text":"field type required default description maxHistoryDays int false none Maximum number of days to keep files (after collection). If the limit is exceeded, the file will be deleted directly from the disk. If not configured, the file will never be deleted","title":"maxHistoryDays"},{"location":"reference/pipelines/source/grpc/","text":"grpc \u00b6 Grpc source is used to receive data requests in Loggie Grpc format. It is generally used in Aggregator to receive logs sent by other Loggie clusters. Example sources : - type : grpc name : aggre port : 6066 bind \u00b6 field type required default description bind string false 0.0.0.0 host bound to the server port \u00b6 field type required default description port string false 6066 The port number that provides the service timeout \u00b6 field type required default description timeout time.Duration false 20s timeout","title":"grpc"},{"location":"reference/pipelines/source/grpc/#grpc","text":"Grpc source is used to receive data requests in Loggie Grpc format. It is generally used in Aggregator to receive logs sent by other Loggie clusters. Example sources : - type : grpc name : aggre port : 6066","title":"grpc"},{"location":"reference/pipelines/source/grpc/#bind","text":"field type required default description bind string false 0.0.0.0 host bound to the server","title":"bind"},{"location":"reference/pipelines/source/grpc/#port","text":"field type required default description port string false 6066 The port number that provides the service","title":"port"},{"location":"reference/pipelines/source/grpc/#timeout","text":"field type required default description timeout time.Duration false 20s timeout","title":"timeout"},{"location":"reference/pipelines/source/kafka/","text":"kafka \u00b6 Kafka source is used for receice Kafka data. Example sources : - type : kafka brokers : [ \"kafka1.kafka.svc:9092\" ] topic : log-* brokers \u00b6 field type required default description brokers string array true none Kafka broker address topic \u00b6 field type required default description topic string true none receiving topics. You can use regular expressions to match multiple topics groupId \u00b6 field type required default description groupId string false loggie groupId Loggie use to consume kafka queueCapacity \u00b6 field type required default description queueCapacity int false 100 capacity of internal sending queue minAcceptedBytes \u00b6 field type required default description minAcceptedBytes int false 1 minimum bytes of received batch maxAcceptedBytes \u00b6 field type required default description maxAcceptedBytes int false 1e6 maximum bytes of received batch readMaxAttempts \u00b6 field type required default description readMaxAttempts int false 3 maximum number of retries maxPollWait \u00b6 field type required default description maxPollWait time.Duration false 10s maximum time waiting to receive readBackoffMin \u00b6 field type required default description readBackoffMin time.Duration false 100ms minimum time interval before receiving a new message readBackoffMax \u00b6 field type required default description readBackoffMax time.Duration false 1s maximum time interval before receiving a new message enableAutoCommit \u00b6 field type required default description enableAutoCommit bool false false whether to enable autoCommit autoCommitInterval \u00b6 field type required default description autoCommitInterval time.Duration false 1s Interval time for autoCommit autoOffsetReset \u00b6 field type required default description autoOffsetReset string false latest the initial offset adopted when there is no offset, which can be earliest or latest sasl \u00b6 field type required default description sasl false SASL authentication sasl.type string false SASL type, which can be plain , scram sasl.userName string false username sasl.password string false password sasl.algorithm string true when type=scram Algorithm to use when type=scram. sha256 or sha512","title":"kafka"},{"location":"reference/pipelines/source/kafka/#kafka","text":"Kafka source is used for receice Kafka data. Example sources : - type : kafka brokers : [ \"kafka1.kafka.svc:9092\" ] topic : log-*","title":"kafka"},{"location":"reference/pipelines/source/kafka/#brokers","text":"field type required default description brokers string array true none Kafka broker address","title":"brokers"},{"location":"reference/pipelines/source/kafka/#topic","text":"field type required default description topic string true none receiving topics. You can use regular expressions to match multiple topics","title":"topic"},{"location":"reference/pipelines/source/kafka/#groupid","text":"field type required default description groupId string false loggie groupId Loggie use to consume kafka","title":"groupId"},{"location":"reference/pipelines/source/kafka/#queuecapacity","text":"field type required default description queueCapacity int false 100 capacity of internal sending queue","title":"queueCapacity"},{"location":"reference/pipelines/source/kafka/#minacceptedbytes","text":"field type required default description minAcceptedBytes int false 1 minimum bytes of received batch","title":"minAcceptedBytes"},{"location":"reference/pipelines/source/kafka/#maxacceptedbytes","text":"field type required default description maxAcceptedBytes int false 1e6 maximum bytes of received batch","title":"maxAcceptedBytes"},{"location":"reference/pipelines/source/kafka/#readmaxattempts","text":"field type required default description readMaxAttempts int false 3 maximum number of retries","title":"readMaxAttempts"},{"location":"reference/pipelines/source/kafka/#maxpollwait","text":"field type required default description maxPollWait time.Duration false 10s maximum time waiting to receive","title":"maxPollWait"},{"location":"reference/pipelines/source/kafka/#readbackoffmin","text":"field type required default description readBackoffMin time.Duration false 100ms minimum time interval before receiving a new message","title":"readBackoffMin"},{"location":"reference/pipelines/source/kafka/#readbackoffmax","text":"field type required default description readBackoffMax time.Duration false 1s maximum time interval before receiving a new message","title":"readBackoffMax"},{"location":"reference/pipelines/source/kafka/#enableautocommit","text":"field type required default description enableAutoCommit bool false false whether to enable autoCommit","title":"enableAutoCommit"},{"location":"reference/pipelines/source/kafka/#autocommitinterval","text":"field type required default description autoCommitInterval time.Duration false 1s Interval time for autoCommit","title":"autoCommitInterval"},{"location":"reference/pipelines/source/kafka/#autooffsetreset","text":"field type required default description autoOffsetReset string false latest the initial offset adopted when there is no offset, which can be earliest or latest","title":"autoOffsetReset"},{"location":"reference/pipelines/source/kafka/#sasl","text":"field type required default description sasl false SASL authentication sasl.type string false SASL type, which can be plain , scram sasl.userName string false username sasl.password string false password sasl.algorithm string true when type=scram Algorithm to use when type=scram. sha256 or sha512","title":"sasl"},{"location":"reference/pipelines/source/kube-event/","text":"kubeEvent \u00b6 A source to receive Kubernetes events. For usage, see Collect Kubernetes Events \u3002 Example sources : - type : kubeEvent name : event kubeconfig \u00b6 field type required default description kubeconfig string false kubeconfig used to connect to Kubernetes. not required when Loggie is deployed in Kubernetes. master \u00b6 field type required default description master string false master address used to connect to Kubernetes. not required when Loggie is deployed in Kubernetes. bufferSize \u00b6 field type required default description bufferSize int false 1000 The size of the listening queue, the minimum is 1. watchLatestEvents \u00b6 field type required default description watchLatestEvents bool false false Whether to only listen to the latest events Since Loggie will re-list all events after restarting, repeated sending could happen. If you do not want repeated sending, you can set it to true. Of course, it may lead to the loss of newly generated events during the restart time period. blackListNamespaces \u00b6 field type required default description blackListNamespaces string array false Do not receive events generated in the namespaces defined in it.","title":"kubeEvent"},{"location":"reference/pipelines/source/kube-event/#kubeevent","text":"A source to receive Kubernetes events. For usage, see Collect Kubernetes Events \u3002 Example sources : - type : kubeEvent name : event","title":"kubeEvent"},{"location":"reference/pipelines/source/kube-event/#kubeconfig","text":"field type required default description kubeconfig string false kubeconfig used to connect to Kubernetes. not required when Loggie is deployed in Kubernetes.","title":"kubeconfig"},{"location":"reference/pipelines/source/kube-event/#master","text":"field type required default description master string false master address used to connect to Kubernetes. not required when Loggie is deployed in Kubernetes.","title":"master"},{"location":"reference/pipelines/source/kube-event/#buffersize","text":"field type required default description bufferSize int false 1000 The size of the listening queue, the minimum is 1.","title":"bufferSize"},{"location":"reference/pipelines/source/kube-event/#watchlatestevents","text":"field type required default description watchLatestEvents bool false false Whether to only listen to the latest events Since Loggie will re-list all events after restarting, repeated sending could happen. If you do not want repeated sending, you can set it to true. Of course, it may lead to the loss of newly generated events during the restart time period.","title":"watchLatestEvents"},{"location":"reference/pipelines/source/kube-event/#blacklistnamespaces","text":"field type required default description blackListNamespaces string array false Do not receive events generated in the namespaces defined in it.","title":"blackListNamespaces"},{"location":"reference/pipelines/source/overview/","text":"Overview \u00b6 The sources field is an array, and multiple source configurations can be filled in a Pipeline. Therefore, please note that name of all sources is required as the unique identifier of the source in the pipeline. Source Common Configuration \u00b6 The following configurations are available for all sources. enabled \u00b6 field type required default description enabled bool false true whether to enable the source name \u00b6 field type required default description name string true source name fields \u00b6 field type required default description fields map false customize additional fields added to the event For example: Example sources: - type: file name: access paths: - /var/log/*.log fields: service: demo service: demo will be added to all collected logs. fieldsFromEnv \u00b6 field type required default description fieldsFromEnv map false additional fields added to the event, the value is the key of the env For example: Example sources: - type: file name: access paths: - /var/log/*.log fieldsFromEnv: service: SVC_NAME Loggie will get the value ${SVC_NAME} from the environment variable where Loggie is located, and then add a field to all log events: service: ${SVC_NAME} . fieldsUnderRoot \u00b6 field type required default description fieldsUnderRoot bool false false whether the additional fields are placed at the root of the event For example, by default, the output log format is: { \"body\" : \"hello world\" , \"fields\" : { \"service\" : \"demo\" } } If you set fieldsUnderRoot=true, the output log format is: { \"body\" : \"hello world\" , \"service\" : \"demo\" } fieldsUnderKey \u00b6 field type required default description fieldsUnderKey string false fields if fieldsUnderRoot=false , the key of the field For example, you can modify the default field fields to be tag : { \"body\" : \"hello world\" , \"tag\" : { \"service\" : \"demo\" } } codec \u00b6 field type required default description codec false When the source receives data, it is used for parsing and preprocessing codec.type string false none Please note: Currently only file source supports source codec. type: json \u00b6 field type required default description codec.bodyFields false Use this field in the json data as body Configuration example: type: json sources : - type : file name : nginx paths : - /var/log/*.log codec : type : json bodyFields : log If the collected log is: { \"log\" : \"I0610 08:29:07.698664 Waiting for caches to sync\\n\" , \"stream\" : \"stderr\" , \"time:\" 2021-06-10 T 08 : 29 : 07.698731204 Z\"} Then the event after codec is: body: \"I0610 08:29:07.698664 Waiting for caches to sync\" Note: Currently fields other than bodyFields are discarded. type: regex \u00b6 field type required default description codec.pattern true regular expression codec.bodyFields true use the field extracted by the regular as body Configuration example: type: regex sources : - type : file name : nginx paths : - /var/log/*.log codec : type : regex pattern : ^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) (?P<log>.*)$ bodyFields : log If the collected log is: 2021-12-01T03:13:58.298476921Z stderr F INFO [main] Starting service [Catalina] Then the event after codec is: body: \"INFO [main] Starting service [Catalina]\" Note: Currently fields other than bodyFields are discarded.","title":"Overview"},{"location":"reference/pipelines/source/overview/#overview","text":"The sources field is an array, and multiple source configurations can be filled in a Pipeline. Therefore, please note that name of all sources is required as the unique identifier of the source in the pipeline.","title":"Overview"},{"location":"reference/pipelines/source/overview/#source-common-configuration","text":"The following configurations are available for all sources.","title":"Source Common Configuration"},{"location":"reference/pipelines/source/overview/#enabled","text":"field type required default description enabled bool false true whether to enable the source","title":"enabled"},{"location":"reference/pipelines/source/overview/#name","text":"field type required default description name string true source name","title":"name"},{"location":"reference/pipelines/source/overview/#fields","text":"field type required default description fields map false customize additional fields added to the event For example: Example sources: - type: file name: access paths: - /var/log/*.log fields: service: demo service: demo will be added to all collected logs.","title":"fields"},{"location":"reference/pipelines/source/overview/#fieldsfromenv","text":"field type required default description fieldsFromEnv map false additional fields added to the event, the value is the key of the env For example: Example sources: - type: file name: access paths: - /var/log/*.log fieldsFromEnv: service: SVC_NAME Loggie will get the value ${SVC_NAME} from the environment variable where Loggie is located, and then add a field to all log events: service: ${SVC_NAME} .","title":"fieldsFromEnv"},{"location":"reference/pipelines/source/overview/#fieldsunderroot","text":"field type required default description fieldsUnderRoot bool false false whether the additional fields are placed at the root of the event For example, by default, the output log format is: { \"body\" : \"hello world\" , \"fields\" : { \"service\" : \"demo\" } } If you set fieldsUnderRoot=true, the output log format is: { \"body\" : \"hello world\" , \"service\" : \"demo\" }","title":"fieldsUnderRoot"},{"location":"reference/pipelines/source/overview/#fieldsunderkey","text":"field type required default description fieldsUnderKey string false fields if fieldsUnderRoot=false , the key of the field For example, you can modify the default field fields to be tag : { \"body\" : \"hello world\" , \"tag\" : { \"service\" : \"demo\" } }","title":"fieldsUnderKey"},{"location":"reference/pipelines/source/overview/#codec","text":"field type required default description codec false When the source receives data, it is used for parsing and preprocessing codec.type string false none Please note: Currently only file source supports source codec.","title":"codec"},{"location":"reference/pipelines/source/overview/#type-json","text":"field type required default description codec.bodyFields false Use this field in the json data as body Configuration example: type: json sources : - type : file name : nginx paths : - /var/log/*.log codec : type : json bodyFields : log If the collected log is: { \"log\" : \"I0610 08:29:07.698664 Waiting for caches to sync\\n\" , \"stream\" : \"stderr\" , \"time:\" 2021-06-10 T 08 : 29 : 07.698731204 Z\"} Then the event after codec is: body: \"I0610 08:29:07.698664 Waiting for caches to sync\" Note: Currently fields other than bodyFields are discarded.","title":"type: json"},{"location":"reference/pipelines/source/overview/#type-regex","text":"field type required default description codec.pattern true regular expression codec.bodyFields true use the field extracted by the regular as body Configuration example: type: regex sources : - type : file name : nginx paths : - /var/log/*.log codec : type : regex pattern : ^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) (?P<log>.*)$ bodyFields : log If the collected log is: 2021-12-01T03:13:58.298476921Z stderr F INFO [main] Starting service [Catalina] Then the event after codec is: body: \"INFO [main] Starting service [Catalina]\" Note: Currently fields other than bodyFields are discarded.","title":"type: regex"},{"location":"reference/pipelines/source/prometheus-exporter/","text":"prometheusExporter \u00b6 Collect metrics data from Prometheus Metrics. Example sources : - type : prometheusExporter name : metric endpoints : - \"http://127.0.0.1:9196/metrics\" endpoints \u00b6 field type required default description endpoints string array true remote exporter address. Please note that Loggie will not add /metrics to the request path by default. interval \u00b6 field type required default description interval time.Duration false 30s Time interval for regularly request the remote exporter timeout \u00b6 field type required default description timeout time.Duration false 5s The timeout for the request toJson \u00b6 field type required default description toJson bool false false Whether to convert the captured prometheus native metrics into JSON format labels \u00b6 field type required default description labels map false Add extra labels to all metrics labels supports ${_env.XX} to obtain environment variables. For example: labels sources : - type : prometheusExporter name : metric endpoints : - \"http://127.0.0.1:9196/metrics\" labels : svc : ${_env.SVC} Assuming that env SVC=test, the label svc=test will be added to all metrics.","title":"prometheusExporter"},{"location":"reference/pipelines/source/prometheus-exporter/#prometheusexporter","text":"Collect metrics data from Prometheus Metrics. Example sources : - type : prometheusExporter name : metric endpoints : - \"http://127.0.0.1:9196/metrics\"","title":"prometheusExporter"},{"location":"reference/pipelines/source/prometheus-exporter/#endpoints","text":"field type required default description endpoints string array true remote exporter address. Please note that Loggie will not add /metrics to the request path by default.","title":"endpoints"},{"location":"reference/pipelines/source/prometheus-exporter/#interval","text":"field type required default description interval time.Duration false 30s Time interval for regularly request the remote exporter","title":"interval"},{"location":"reference/pipelines/source/prometheus-exporter/#timeout","text":"field type required default description timeout time.Duration false 5s The timeout for the request","title":"timeout"},{"location":"reference/pipelines/source/prometheus-exporter/#tojson","text":"field type required default description toJson bool false false Whether to convert the captured prometheus native metrics into JSON format","title":"toJson"},{"location":"reference/pipelines/source/prometheus-exporter/#labels","text":"field type required default description labels map false Add extra labels to all metrics labels supports ${_env.XX} to obtain environment variables. For example: labels sources : - type : prometheusExporter name : metric endpoints : - \"http://127.0.0.1:9196/metrics\" labels : svc : ${_env.SVC} Assuming that env SVC=test, the label svc=test will be added to all metrics.","title":"labels"},{"location":"reference/pipelines/source/unix/","text":"unix \u00b6 Receive data through unix socket. Example sources : - type : unix name : demo path : \"/tmp/loggie.sock\" path \u00b6 field type required default description path string true receiving pathname maxBytes \u00b6 field type required default description maxBytes int false 40960 Maximum number of bytes received maxConnections \u00b6 field type required default description maxConnections int false 512 Maximum number of simultaneous connections timeout \u00b6 field type required default description timeout time.Duration false 5m connection timeout mode \u00b6 field type required default description mode string false 0755","title":"unix"},{"location":"reference/pipelines/source/unix/#unix","text":"Receive data through unix socket. Example sources : - type : unix name : demo path : \"/tmp/loggie.sock\"","title":"unix"},{"location":"reference/pipelines/source/unix/#path","text":"field type required default description path string true receiving pathname","title":"path"},{"location":"reference/pipelines/source/unix/#maxbytes","text":"field type required default description maxBytes int false 40960 Maximum number of bytes received","title":"maxBytes"},{"location":"reference/pipelines/source/unix/#maxconnections","text":"field type required default description maxConnections int false 512 Maximum number of simultaneous connections","title":"maxConnections"},{"location":"reference/pipelines/source/unix/#timeout","text":"field type required default description timeout time.Duration false 5m connection timeout","title":"timeout"},{"location":"reference/pipelines/source/unix/#mode","text":"field type required default description mode string false 0755","title":"mode"},{"location":"user-guide/","text":"User Guide \u00b6 In this USER GUIDE section, we mainly introduce the main functions and features of Loggie, and show how to use Loggie to meet various needs in various scenarios. If you want to inquire about the usage and configuration of specific components, please click Configuration . What problems will be encountered when implementing a log system? \u00b6 In the enterprise, how do we build a complete log system? How to choose according to the actual situation, and what problems will be encountered? Under different business types, different usage scenarios, and different log scales, what log system architectures can we adopt? Please refer to Enterprise Pracitice . After we have a preliminary understanding of the implementation of a log system with complete functions and complete architecture, we want to know: Why can Loggie solve these problems? \u00b6 There are already many open source log collection agents on the market. Why did we choose to develop Loggie? What problem was Loggie born to solve? What is the difference between Loggie and other open source log agents? Refer to Architecture and Features . How to use Loggie to solve these problems? \u00b6 How to use Loggie in Kubernetes? Refer to Kubernetes . How can I configure various log collecting architectures? How to adapt Loggie to existing services? How to deal with log processing and segmentation? What other useful functions does Loggie provide? Refer to Best Practice . How to configure the overall monitoring and alarm to ensure the normal operation of Loggie? How to monitor whether ERROR logs are collected? Refer to Monitor and Alerm .","title":"Overview"},{"location":"user-guide/#user-guide","text":"In this USER GUIDE section, we mainly introduce the main functions and features of Loggie, and show how to use Loggie to meet various needs in various scenarios. If you want to inquire about the usage and configuration of specific components, please click Configuration .","title":"User Guide"},{"location":"user-guide/#what-problems-will-be-encountered-when-implementing-a-log-system","text":"In the enterprise, how do we build a complete log system? How to choose according to the actual situation, and what problems will be encountered? Under different business types, different usage scenarios, and different log scales, what log system architectures can we adopt? Please refer to Enterprise Pracitice . After we have a preliminary understanding of the implementation of a log system with complete functions and complete architecture, we want to know:","title":"What problems will be encountered when implementing a log system?"},{"location":"user-guide/#why-can-loggie-solve-these-problems","text":"There are already many open source log collection agents on the market. Why did we choose to develop Loggie? What problem was Loggie born to solve? What is the difference between Loggie and other open source log agents? Refer to Architecture and Features .","title":"Why can Loggie solve these problems?"},{"location":"user-guide/#how-to-use-loggie-to-solve-these-problems","text":"How to use Loggie in Kubernetes? Refer to Kubernetes . How can I configure various log collecting architectures? How to adapt Loggie to existing services? How to deal with log processing and segmentation? What other useful functions does Loggie provide? Refer to Best Practice . How to configure the overall monitoring and alarm to ensure the normal operation of Loggie? How to monitor whether ERROR logs are collected? Refer to Monitor and Alerm .","title":"How to use Loggie to solve these problems?"},{"location":"user-guide/architecture/advantages/","text":"Advantages and Features of Loggie \u00b6 Loggie supports multiple Pipelines, each of which is based on a simple and intuitive source->interceptor->sink architecture. The benefits of this design are: Strong Isolation \u00b6 Multi-Pipeline design to reduce mutual interference. For example, we can put important business logs in a Pipeline, and configure other unimportant logs as another Pipeline. When unimportant log configuration changes or downstream congestion occurs, the collection and sending of important logs will not be affected. Better Commonality \u00b6 In some scenarios, we may mix and deploy different types of services on a node, and it is likely that their logs will be sent to different Kafka clusters. If there is only one global output source, two agents need to be deployed on the node. If you use Loggie, you only need to use different Pipelines. Each Pipeline is configured with different sinks to reduce deployment costs. We can even collect the same logs, send them to different back-end output sources, and configure them flexibly according to actual needs. Flexible, Hot-pluggable, Extensive \u00b6 In essence, the source->interceptor->sink architecture is a data streaming. The arrangement and combination of different types of source/interceptor/sink can meet the different needs of logs. Not classified into Filter/Formater and other types, interceptor undertakes most of the work except for source reading and sink sending. It only needs to configure different interceptors to have the capabilities of transit, filtering, parsing, segmentation, and log alarming. So, Loggie can: be deployed standalone as a log transit station. parse and process logs on agent side or transit side monitoring k8s events, synchronize data as cronjob, etc. in Non-log collection scenarios. Loggie brings benefits at the deployment and maintenance level. Compared with the conventional ELK architecture used before (Filebeat to collect logs, Logstash to transfer and parse logs), costs of troubleshooting and customized development are lower, because Filebeat and Logstash are written in two different languages. After switching to Loggie, we do not need to maintain two projects. And if there is no need for data transfer, we can choose to configure log parsing on the Agent side. In addition, the current open source solution for log alarms is elastAlert. But elastAlert cannot directly connect to AlertManager, and there are problems in high availability. At the same time elastAlert strongly relies on Elasticsearch. Therefore, if you use Loggie, you can directly use Loggie to detect abnormal logs and dock alarm service without introducing additional components. Build a Component Quickly \u00b6 Loggie is based on the microkernel architecture, and all source/interceptor/sink/queue are abstracted into components. You only need to implement the Golang interface in the code to easily develop a component. If Loggie cannot meet your needs in some scenarios, you can try to write a component of your own. For example, if you need to convert Loggie into a specific log format, you can write an interceptor to process it; if you need Loggie to send the collected logs to services that are not yet supported, you can write a sink. Loggie is written in Golang, so you currently need to write components in Golang. Compared with Java or C/C++, Golang has a compromise between performance and development efficiency, and is more suitable for scenarios similar to log agents. More Convenient Container Log Collection in Kubernetes, Better Cloud Native Support \u00b6 If you have tried to collect logs in Kubernetes, you should have encountered these problems: Use sidecar or daemonset to deploy log agent? How to mount log files? Is it only recommended to use stdout to output logs? How are collected logs associated with information such as Pod, Namespace, and Node? In Kubernetes, Pods will be dynamically created and destroyed, and migrated to other Nodes. What about the corresponding log Agent configuration? If you use Loggie: Quick deployment. Various deployment architectures supported. ClusterLogConfig/LogConfig CRD used to configure and manage log configurations. More convenient to access various container cloud platforms, without intrusion to business, without worrying about Pod migration, without manually operating and configuring log files on nodes. Meta information such as Namespace/PodName/NodeName can be injected for query. Better Stability, More Detailed Monitoring Indicators, More Convenient to Troubleshoot \u00b6 In the actual production environment, the stability of the log agent itself is very important, and it is also important not to affect the business. Loggie can configure a current-limiting interceptor, which can prevent occupying too much network bandwidth when the log volume is too large. Loggie has a reasonable file handle processing mechanism to avoid node instability caused in various abnormal scenarios where fd is occupied. In addition, considering various problems we have encountered in various environments, Loggie detects and exposes targeted indicators. For example, collection and transmission delay detection, file size growth or file size detection. These types of metric reporting are supported. At the same time, Loggie supports native Prometheus metric, which can avoid deployment cost and resource consumption caused by additional deployment of exporter. Loggie also provides a complete Grafana monitoring chart, which can be easily used. Lower Resource Usage, Better Performance \u00b6 Loggie is written based on Golang. We perform a lot of optimizations at the code level, which can provide powerful throughput performance while occupying less resources.","title":"Advantages and Features"},{"location":"user-guide/architecture/advantages/#advantages-and-features-of-loggie","text":"Loggie supports multiple Pipelines, each of which is based on a simple and intuitive source->interceptor->sink architecture. The benefits of this design are:","title":"Advantages and Features of Loggie"},{"location":"user-guide/architecture/advantages/#strong-isolation","text":"Multi-Pipeline design to reduce mutual interference. For example, we can put important business logs in a Pipeline, and configure other unimportant logs as another Pipeline. When unimportant log configuration changes or downstream congestion occurs, the collection and sending of important logs will not be affected.","title":"Strong Isolation"},{"location":"user-guide/architecture/advantages/#better-commonality","text":"In some scenarios, we may mix and deploy different types of services on a node, and it is likely that their logs will be sent to different Kafka clusters. If there is only one global output source, two agents need to be deployed on the node. If you use Loggie, you only need to use different Pipelines. Each Pipeline is configured with different sinks to reduce deployment costs. We can even collect the same logs, send them to different back-end output sources, and configure them flexibly according to actual needs.","title":"Better Commonality"},{"location":"user-guide/architecture/advantages/#flexible-hot-pluggable-extensive","text":"In essence, the source->interceptor->sink architecture is a data streaming. The arrangement and combination of different types of source/interceptor/sink can meet the different needs of logs. Not classified into Filter/Formater and other types, interceptor undertakes most of the work except for source reading and sink sending. It only needs to configure different interceptors to have the capabilities of transit, filtering, parsing, segmentation, and log alarming. So, Loggie can: be deployed standalone as a log transit station. parse and process logs on agent side or transit side monitoring k8s events, synchronize data as cronjob, etc. in Non-log collection scenarios. Loggie brings benefits at the deployment and maintenance level. Compared with the conventional ELK architecture used before (Filebeat to collect logs, Logstash to transfer and parse logs), costs of troubleshooting and customized development are lower, because Filebeat and Logstash are written in two different languages. After switching to Loggie, we do not need to maintain two projects. And if there is no need for data transfer, we can choose to configure log parsing on the Agent side. In addition, the current open source solution for log alarms is elastAlert. But elastAlert cannot directly connect to AlertManager, and there are problems in high availability. At the same time elastAlert strongly relies on Elasticsearch. Therefore, if you use Loggie, you can directly use Loggie to detect abnormal logs and dock alarm service without introducing additional components.","title":"Flexible, Hot-pluggable, Extensive"},{"location":"user-guide/architecture/advantages/#build-a-component-quickly","text":"Loggie is based on the microkernel architecture, and all source/interceptor/sink/queue are abstracted into components. You only need to implement the Golang interface in the code to easily develop a component. If Loggie cannot meet your needs in some scenarios, you can try to write a component of your own. For example, if you need to convert Loggie into a specific log format, you can write an interceptor to process it; if you need Loggie to send the collected logs to services that are not yet supported, you can write a sink. Loggie is written in Golang, so you currently need to write components in Golang. Compared with Java or C/C++, Golang has a compromise between performance and development efficiency, and is more suitable for scenarios similar to log agents.","title":"Build a Component Quickly"},{"location":"user-guide/architecture/advantages/#more-convenient-container-log-collection-in-kubernetes-better-cloud-native-support","text":"If you have tried to collect logs in Kubernetes, you should have encountered these problems: Use sidecar or daemonset to deploy log agent? How to mount log files? Is it only recommended to use stdout to output logs? How are collected logs associated with information such as Pod, Namespace, and Node? In Kubernetes, Pods will be dynamically created and destroyed, and migrated to other Nodes. What about the corresponding log Agent configuration? If you use Loggie: Quick deployment. Various deployment architectures supported. ClusterLogConfig/LogConfig CRD used to configure and manage log configurations. More convenient to access various container cloud platforms, without intrusion to business, without worrying about Pod migration, without manually operating and configuring log files on nodes. Meta information such as Namespace/PodName/NodeName can be injected for query.","title":"More Convenient Container Log Collection in Kubernetes, Better Cloud Native Support"},{"location":"user-guide/architecture/advantages/#better-stability-more-detailed-monitoring-indicators-more-convenient-to-troubleshoot","text":"In the actual production environment, the stability of the log agent itself is very important, and it is also important not to affect the business. Loggie can configure a current-limiting interceptor, which can prevent occupying too much network bandwidth when the log volume is too large. Loggie has a reasonable file handle processing mechanism to avoid node instability caused in various abnormal scenarios where fd is occupied. In addition, considering various problems we have encountered in various environments, Loggie detects and exposes targeted indicators. For example, collection and transmission delay detection, file size growth or file size detection. These types of metric reporting are supported. At the same time, Loggie supports native Prometheus metric, which can avoid deployment cost and resource consumption caused by additional deployment of exporter. Loggie also provides a complete Grafana monitoring chart, which can be easily used.","title":"Better Stability, More Detailed Monitoring Indicators, More Convenient to Troubleshoot"},{"location":"user-guide/architecture/advantages/#lower-resource-usage-better-performance","text":"Loggie is written based on Golang. We perform a lot of optimizations at the code level, which can provide powerful throughput performance while occupying less resources.","title":"Lower Resource Usage, Better Performance"},{"location":"user-guide/architecture/background/","text":"Loggie's Background \u00b6 Why did we choose to develop Loggie? What was the background and reason at that time? 1\u3001The Problems We Encountered \u00b6 Before the development of Loggie, we used Filebeat as log collectors. Why did we choose Filebeat at that time? Filebeat is a product of Elastic. It is mainly lightweight and is used to replace the original Logstash to implement log collection. Compared with Logstash based on the JRuby language, Filebeat is indeed lightweight and occupies less resources. Compared with other open source log collection agents, Filebeat based on Golang also has many advantages. For example, compared with Flume based on Java, it has better performance and less resource consumption; compared with Fluentd based on Ruby, it has better performance and is more convenient for secondary development; compared with Fluentd-bit based on C, it has more complete functions and is friendly for development. Therefore, in general, Filebeat is a relatively balanced log collection agent, which is why we chose Filebeat as the default log collection agent. However, as we used Filebeat more deeply, we also encountered some problems in internal practice of the company and production implementation for external customers. Since Filebeat was originally designed in order to distinguish it from Logstash, it highlighted lightweight feature and sacrificed a lot of design for extensibility. The most obvious is that Filebeat has only one Queue and one Output. This also resulted in: Weak Isolation \u00b6 Since all service logs will be sent to the globally unique queue, the service log data will be mixed together, and isolation cannot be guaranteed when abnormality occurs. For example, accumulation of Filebeat's global queue will cause all service logs of the node unable to be sent. If we have different log levels and requirements for different services, all logs will be affected. Multiple Outputs are Not Supported \u00b6 In some scenarios, we may need to send different types of logs of different services to different backends, but Filebeat cannot use the same Agent to send log to different Kafka clusters. We can only deploy multiple Agents on the node, resulting in maintenance and Resource costs rising. Limited Extensibility \u00b6 Compared with Logstash/Flume, etc., Filebeat does not use a flexible multiple pipeline design similar to input->queue->output. For the processing/filtering/enhancement of log data, Filebeat relies on some limited processors. At the same time, Filebeat cannot be used as a transit aggregator. So, it is greatly limited in usage scenarios, and additional components need to be introduced. In addition, Filebeat cannot satisfy scenarios such as log alarms. We have also tried customized development, but Filebeat's own architecture design is difficult to achieve more extensive capabilities, and this will bring the problem of synchronization between upgrades and upstream community code in the long run. Log Operation and Maintenance and Troubleshooting Dilemma \u00b6 Filebeat\u2019s metrics are relatively limited. In many cases, we want to check whether common logs are collected, whether collected logs are complete, whether there is a delay in sending, etc. Filebeat does not provide corresponding functions, which greatly affects efficiency of online troubleshooting. Moreover, Filebeat does not provide monitoring indicators in Prometheus format, which requires additional injection of exporter. Performance Bottleneck \u00b6 Although the performance of Filebeat is acceptable, in our actual use, when the log scene is complex and the log volume is large, there is a bottleneck in throughput, which cannot meet the real-time requirements. 2\u3001Why Can't Other Existing Open Source Log Projects Meet the Needs? \u00b6 Fluentd/Fluent-bit Fluentd is based on Ruby with average performance and is designed to be single thread; Fluent-bit is based on C. For our technology stack, the maintenance and secondary development costs of Ruby and C are relatively high. Logstash The performance of Logstash is poor, and the resource occupation and consumption based on JRuby are relatively large. Flume The resource usage is large and the performance is average. Some internal departments have used Flume before, and the pressure test results confirm that it is indeed inferior to Filebeat. The most important thing is that none of the current open source agents have good native support for K8s, and some of them can only collect stdout logs. It is precisely because the current open source Agent has some problems that it cannot meet the long-term needs, we decided to start self-research. 3\u3001What is Our Ideal Log Agent? \u00b6 The overall goal: high performance, low resource consumption, high availability, strong stability, strong extensibility, and more suitable for cloud native applications. Performance and resources: Under the same performance, the CPU is much lower than that of Filebeat of community version, and the throughput limit is much higher than that of Filebeat. High availability and strong stability: Resource isolation. As an infrastructure, agent must be stable and reliable. At the same time, it supports a large number of monitoring indicators by default, which has good support for common operation and maintenance problems and reduces the operation and maintenance burden. Extensibility: Convenient for users to extend and realize filtering, routing, encoding and other capabilities. For example, a processing logic can be written very quickly, and performed. To sum up, our ideal log agent should be: Out-of-the-box: Log collection services which can be quickly deployed in containerized scenarios; Complete documentation and introduction; High performance: higher performance and less resource usage than native Filebeat; High reliability: stronger isolation and stability; More integrated monitoring indicators by default to facilitate operation and maintenance troubleshooting; Extensibility: Based on the microkernel architecture, users can easily and quickly write their own plug-ins to meet various customization requirements;","title":"Background"},{"location":"user-guide/architecture/background/#loggies-background","text":"Why did we choose to develop Loggie? What was the background and reason at that time?","title":"Loggie's Background"},{"location":"user-guide/architecture/background/#1the-problems-we-encountered","text":"Before the development of Loggie, we used Filebeat as log collectors. Why did we choose Filebeat at that time? Filebeat is a product of Elastic. It is mainly lightweight and is used to replace the original Logstash to implement log collection. Compared with Logstash based on the JRuby language, Filebeat is indeed lightweight and occupies less resources. Compared with other open source log collection agents, Filebeat based on Golang also has many advantages. For example, compared with Flume based on Java, it has better performance and less resource consumption; compared with Fluentd based on Ruby, it has better performance and is more convenient for secondary development; compared with Fluentd-bit based on C, it has more complete functions and is friendly for development. Therefore, in general, Filebeat is a relatively balanced log collection agent, which is why we chose Filebeat as the default log collection agent. However, as we used Filebeat more deeply, we also encountered some problems in internal practice of the company and production implementation for external customers. Since Filebeat was originally designed in order to distinguish it from Logstash, it highlighted lightweight feature and sacrificed a lot of design for extensibility. The most obvious is that Filebeat has only one Queue and one Output. This also resulted in:","title":"1\u3001The Problems We Encountered"},{"location":"user-guide/architecture/background/#weak-isolation","text":"Since all service logs will be sent to the globally unique queue, the service log data will be mixed together, and isolation cannot be guaranteed when abnormality occurs. For example, accumulation of Filebeat's global queue will cause all service logs of the node unable to be sent. If we have different log levels and requirements for different services, all logs will be affected.","title":"Weak Isolation"},{"location":"user-guide/architecture/background/#multiple-outputs-are-not-supported","text":"In some scenarios, we may need to send different types of logs of different services to different backends, but Filebeat cannot use the same Agent to send log to different Kafka clusters. We can only deploy multiple Agents on the node, resulting in maintenance and Resource costs rising.","title":"Multiple Outputs are Not Supported"},{"location":"user-guide/architecture/background/#limited-extensibility","text":"Compared with Logstash/Flume, etc., Filebeat does not use a flexible multiple pipeline design similar to input->queue->output. For the processing/filtering/enhancement of log data, Filebeat relies on some limited processors. At the same time, Filebeat cannot be used as a transit aggregator. So, it is greatly limited in usage scenarios, and additional components need to be introduced. In addition, Filebeat cannot satisfy scenarios such as log alarms. We have also tried customized development, but Filebeat's own architecture design is difficult to achieve more extensive capabilities, and this will bring the problem of synchronization between upgrades and upstream community code in the long run.","title":"Limited Extensibility"},{"location":"user-guide/architecture/background/#log-operation-and-maintenance-and-troubleshooting-dilemma","text":"Filebeat\u2019s metrics are relatively limited. In many cases, we want to check whether common logs are collected, whether collected logs are complete, whether there is a delay in sending, etc. Filebeat does not provide corresponding functions, which greatly affects efficiency of online troubleshooting. Moreover, Filebeat does not provide monitoring indicators in Prometheus format, which requires additional injection of exporter.","title":"Log Operation and Maintenance and Troubleshooting Dilemma"},{"location":"user-guide/architecture/background/#performance-bottleneck","text":"Although the performance of Filebeat is acceptable, in our actual use, when the log scene is complex and the log volume is large, there is a bottleneck in throughput, which cannot meet the real-time requirements.","title":"Performance Bottleneck"},{"location":"user-guide/architecture/background/#2why-cant-other-existing-open-source-log-projects-meet-the-needs","text":"Fluentd/Fluent-bit Fluentd is based on Ruby with average performance and is designed to be single thread; Fluent-bit is based on C. For our technology stack, the maintenance and secondary development costs of Ruby and C are relatively high. Logstash The performance of Logstash is poor, and the resource occupation and consumption based on JRuby are relatively large. Flume The resource usage is large and the performance is average. Some internal departments have used Flume before, and the pressure test results confirm that it is indeed inferior to Filebeat. The most important thing is that none of the current open source agents have good native support for K8s, and some of them can only collect stdout logs. It is precisely because the current open source Agent has some problems that it cannot meet the long-term needs, we decided to start self-research.","title":"2\u3001Why Can't Other Existing Open Source Log Projects Meet the Needs?"},{"location":"user-guide/architecture/background/#3what-is-our-ideal-log-agent","text":"The overall goal: high performance, low resource consumption, high availability, strong stability, strong extensibility, and more suitable for cloud native applications. Performance and resources: Under the same performance, the CPU is much lower than that of Filebeat of community version, and the throughput limit is much higher than that of Filebeat. High availability and strong stability: Resource isolation. As an infrastructure, agent must be stable and reliable. At the same time, it supports a large number of monitoring indicators by default, which has good support for common operation and maintenance problems and reduces the operation and maintenance burden. Extensibility: Convenient for users to extend and realize filtering, routing, encoding and other capabilities. For example, a processing logic can be written very quickly, and performed. To sum up, our ideal log agent should be: Out-of-the-box: Log collection services which can be quickly deployed in containerized scenarios; Complete documentation and introduction; High performance: higher performance and less resource usage than native Filebeat; High reliability: stronger isolation and stability; More integrated monitoring indicators by default to facilitate operation and maintenance troubleshooting; Extensibility: Based on the microkernel architecture, users can easily and quickly write their own plug-ins to meet various customization requirements;","title":"3\u3001What is Our Ideal Log Agent?"},{"location":"user-guide/architecture/compare/","text":"Comparison \u00b6 Loggie Filebeat Fluentd Logstash Flume Development Language Golang Golang Ruby JRuby Java Multiple Pipeline support single single support support Multiple Output Sources support single output config copy support support Transfer support not support support support support Log Alarm support not support not support not support not support Container Log Collection in Kubernetes support container stdout and container internal log files only container stdout only container stdout not support not support Configuration Delivery In Kubernetes, it can be configured through CRD. And host configuration center is gradually supported manual configuration manual configuration manual configuration manual configuration Monitor natively support Prometheus metrics, and can be configured to output metrics log files separately, send metrics, etc. The API interface is exposed, and an additional exporter is required to access Prometheus Support API and Prometheus metrics Need additional exporter Need additional exporter Resource Occupancy low low medium high high Benchmarks and Comparisons \u00b6 Test Environment: Physical machine 48C, 256G Kafka 3 Broker, mount SSD disk Filebeat v7.8 version, no processor configured; Loggie includes cost, retry, metric interceptor by default; Test Purposes: Performance comparison between Filebeat and Loggie Test Idea: Both Filebeat and Loggie collect logs and send them to Kafka. Observe the corresponding resource usage and sending throughput. Test Details: Automatically generate 5,000,000 lines of logs in a single file. The content of each line is as follows: [13/May/2021:10:20:29 +0800] 0.015 10.200.170.107 \"GET /static/3tJHS3Ubrf.html?activity_channel_id=22=1_00000&fromMiniapp=1&miniapp_uuid=uEd93lG2eG8Qj5fRXuiJwNt4bmiylkmg HTTP/1.1\" 200 138957 \"110.183.45.54, 10.200.151.37\" act.you.163.com \"\" \"Mozilla/5.0 (Linux; Android 8.1.0; PADM00Build/O11019; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/67.0.3396.87 XWEB/568 MMWEBSDK/190102 Mobile Safari/537.36 MMWEBID/6881 MicroMessenger/7.0.3.1400(0x2700033B) Process/appbrand0 NetType/WIFI Language/zh_CN miniProgram\" \"\" [127.0.0.1:8990] [0.014] [] [] immsg={\"st\":1553307293614,\"sb\":138963,\"rc\":200,\"cf\":{\"sr\":1},\"if\":\"default\",\"ut\":14,\"sv\":\"static\",\"pd\":\"activity\",\"qb\":764} Configure Filebeat and Loggie to collect logs and send them to a topic in Kafka without client-side compression. The Kafka topic is configured with 3 partitions. In the case of ensuring sufficient resources of the Agent, modify the number of files collected, the concurrency of the sending client (configure Filebeat worker and Loggie parallelism). Observe the sending rate of CPU, Memory, and Pod network. The test got the following data: Agent Fize Size Number of Log Files Sending Concurrency CPU MEM (rss) NIC sending rate Filebeat 3.2G 1 3 7.5~8.5c 63.8MiB 75.9MiB/s Filebeat 3.2G 1 8 10c 65MiB 70MiB/s Filebeat 3.2G 10 8 11c 65MiB 80MiB/s Loggie 3.2G 1 3 2.1c 60MiB 120MiB/s Loggie 3.2G 1 8 2.4c 68.7MiB 120MiB/s Loggie 3.2G 10 8 3.5c 70MiB 210MiB/s Test Conclusion: Under the same pressure test conditions and scenarios: The CPU consumption of Loggie is only about \u00bc of Filebeat, and the sending throughput is 1.6 to 2.6 times that of the Filebeat. Loggie and Filebeat have nearly same memory consumption. Both low. There is a bottleneck in the limit throughput of Filebeat, and it is difficult to increase after 80MB/s. Loggie can reach more than 200MiB/s.","title":"Progress and Tests"},{"location":"user-guide/architecture/compare/#comparison","text":"Loggie Filebeat Fluentd Logstash Flume Development Language Golang Golang Ruby JRuby Java Multiple Pipeline support single single support support Multiple Output Sources support single output config copy support support Transfer support not support support support support Log Alarm support not support not support not support not support Container Log Collection in Kubernetes support container stdout and container internal log files only container stdout only container stdout not support not support Configuration Delivery In Kubernetes, it can be configured through CRD. And host configuration center is gradually supported manual configuration manual configuration manual configuration manual configuration Monitor natively support Prometheus metrics, and can be configured to output metrics log files separately, send metrics, etc. The API interface is exposed, and an additional exporter is required to access Prometheus Support API and Prometheus metrics Need additional exporter Need additional exporter Resource Occupancy low low medium high high","title":"Comparison"},{"location":"user-guide/architecture/compare/#benchmarks-and-comparisons","text":"Test Environment: Physical machine 48C, 256G Kafka 3 Broker, mount SSD disk Filebeat v7.8 version, no processor configured; Loggie includes cost, retry, metric interceptor by default; Test Purposes: Performance comparison between Filebeat and Loggie Test Idea: Both Filebeat and Loggie collect logs and send them to Kafka. Observe the corresponding resource usage and sending throughput. Test Details: Automatically generate 5,000,000 lines of logs in a single file. The content of each line is as follows: [13/May/2021:10:20:29 +0800] 0.015 10.200.170.107 \"GET /static/3tJHS3Ubrf.html?activity_channel_id=22=1_00000&fromMiniapp=1&miniapp_uuid=uEd93lG2eG8Qj5fRXuiJwNt4bmiylkmg HTTP/1.1\" 200 138957 \"110.183.45.54, 10.200.151.37\" act.you.163.com \"\" \"Mozilla/5.0 (Linux; Android 8.1.0; PADM00Build/O11019; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/67.0.3396.87 XWEB/568 MMWEBSDK/190102 Mobile Safari/537.36 MMWEBID/6881 MicroMessenger/7.0.3.1400(0x2700033B) Process/appbrand0 NetType/WIFI Language/zh_CN miniProgram\" \"\" [127.0.0.1:8990] [0.014] [] [] immsg={\"st\":1553307293614,\"sb\":138963,\"rc\":200,\"cf\":{\"sr\":1},\"if\":\"default\",\"ut\":14,\"sv\":\"static\",\"pd\":\"activity\",\"qb\":764} Configure Filebeat and Loggie to collect logs and send them to a topic in Kafka without client-side compression. The Kafka topic is configured with 3 partitions. In the case of ensuring sufficient resources of the Agent, modify the number of files collected, the concurrency of the sending client (configure Filebeat worker and Loggie parallelism). Observe the sending rate of CPU, Memory, and Pod network. The test got the following data: Agent Fize Size Number of Log Files Sending Concurrency CPU MEM (rss) NIC sending rate Filebeat 3.2G 1 3 7.5~8.5c 63.8MiB 75.9MiB/s Filebeat 3.2G 1 8 10c 65MiB 70MiB/s Filebeat 3.2G 10 8 11c 65MiB 80MiB/s Loggie 3.2G 1 3 2.1c 60MiB 120MiB/s Loggie 3.2G 1 8 2.4c 68.7MiB 120MiB/s Loggie 3.2G 10 8 3.5c 70MiB 210MiB/s Test Conclusion: Under the same pressure test conditions and scenarios: The CPU consumption of Loggie is only about \u00bc of Filebeat, and the sending throughput is 1.6 to 2.6 times that of the Filebeat. Loggie and Filebeat have nearly same memory consumption. Both low. There is a bottleneck in the limit throughput of Filebeat, and it is difficult to increase after 80MB/s. Loggie can reach more than 200MiB/s.","title":"Benchmarks and Comparisons"},{"location":"user-guide/architecture/core-arch/","text":"Philosophy \u00b6 Architecture \u00b6 The detailed design of Loggie is shown in the following figure: Data flow : Multiple pipelines supported, and each pipeline is relatively independent. Each pipeline has multiple sources, a queue and a sink. The interceptor is actually divided into two different types: source interceptor and sink interceptor. The interceptor of the source can be configured to be exclusively owned by a source. Control flow : At present, Kubernetes is the configuration center, and log configuration can be issued through CRD. Of course, manual configuration is ok. Reloader can dynamically detect configuration changes and reload configurations. Components can send metrics corresponding to topics, which are consumed and processed by listeners, and further exposed or sent. Forms of Usage \u00b6 In essence, Loggie is a data transmission streaming, so we can use Loggie's Pipeline flexibly. From the form of usage, it can be divided into: Agent : One per node or one per Pod, used to collect logs or other data. Aggregator : Used for transit and forwarding, and can be deployed as a cluster independently. In addition to log collection, you can consider using Loggie in many scenarios involving data transmission and conversion. Even if Loggie does not have the components you want, you can quickly develop a source, sink or interceptor, and reuse many of Loggie's capabilities to avoid duplicate development work, such as: In the Kubernetes cluster, it is convenient to use the CRD to issue the configuration directly. Automatic reload and specified Loggie cluster are supported. Deployment, configuration updates and other issues do not need to be considered. Rely on Loggie to provide the stability and reliability of the transmission process, ensure at-least-once and retry mechanisms, avoidance of data loss, and protection from hidden dangers caused by excessive or excessive data volume. Using a series of monitoring indicators provided by Loggie, such as queue length, transmission delay, sending QPS, etc. Loggie can be quickly connected with Prometheus, and some built-in interfaces and capabilities for quick troubleshooting can be utilized. Pluggable Interceptor can be used for custom data processing, format conversion, etc., and to avoid excessive customized development ... Application scenarios \u00b6 Log collection: collect container logs, collect node logs Data transfer: log aggregation, forwarding, and offloading Data processing: data segmentation, transformation and processing Log alarm: detect and alarm abnormal logs ...","title":"Philosophy"},{"location":"user-guide/architecture/core-arch/#philosophy","text":"","title":"Philosophy"},{"location":"user-guide/architecture/core-arch/#architecture","text":"The detailed design of Loggie is shown in the following figure: Data flow : Multiple pipelines supported, and each pipeline is relatively independent. Each pipeline has multiple sources, a queue and a sink. The interceptor is actually divided into two different types: source interceptor and sink interceptor. The interceptor of the source can be configured to be exclusively owned by a source. Control flow : At present, Kubernetes is the configuration center, and log configuration can be issued through CRD. Of course, manual configuration is ok. Reloader can dynamically detect configuration changes and reload configurations. Components can send metrics corresponding to topics, which are consumed and processed by listeners, and further exposed or sent.","title":"Architecture"},{"location":"user-guide/architecture/core-arch/#forms-of-usage","text":"In essence, Loggie is a data transmission streaming, so we can use Loggie's Pipeline flexibly. From the form of usage, it can be divided into: Agent : One per node or one per Pod, used to collect logs or other data. Aggregator : Used for transit and forwarding, and can be deployed as a cluster independently. In addition to log collection, you can consider using Loggie in many scenarios involving data transmission and conversion. Even if Loggie does not have the components you want, you can quickly develop a source, sink or interceptor, and reuse many of Loggie's capabilities to avoid duplicate development work, such as: In the Kubernetes cluster, it is convenient to use the CRD to issue the configuration directly. Automatic reload and specified Loggie cluster are supported. Deployment, configuration updates and other issues do not need to be considered. Rely on Loggie to provide the stability and reliability of the transmission process, ensure at-least-once and retry mechanisms, avoidance of data loss, and protection from hidden dangers caused by excessive or excessive data volume. Using a series of monitoring indicators provided by Loggie, such as queue length, transmission delay, sending QPS, etc. Loggie can be quickly connected with Prometheus, and some built-in interfaces and capabilities for quick troubleshooting can be utilized. Pluggable Interceptor can be used for custom data processing, format conversion, etc., and to avoid excessive customized development ...","title":"Forms of Usage"},{"location":"user-guide/architecture/core-arch/#application-scenarios","text":"Log collection: collect container logs, collect node logs Data transfer: log aggregation, forwarding, and offloading Data processing: data segmentation, transformation and processing Log alarm: detect and alarm abnormal logs ...","title":"Application scenarios"},{"location":"user-guide/architecture/schema/","text":"Data Format \u00b6 Info Understanding the design of Loggie's internal data format can help us configure appropriate log processing and log format conversion. Structure \u00b6 Log data inside Loggie, including: body : The raw data received by the source. For example, a line of log data in a log file collected using file source. header : Fields used by the user. For example, the \"fields\" field that the user adds to the source, the fields obtained after log is splits, etc. meta : The built-in meta information of the Loggie system, which will not be sent to the downstream by default. Format Conversion \u00b6 If the above format does not meet your needs, you can refer to: Log Format and Metadata Log Segmentation \u00b6 For the segmentation and processing of raw log data, please use normalize interceptor . Refer to: Log Segmentation","title":"Data Format"},{"location":"user-guide/architecture/schema/#data-format","text":"Info Understanding the design of Loggie's internal data format can help us configure appropriate log processing and log format conversion.","title":"Data Format"},{"location":"user-guide/architecture/schema/#structure","text":"Log data inside Loggie, including: body : The raw data received by the source. For example, a line of log data in a log file collected using file source. header : Fields used by the user. For example, the \"fields\" field that the user adds to the source, the fields obtained after log is splits, etc. meta : The built-in meta information of the Loggie system, which will not be sent to the downstream by default.","title":"Structure"},{"location":"user-guide/architecture/schema/#format-conversion","text":"If the above format does not meet your needs, you can refer to: Log Format and Metadata","title":"Format Conversion"},{"location":"user-guide/architecture/schema/#log-segmentation","text":"For the segmentation and processing of raw log data, please use normalize interceptor . Refer to: Log Segmentation","title":"Log Segmentation"},{"location":"user-guide/best-practice/aggregator/","text":"Use Loggie Aggregator \u00b6 Loggie can be deployed as an Agent, and also support independent deployment for aggregation, forwarding and processing. Preparation: Choose Architecture \u00b6 There are many ways to use the aggregator architecture. Common ones are: Agent -> Aggregator : The Agent sends data directly to the Aggregator, and the Aggregator sends data to the backend storage. Agent -> MQ -> Aggregator : Agent sends data to a message queue, such as Kafka, and then Aggregator consumes Kafka messages and sends them to the backend. Whether to introduce message queues such as Kafka mainly depends on scenario requirements and magnitude of data. Preparation: Deploy \u00b6 Deploy Agent Deploy Aggregator When deploy Loggie as Aggregator, be sure to specify the cluster name in the Kubernetes configuration. Configuration \u00b6 Agent \u00b6 There is no difference in collection configuration of the LogConfig. You only need to modify the sink to send data to Loggie Aggregator or Kafka. For the creation of the containers (to be collected) and the matching LogConfig, please refer to Loggie collect container log \u3002 Here we modify the sink: Example Aggregator apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : aggregator spec : sink : | type: grpc host: \"loggie-aggregator.loggie-aggregator:6066\" Kafka apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : kafka spec : sink : | type: kafka brokers: [\"127.0.0.1:6400\"] topic: \"log-${fields.topic}\" Aggregator \u00b6 Configuring LogConfig to be delivered to the Aggregator is essentially the same. Only selector needs to be modified. Example apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : aggre spec : selector : type : cluster cluster : aggregator pipeline : sources : | - type: grpc name: rec1 port: 6066 sinkRef : dev apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : dev spec : sink : | type: dev printEvents: true codec: type: json pretty: true type: cluster indicates that the configuration is selected to be delivered to Loggie cluster specified by cluster , that is, the aggregator cluster we just deployed. If it is not filled in, the configuration will be assigned to the default agent cluster, which will not take effect. The source here is Grpc, which receives the data sent by the Agent Grpc sink, and then forwards it to the sink specified by its own sinkRef. Here we create a dev sink to view the data output by the aggregator. Logs \u00b6 Use kubectl -nloggie-aggregator logs -f <podName> --tail=200 to view logs similar to the following on the aggregator node: events 2021-12-20 09 : 58 : 50 INF go/src/loggie.io/loggie/pkg/si n k/dev/si n k.go : 98 > eve nt : { \"body\" : \"14-Dec-2021 06:19:58.306 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [141] milliseconds\" , \"fields\" : { \"podname\" : \"tomcat-684c698b66-gkrfs\" , \"containername\" : \"tomcat\" , \"logconfig\" : \"tomcat\" , \"namespace\" : \"default\" , \"nodename\" : \"kind-control-plane\" }, }","title":"Loggie Aggregator"},{"location":"user-guide/best-practice/aggregator/#use-loggie-aggregator","text":"Loggie can be deployed as an Agent, and also support independent deployment for aggregation, forwarding and processing.","title":"Use Loggie Aggregator"},{"location":"user-guide/best-practice/aggregator/#preparation-choose-architecture","text":"There are many ways to use the aggregator architecture. Common ones are: Agent -> Aggregator : The Agent sends data directly to the Aggregator, and the Aggregator sends data to the backend storage. Agent -> MQ -> Aggregator : Agent sends data to a message queue, such as Kafka, and then Aggregator consumes Kafka messages and sends them to the backend. Whether to introduce message queues such as Kafka mainly depends on scenario requirements and magnitude of data.","title":"Preparation: Choose Architecture"},{"location":"user-guide/best-practice/aggregator/#preparation-deploy","text":"Deploy Agent Deploy Aggregator When deploy Loggie as Aggregator, be sure to specify the cluster name in the Kubernetes configuration.","title":"Preparation: Deploy"},{"location":"user-guide/best-practice/aggregator/#configuration","text":"","title":"Configuration"},{"location":"user-guide/best-practice/aggregator/#agent","text":"There is no difference in collection configuration of the LogConfig. You only need to modify the sink to send data to Loggie Aggregator or Kafka. For the creation of the containers (to be collected) and the matching LogConfig, please refer to Loggie collect container log \u3002 Here we modify the sink: Example Aggregator apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : aggregator spec : sink : | type: grpc host: \"loggie-aggregator.loggie-aggregator:6066\" Kafka apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : kafka spec : sink : | type: kafka brokers: [\"127.0.0.1:6400\"] topic: \"log-${fields.topic}\"","title":"Agent"},{"location":"user-guide/best-practice/aggregator/#aggregator","text":"Configuring LogConfig to be delivered to the Aggregator is essentially the same. Only selector needs to be modified. Example apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : aggre spec : selector : type : cluster cluster : aggregator pipeline : sources : | - type: grpc name: rec1 port: 6066 sinkRef : dev apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : dev spec : sink : | type: dev printEvents: true codec: type: json pretty: true type: cluster indicates that the configuration is selected to be delivered to Loggie cluster specified by cluster , that is, the aggregator cluster we just deployed. If it is not filled in, the configuration will be assigned to the default agent cluster, which will not take effect. The source here is Grpc, which receives the data sent by the Agent Grpc sink, and then forwards it to the sink specified by its own sinkRef. Here we create a dev sink to view the data output by the aggregator.","title":"Aggregator"},{"location":"user-guide/best-practice/aggregator/#logs","text":"Use kubectl -nloggie-aggregator logs -f <podName> --tail=200 to view logs similar to the following on the aggregator node: events 2021-12-20 09 : 58 : 50 INF go/src/loggie.io/loggie/pkg/si n k/dev/si n k.go : 98 > eve nt : { \"body\" : \"14-Dec-2021 06:19:58.306 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [141] milliseconds\" , \"fields\" : { \"podname\" : \"tomcat-684c698b66-gkrfs\" , \"containername\" : \"tomcat\" , \"logconfig\" : \"tomcat\" , \"namespace\" : \"default\" , \"nodename\" : \"kind-control-plane\" }, }","title":"Logs"},{"location":"user-guide/best-practice/log-enrich/","text":"Log format and Meta Information Fields \u00b6 It is recommended to first understand the schema design of Loggie's internal log data schema design . Loggie is deployed in different environments. If you need to add some meta information to the original log data and want it be compatible with existing formats, you can refer to the following methods. Field Format Conversion \u00b6 Use Schema Interceptor \u00b6 Use schema interceptor to add time fields, as well as pipelineName and sourceName fields. In addition, you can rename the field, such as modify body to message . Refer to schema interceptor . In most cases, we need the configuration to take effect globally instead of just adding the interceptor in a pipeline. Therefore, it is recommended to add the schema interceptor to the defaults of the system configuration, so as to avoid to configure the interceptor for each pipeline. loggie.yml loggie : defaults : interceptors : - type : schema name : global order : 700 addMeta : timestamp : key : \"@timestamp\" remap : body : key : message The name here is for identification, to avoid that when a schema interceptor is added to the pipeline, the verification will fail. In addition, set the order field to a smaller value (the default is 900), so that the interceptor in default will be executed prior to other interceptors defined in the pipeline. Use Transformer Interceptor \u00b6 Tranformer provides richer functions and can deal with complex log scenarios. Please refer to transformer interceptor \u3002 Add Meta Information \u00b6 Add Custom Meta Information to Fields \u00b6 If we configure some custom fields on the source. pipelines.yml pipelines : - name : local sources : - type : file name : demo paths : - /tmp/log/*.log fields : topic : \"loggie\" sink : type : dev printEvents : true codec : pretty : true Then the sink output is: { \"fields\" : { \"topic\" : \"loggie\" , }, \"body\" : \"01-Dec-2021 03:13:58.298 INFO [main] Starting service [Catalina]\" } We can also configure fieldsUnderRoot: true , so that key:value are at the same level as body. pipelines.yml pipelines : - name : local sources : - type : file fields : topic : \"loggie\" fieldsUnderRoot : true ... { \"topic\" : \"loggie\" , \"body\" : \"01-Dec-2021 03:13:58.298 INFO [main] Starting service [Catalina]\" } Add Log Collection Status Information of File Source \u00b6 When we use the file source, we may want to automatically add some log collection status to the raw data, such as the name of the collected file, the offset of the collected file, etc. The file source provides a addonMeta configuration that can be quickly enabled. Example: add addonMeta: true . file source sources : - type : file paths : - /var/log/*.log addonMeta : true At this point, the collected events will become similar to the following: Example { \"body\" : \"this is test\" , \"state\" : { \"pipeline\" : \"local\" , \"source\" : \"demo\" , \"filename\" : \"/var/log/a.log\" , \"timestamp\" : \"2006-01-02T15:04:05.000Z\" , \"offset\" : 1024 , \"bytes\" : 4096 , \"hostname\" : \"node-1\" } } For specific field meanings, please refer to file source Add Kubernetes Meta Information \u00b6 In Kubernetes, in order to retrieve the collected container logs using the namespace/podName and other information during query, it is often necessary to add relevant metadata. We can configure additional k8s fields in discovery.kubernetes of the system configuration. See discovery \u3002 Add System Built-in Meta Information \u00b6 There is some built-in meta information in the Loggie system, and we also want to send it to the downstream. At this time, we need to use the addMeta processor in the normalize interceptor. (It should be noted that this operation will have a certain impact on the collection and transmission performance. Under normal circumstances, this method is not recommended) pipelines.yml pipelines : - name : local sources : - type : file name : demo paths : - /tmp/log/*.log fields : topic : \"loggie\" interceptors : - type : normalize processors : - addMeta : ~ sink : type : dev printEvents : true codec : pretty : true After the addMeta processor is configured, all the built-in meta information of the system will be output by default. The default Json format output example is as follows: Example { \"fields\" : { \"topic\" : \"loggie\" }, \"meta\" : { \"systemState\" : { \"nextOffset\" : 720 , \"filename\" : \"/tmp/log/a.log\" , \"collectTime\" : \"2022-03-08T11:33:47.369813+08:00\" , \"contentBytes\" : 90 , \"jobUid\" : \"43772050-16777231\" , \"lineNumber\" : 8 , \"offset\" : 630 }, \"systemProductTime\" : \"2022-03-08T11:33:47.370166+08:00\" , \"systemPipelineName\" : \"local\" , \"systemSourceName\" : \"demo\" }, \"body\" : \"01-Dec-2021 03:13:58.298 INFO [main] Starting service [Catalina]\" } We may feel that this data is too much, or want to modify the field. We can use the action in the transformer interceptor .","title":"Log Format and Metadata"},{"location":"user-guide/best-practice/log-enrich/#log-format-and-meta-information-fields","text":"It is recommended to first understand the schema design of Loggie's internal log data schema design . Loggie is deployed in different environments. If you need to add some meta information to the original log data and want it be compatible with existing formats, you can refer to the following methods.","title":"Log format and Meta Information Fields"},{"location":"user-guide/best-practice/log-enrich/#field-format-conversion","text":"","title":"Field Format Conversion"},{"location":"user-guide/best-practice/log-enrich/#use-schema-interceptor","text":"Use schema interceptor to add time fields, as well as pipelineName and sourceName fields. In addition, you can rename the field, such as modify body to message . Refer to schema interceptor . In most cases, we need the configuration to take effect globally instead of just adding the interceptor in a pipeline. Therefore, it is recommended to add the schema interceptor to the defaults of the system configuration, so as to avoid to configure the interceptor for each pipeline. loggie.yml loggie : defaults : interceptors : - type : schema name : global order : 700 addMeta : timestamp : key : \"@timestamp\" remap : body : key : message The name here is for identification, to avoid that when a schema interceptor is added to the pipeline, the verification will fail. In addition, set the order field to a smaller value (the default is 900), so that the interceptor in default will be executed prior to other interceptors defined in the pipeline.","title":"Use Schema Interceptor"},{"location":"user-guide/best-practice/log-enrich/#use-transformer-interceptor","text":"Tranformer provides richer functions and can deal with complex log scenarios. Please refer to transformer interceptor \u3002","title":"Use Transformer Interceptor"},{"location":"user-guide/best-practice/log-enrich/#add-meta-information","text":"","title":"Add Meta Information"},{"location":"user-guide/best-practice/log-enrich/#add-custom-meta-information-to-fields","text":"If we configure some custom fields on the source. pipelines.yml pipelines : - name : local sources : - type : file name : demo paths : - /tmp/log/*.log fields : topic : \"loggie\" sink : type : dev printEvents : true codec : pretty : true Then the sink output is: { \"fields\" : { \"topic\" : \"loggie\" , }, \"body\" : \"01-Dec-2021 03:13:58.298 INFO [main] Starting service [Catalina]\" } We can also configure fieldsUnderRoot: true , so that key:value are at the same level as body. pipelines.yml pipelines : - name : local sources : - type : file fields : topic : \"loggie\" fieldsUnderRoot : true ... { \"topic\" : \"loggie\" , \"body\" : \"01-Dec-2021 03:13:58.298 INFO [main] Starting service [Catalina]\" }","title":"Add Custom Meta Information to Fields"},{"location":"user-guide/best-practice/log-enrich/#add-log-collection-status-information-of-file-source","text":"When we use the file source, we may want to automatically add some log collection status to the raw data, such as the name of the collected file, the offset of the collected file, etc. The file source provides a addonMeta configuration that can be quickly enabled. Example: add addonMeta: true . file source sources : - type : file paths : - /var/log/*.log addonMeta : true At this point, the collected events will become similar to the following: Example { \"body\" : \"this is test\" , \"state\" : { \"pipeline\" : \"local\" , \"source\" : \"demo\" , \"filename\" : \"/var/log/a.log\" , \"timestamp\" : \"2006-01-02T15:04:05.000Z\" , \"offset\" : 1024 , \"bytes\" : 4096 , \"hostname\" : \"node-1\" } } For specific field meanings, please refer to file source","title":"Add Log Collection Status Information of File Source"},{"location":"user-guide/best-practice/log-enrich/#add-kubernetes-meta-information","text":"In Kubernetes, in order to retrieve the collected container logs using the namespace/podName and other information during query, it is often necessary to add relevant metadata. We can configure additional k8s fields in discovery.kubernetes of the system configuration. See discovery \u3002","title":"Add Kubernetes Meta Information"},{"location":"user-guide/best-practice/log-enrich/#add-system-built-in-meta-information","text":"There is some built-in meta information in the Loggie system, and we also want to send it to the downstream. At this time, we need to use the addMeta processor in the normalize interceptor. (It should be noted that this operation will have a certain impact on the collection and transmission performance. Under normal circumstances, this method is not recommended) pipelines.yml pipelines : - name : local sources : - type : file name : demo paths : - /tmp/log/*.log fields : topic : \"loggie\" interceptors : - type : normalize processors : - addMeta : ~ sink : type : dev printEvents : true codec : pretty : true After the addMeta processor is configured, all the built-in meta information of the system will be output by default. The default Json format output example is as follows: Example { \"fields\" : { \"topic\" : \"loggie\" }, \"meta\" : { \"systemState\" : { \"nextOffset\" : 720 , \"filename\" : \"/tmp/log/a.log\" , \"collectTime\" : \"2022-03-08T11:33:47.369813+08:00\" , \"contentBytes\" : 90 , \"jobUid\" : \"43772050-16777231\" , \"lineNumber\" : 8 , \"offset\" : 630 }, \"systemProductTime\" : \"2022-03-08T11:33:47.370166+08:00\" , \"systemPipelineName\" : \"local\" , \"systemSourceName\" : \"demo\" }, \"body\" : \"01-Dec-2021 03:13:58.298 INFO [main] Starting service [Catalina]\" } We may feel that this data is too much, or want to modify the field. We can use the action in the transformer interceptor .","title":"Add System Built-in Meta Information"},{"location":"user-guide/best-practice/log-process/","text":"Log Segmentation \u00b6 Loggie can use transformer interceptor to segment and process the log, extract the log data in a structured manner, and process the extracted fields. It is recommended to understand schema design of Loggie's log data. Requirements \u00b6 The thing is to segment, parse, extract and process logs. For example: 01-Dec-2021 03:13:58.298 INFO [main] Starting service [Catalina] We may need to parse out the date and log level: { \"time\" : \"01-Dec-2021 03:13:58.298\" , \"level\" : \"INFO\" , \"message\" : \"[main] Starting service [Catalina]\" } This structured data is easy to filter and query when stored, or to sort according to the time in the log instead of the time when it is collected, or to filter according to the log level, which can facilitate the query of ERROR-level logs and so on. Similar requirements and usage scenarios may need not only the operation and maintenance logs like the tomcat logs above, or logs such as some business orders, etc. Parsing and extraction of stdout logs The following example only provides a reference idea for log segmentation. If you need to extract the original logs of the standard output of the container, please refer to Collect Container logs . Configuration \u00b6 Log segmentation can be performed on the Loggie Agent or on the Loggie Aggregator, depending on whether we need an Aggregator, and whether we want the CPU-intensive computation of log processing to be distributed on the Agent and undertaken by each node, or we want to do it in the Aggregator clusters. The following uses the collection of access logs of the tomcat service as an example to show how to segment the access logs by fields. For simplicity, the example uses the CRD instance to deliver configuration to the Agent, and uses the dev sink to directly output the processed results for display. Create Tomcat Deployment \u00b6 See Reference Create Logconfig \u00b6 Configure logconfig as follows: Example apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : tomcat namespace : default spec : selector : labelSelector : app : tomcat type : pod pipeline : sources : | - type: file name: access paths: - /usr/local/tomcat/logs/localhost_access_log.*.txt interceptors : | - type: transformer actions: - action: regex(body) pattern: (?<ip>\\S+) (?<id>\\S+) (?<u>\\S+) (?<time>\\[.*?\\]) (?<url>\\\".*?\\\") (?<status>\\S+) (?<size>\\S+) sink : | type: dev printEvents: true codec: type: json pretty: true Here we configure the regex action in the transformer interceptors to perform regular extraction for access log. The original access log looks like this: 10.244.0.1 - - [31/Aug/2022:03:13:40 +0000] \"GET / HTTP/1.1\" 404 683 After logs are processed by the transformer, we can kubectl -nloggie logs -f <loggie-pod-name> --tail=100 to view the output log. An example of the converted event is as follows: { \"status\" : \"404\" , \"size\" : \"683\" , \"fields\" : { \"logconfig\" : \"tomcat\" , \"namespace\" : \"test1\" , \"nodename\" : \"kind-control-plane\" , \"podname\" : \"tomcat-85c84988d8-frs4n\" , \"containername\" : \"tomcat\" }, \"ip\" : \"10.244.0.1\" , \"id\" : \"-\" , \"u\" : \"-\" , \"time\" : \"[31/Aug/2022:03:13:40 +0000]\" , \"url\" : \"\\\"GET / HTTP/1.1\\\"\" }","title":"Log Segmentation"},{"location":"user-guide/best-practice/log-process/#log-segmentation","text":"Loggie can use transformer interceptor to segment and process the log, extract the log data in a structured manner, and process the extracted fields. It is recommended to understand schema design of Loggie's log data.","title":"Log Segmentation"},{"location":"user-guide/best-practice/log-process/#requirements","text":"The thing is to segment, parse, extract and process logs. For example: 01-Dec-2021 03:13:58.298 INFO [main] Starting service [Catalina] We may need to parse out the date and log level: { \"time\" : \"01-Dec-2021 03:13:58.298\" , \"level\" : \"INFO\" , \"message\" : \"[main] Starting service [Catalina]\" } This structured data is easy to filter and query when stored, or to sort according to the time in the log instead of the time when it is collected, or to filter according to the log level, which can facilitate the query of ERROR-level logs and so on. Similar requirements and usage scenarios may need not only the operation and maintenance logs like the tomcat logs above, or logs such as some business orders, etc. Parsing and extraction of stdout logs The following example only provides a reference idea for log segmentation. If you need to extract the original logs of the standard output of the container, please refer to Collect Container logs .","title":"Requirements"},{"location":"user-guide/best-practice/log-process/#configuration","text":"Log segmentation can be performed on the Loggie Agent or on the Loggie Aggregator, depending on whether we need an Aggregator, and whether we want the CPU-intensive computation of log processing to be distributed on the Agent and undertaken by each node, or we want to do it in the Aggregator clusters. The following uses the collection of access logs of the tomcat service as an example to show how to segment the access logs by fields. For simplicity, the example uses the CRD instance to deliver configuration to the Agent, and uses the dev sink to directly output the processed results for display.","title":"Configuration"},{"location":"user-guide/best-practice/log-process/#create-tomcat-deployment","text":"See Reference","title":"Create Tomcat Deployment"},{"location":"user-guide/best-practice/log-process/#create-logconfig","text":"Configure logconfig as follows: Example apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : tomcat namespace : default spec : selector : labelSelector : app : tomcat type : pod pipeline : sources : | - type: file name: access paths: - /usr/local/tomcat/logs/localhost_access_log.*.txt interceptors : | - type: transformer actions: - action: regex(body) pattern: (?<ip>\\S+) (?<id>\\S+) (?<u>\\S+) (?<time>\\[.*?\\]) (?<url>\\\".*?\\\") (?<status>\\S+) (?<size>\\S+) sink : | type: dev printEvents: true codec: type: json pretty: true Here we configure the regex action in the transformer interceptors to perform regular extraction for access log. The original access log looks like this: 10.244.0.1 - - [31/Aug/2022:03:13:40 +0000] \"GET / HTTP/1.1\" 404 683 After logs are processed by the transformer, we can kubectl -nloggie logs -f <loggie-pod-name> --tail=100 to view the output log. An example of the converted event is as follows: { \"status\" : \"404\" , \"size\" : \"683\" , \"fields\" : { \"logconfig\" : \"tomcat\" , \"namespace\" : \"test1\" , \"nodename\" : \"kind-control-plane\" , \"podname\" : \"tomcat-85c84988d8-frs4n\" , \"containername\" : \"tomcat\" }, \"ip\" : \"10.244.0.1\" , \"id\" : \"-\" , \"u\" : \"-\" , \"time\" : \"[31/Aug/2022:03:13:40 +0000]\" , \"url\" : \"\\\"GET / HTTP/1.1\\\"\" }","title":"Create Logconfig"},{"location":"user-guide/enterprise-practice/architecture-and-evolution/","text":"Logging System Architecture and Evolution \u00b6 Bases on different business types, different scenarios, and different log scales, we may adopt different log system architectures. There is no good or bad architecture, only suitable architecture. In a simple scenario, a log system built with a complex architecture may bring O&M disasters. Here is a summary of common log system architectures from the perspective of scale evolution. Of course, there are many actual technical options and variants, and we cannot list them one by one. I believe you can build an architecture suitable for your own business by referring to the following. Architecture Evolution \u00b6 It should be noted in advance that: The following log storage example is Elasticsearch, and the message queue is Kafka. The specific selection should be determined according to the actual situation . The following scale data volume is only a reference value. More factors need to be comprehensively considered for the specific implementation situation. At the same time, different selections can be integrated to achieve the most suitable architecture. Small-scale Business Scenarios \u00b6 The daily log scale is small, for example, only about a few hundred gigabytes (estimated below 500G). The scenario of the log is only used for daily operation and maintenance troubleshooting. Loggie can be used to send log directly to the Elasticsearch cluster. The architecture diagram is shown below: advantage: Simple structure and easy maintenance disadvantage: Due to the limited performance of Elasticsearch, when the log amount suddenly increases, the direct sending of the agent may cause a large number of retries or failures, resulting in the instability of Elasticsearch Poor scalability Variant: Elasticsearch is the most commonly used log store due to the popularity of the ELK architecture. If there are other services that depend on Elasticsearch, or if you have experience in the operation and maintenance of Elasticsearch, Elasticsearch is a good choice. However, Elasticsearch has certain requirements on resources and operation and maintenance. In some lightweight and resource-sensitive environments, you can consider: Store with Loki If you have relevant technical resources, you can also consider sending log to Clickhouse/Hive/Doris, etc. Medium-scale business scenarios \u00b6 When the daily log level is slightly larger, for example, 500G to 1T, and there are considerations in scalability in terms of architecture and business use, the introduction of a Loggie Aggregator cluster can be considered. advantage: The aggregator cluster can undertake log segmentation and other capabilities. The aggregator cluster has a certain buffer capacity disadvantage: The buffering capacity is weaker than the message queue. Large-scale business scenarios \u00b6 If the log volume is large (more than 1T) and the performance and stability are required, you can consider using a message queue cluster such as Kafka. It should be noted that Kafka itself cannot directly send data to the backend, so it is necessary to consider how to import Kafka data into the backend storage in real time. At this time, we can choose some components to consume Kafka and send data to the backend, such as Loggie/Logstash/Kafka connect/Flink, etc. However, Flink is suitable for enterprises with their own real-time streaming platform or operation and maintenance capabilities, otherwise it may introduce more operation and maintenance costs. advantage: Using message queues such as Kafka, you can realize caching and peak reduction. Allows more consumers to consume Kafka, providing more scalability Ultra-large-scale business scenarios \u00b6 Dozens of TB to PB level. Compared with the above large-scale scenarios, the number of clusters is larger, and the architecture is more complex. More flexible extension can be added according to the above architecture. For example: Use Loggie's multi-pipeline feature to split business logs and send them to multiple Kafka clusters. Add a front-end Loggie aggregator cluster in a large-scale architecture, and perform traffic distribution and forwarding in advance. More \u00b6 In fact, in order to implement a complete log architecture, you also need to consider: Reliable: The entire link of collection, transmission, processing, and query needs to ensure that logs are not lost as much as possible, and that logs do not affect business stability. Observable and easy to troubleshoot: The system must have complete monitoring indicators, and there must be quick troubleshooting methods for problems or failures to reduce the cost of human operation and maintenance. Good performance: Collection, transmission and processing need to be lightweight and do not take up too many resources. And in the case of large data volumes, low latency and high throughput are also required. Easy to use: Log collection is easy to configure, reducing business usage costs. Do not intrude on application deployment. Complete: Meet the requirements of log processing, querying, monitoring and alarming, etc.","title":"Logging System Architecture and Evolution"},{"location":"user-guide/enterprise-practice/architecture-and-evolution/#logging-system-architecture-and-evolution","text":"Bases on different business types, different scenarios, and different log scales, we may adopt different log system architectures. There is no good or bad architecture, only suitable architecture. In a simple scenario, a log system built with a complex architecture may bring O&M disasters. Here is a summary of common log system architectures from the perspective of scale evolution. Of course, there are many actual technical options and variants, and we cannot list them one by one. I believe you can build an architecture suitable for your own business by referring to the following.","title":"Logging System Architecture and Evolution"},{"location":"user-guide/enterprise-practice/architecture-and-evolution/#architecture-evolution","text":"It should be noted in advance that: The following log storage example is Elasticsearch, and the message queue is Kafka. The specific selection should be determined according to the actual situation . The following scale data volume is only a reference value. More factors need to be comprehensively considered for the specific implementation situation. At the same time, different selections can be integrated to achieve the most suitable architecture.","title":"Architecture Evolution"},{"location":"user-guide/enterprise-practice/architecture-and-evolution/#small-scale-business-scenarios","text":"The daily log scale is small, for example, only about a few hundred gigabytes (estimated below 500G). The scenario of the log is only used for daily operation and maintenance troubleshooting. Loggie can be used to send log directly to the Elasticsearch cluster. The architecture diagram is shown below: advantage: Simple structure and easy maintenance disadvantage: Due to the limited performance of Elasticsearch, when the log amount suddenly increases, the direct sending of the agent may cause a large number of retries or failures, resulting in the instability of Elasticsearch Poor scalability Variant: Elasticsearch is the most commonly used log store due to the popularity of the ELK architecture. If there are other services that depend on Elasticsearch, or if you have experience in the operation and maintenance of Elasticsearch, Elasticsearch is a good choice. However, Elasticsearch has certain requirements on resources and operation and maintenance. In some lightweight and resource-sensitive environments, you can consider: Store with Loki If you have relevant technical resources, you can also consider sending log to Clickhouse/Hive/Doris, etc.","title":"Small-scale Business Scenarios"},{"location":"user-guide/enterprise-practice/architecture-and-evolution/#medium-scale-business-scenarios","text":"When the daily log level is slightly larger, for example, 500G to 1T, and there are considerations in scalability in terms of architecture and business use, the introduction of a Loggie Aggregator cluster can be considered. advantage: The aggregator cluster can undertake log segmentation and other capabilities. The aggregator cluster has a certain buffer capacity disadvantage: The buffering capacity is weaker than the message queue.","title":"Medium-scale business scenarios"},{"location":"user-guide/enterprise-practice/architecture-and-evolution/#large-scale-business-scenarios","text":"If the log volume is large (more than 1T) and the performance and stability are required, you can consider using a message queue cluster such as Kafka. It should be noted that Kafka itself cannot directly send data to the backend, so it is necessary to consider how to import Kafka data into the backend storage in real time. At this time, we can choose some components to consume Kafka and send data to the backend, such as Loggie/Logstash/Kafka connect/Flink, etc. However, Flink is suitable for enterprises with their own real-time streaming platform or operation and maintenance capabilities, otherwise it may introduce more operation and maintenance costs. advantage: Using message queues such as Kafka, you can realize caching and peak reduction. Allows more consumers to consume Kafka, providing more scalability","title":"Large-scale business scenarios"},{"location":"user-guide/enterprise-practice/architecture-and-evolution/#ultra-large-scale-business-scenarios","text":"Dozens of TB to PB level. Compared with the above large-scale scenarios, the number of clusters is larger, and the architecture is more complex. More flexible extension can be added according to the above architecture. For example: Use Loggie's multi-pipeline feature to split business logs and send them to multiple Kafka clusters. Add a front-end Loggie aggregator cluster in a large-scale architecture, and perform traffic distribution and forwarding in advance.","title":"Ultra-large-scale business scenarios"},{"location":"user-guide/enterprise-practice/architecture-and-evolution/#more","text":"In fact, in order to implement a complete log architecture, you also need to consider: Reliable: The entire link of collection, transmission, processing, and query needs to ensure that logs are not lost as much as possible, and that logs do not affect business stability. Observable and easy to troubleshoot: The system must have complete monitoring indicators, and there must be quick troubleshooting methods for problems or failures to reduce the cost of human operation and maintenance. Good performance: Collection, transmission and processing need to be lightweight and do not take up too many resources. And in the case of large data volumes, low latency and high throughput are also required. Easy to use: Log collection is easy to configure, reducing business usage costs. Do not intrude on application deployment. Complete: Meet the requirements of log processing, querying, monitoring and alarming, etc.","title":"More"},{"location":"user-guide/enterprise-practice/sls/","text":"Using Alibaba Cloud Observable Unified Storage SLS \u00b6 Regardless of whether your service is deployed on Alibaba Cloud or using a hybrid cloud architecture, Loggie supports collecting log data and send it to Alibaba Cloud SLS . And you just need to use sls sink . For the use of Loggie in Alibaba Cloud's official documentation, please refer to: upload logs with Loggie . Preparation: Create SLS Project \u00b6 In order to use Alibaba Cloud Observable Unified Storage SLS, we need to create a corresponding Project first. As shown below, on SLS page, click Create Project , fill in the project name and the corresponding region, and create the corresponding Logstore. Then we can prepare sls sink configuration. The example is as follows. Please refer to sls sink configuration for details: sls sink sink : type : sls name : demo endpoint : cn-hangzhou.log.aliyuncs.com accessKeyId : xxxx accessKeySecret : xxxx project : test logstore : test1 topic : myservice Collect Logs on ECS \u00b6 Refer to Deploy on hosts . We can modify the pipeline.yml and add the configuration of the sls sink. The pipeline configurationexample is as follows: sls pipeline pipelines : - name : test sources : - type : file name : demo addonMeta : true paths : - /tmp/log/*.log sink : type : sls endpoint : cn-hangzhou.log.aliyuncs.com accessKeyId : ${accessKeyId} accessKeySecret : ${accessKeySecret} project : loggietest logstore : demo1 topic : myservice Collect Logs of ACK Kubernetes Cluster \u00b6 Like the self-built Kubernetes cluster, Loggie can also be deployed in Alibaba Cloud ACK. We can use kubectl or helm to deploy according to the kubeconfig file provided by ACK. Refer to Deploy in Kubernetes . Node: If you want to collect Pod logs that are not mounted with emptyDir/hostPath, make sure that the system configuration in values.yaml discovery.kubernetes.rootFsCollectionEnabled set true. Configure discovery.kubernetes.containerRuntime to match the Kubernetes cluster. The default is containerd. After deployment, similar to the usage in conventional Kubernetes cluster, ClusterLogConfig/LogConfig/Interceptor/Sink CR can also be used. ACK provides a CRD page, and we can create and modify it directly in this page. For an example of collecting logs of a service, just use sls sink . You can also create a sink CR storage sls sink configuration separately and refer to it through sinkRef in LogConfig. logconfig apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : tomcat namespace : default spec : selector : labelSelector : app : tomcat type : pod pipeline : sources : | - type: file name: stdout paths: - stdout - type: file name: access ignoreOlder: 3d paths: - /usr/local/tomcat/logs/*.log sink : | type: sls endpoint: cn-hangzhou.log.aliyuncs.com accessKeyId: ${accessKeyId} accessKeySecret: ${accessKeySecret} project: loggietest logstore: demo1 topic: myservice View logs \u00b6 We can view the collected logs in the specific Project of SLS, as shown in the following figure: For details on adding meta information and modifying log fields, refer to Log Segmentation and Add Meta Info \u3002","title":"Using Alibaba SLS Sink"},{"location":"user-guide/enterprise-practice/sls/#using-alibaba-cloud-observable-unified-storage-sls","text":"Regardless of whether your service is deployed on Alibaba Cloud or using a hybrid cloud architecture, Loggie supports collecting log data and send it to Alibaba Cloud SLS . And you just need to use sls sink . For the use of Loggie in Alibaba Cloud's official documentation, please refer to: upload logs with Loggie .","title":"Using Alibaba Cloud Observable Unified Storage SLS"},{"location":"user-guide/enterprise-practice/sls/#preparation-create-sls-project","text":"In order to use Alibaba Cloud Observable Unified Storage SLS, we need to create a corresponding Project first. As shown below, on SLS page, click Create Project , fill in the project name and the corresponding region, and create the corresponding Logstore. Then we can prepare sls sink configuration. The example is as follows. Please refer to sls sink configuration for details: sls sink sink : type : sls name : demo endpoint : cn-hangzhou.log.aliyuncs.com accessKeyId : xxxx accessKeySecret : xxxx project : test logstore : test1 topic : myservice","title":"Preparation: Create SLS Project"},{"location":"user-guide/enterprise-practice/sls/#collect-logs-on-ecs","text":"Refer to Deploy on hosts . We can modify the pipeline.yml and add the configuration of the sls sink. The pipeline configurationexample is as follows: sls pipeline pipelines : - name : test sources : - type : file name : demo addonMeta : true paths : - /tmp/log/*.log sink : type : sls endpoint : cn-hangzhou.log.aliyuncs.com accessKeyId : ${accessKeyId} accessKeySecret : ${accessKeySecret} project : loggietest logstore : demo1 topic : myservice","title":"Collect Logs on ECS"},{"location":"user-guide/enterprise-practice/sls/#collect-logs-of-ack-kubernetes-cluster","text":"Like the self-built Kubernetes cluster, Loggie can also be deployed in Alibaba Cloud ACK. We can use kubectl or helm to deploy according to the kubeconfig file provided by ACK. Refer to Deploy in Kubernetes . Node: If you want to collect Pod logs that are not mounted with emptyDir/hostPath, make sure that the system configuration in values.yaml discovery.kubernetes.rootFsCollectionEnabled set true. Configure discovery.kubernetes.containerRuntime to match the Kubernetes cluster. The default is containerd. After deployment, similar to the usage in conventional Kubernetes cluster, ClusterLogConfig/LogConfig/Interceptor/Sink CR can also be used. ACK provides a CRD page, and we can create and modify it directly in this page. For an example of collecting logs of a service, just use sls sink . You can also create a sink CR storage sls sink configuration separately and refer to it through sinkRef in LogConfig. logconfig apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : tomcat namespace : default spec : selector : labelSelector : app : tomcat type : pod pipeline : sources : | - type: file name: stdout paths: - stdout - type: file name: access ignoreOlder: 3d paths: - /usr/local/tomcat/logs/*.log sink : | type: sls endpoint: cn-hangzhou.log.aliyuncs.com accessKeyId: ${accessKeyId} accessKeySecret: ${accessKeySecret} project: loggietest logstore: demo1 topic: myservice","title":"Collect Logs of ACK Kubernetes Cluster"},{"location":"user-guide/enterprise-practice/sls/#view-logs","text":"We can view the collected logs in the specific Project of SLS, as shown in the following figure: For details on adding meta information and modifying log fields, refer to Log Segmentation and Add Meta Info \u3002","title":"View logs"},{"location":"user-guide/monitor/loggie-monitor/","text":"Loggie's Monitoring and Alarming \u00b6 Loggie's monitor eventbus is designed in a publish and subscribe mode. Each component sends metrics to a specified topic, which is consumed and processed by an independent listener. For example, file source collects some indicators from the collected logs and send them to filesource topic . After aggregated and calculated by filesource listener , these indicators will be printed and exposed as Prometheus indicators. There is a loose coupling relationship between components, topics and listeners. For example, file source will regularly send the full matching indicators to filewatcher topic , filewatcher listener process and expose the indicators. Monitor Configuration \u00b6 The monitor eventbus is configured in the global system configuration, the example is as follows: Config loggie : monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ queue : ~ sink : ~ http : enabled : true port : 9196 logger controlls the log printing of all metrics indicators. Metrics generated by the configured listeners will be aggregated and printed in Loggie log at fixed intervals (set by period ), which is convenient for backtracking and troubleshooting. listeners is used to configure whether the related listener is enabled. Prometheus metrics are exposed at /metrics on http.port by default. You can curl <podIp>:9196/metrics to view the current metrics. Core Indicators of Log Collection \u00b6 Currently there are the following listeners: filesource : The indicator data about current log collection, such as which files are currently being collected, and what is the collection status? filewatcher : Timed full traversal (default 5min) to check all files matching the path, monitor the global collection status, and determine whether there are files that have not been collected in time, etc. reload : Number of reloads queue : Queue status sink : Metrics of sending, such as the number of successes or failures, etc. Deploy Prometheus and Grafana \u00b6 You can use the existing Prometheus or Grafana. If you need a new deployment, please refer to https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack . Deploy with Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -nprometheus --create-namespace Note Due to some reasons, some of these k8s.gcr.io images may not be available for download. You can consider downloading the chart package to replace it and then redeploy it. After confirming that the Pod is running normally, you can access grafana. The way to access grafana through a proxy can be referred to: kubectl -nprometheus port-forward --address 0 .0.0.0 service/prometheus-grafana 8181 :80 Grafana username and password can be viewed in prometheus-grafana sercret. Use base64 -d . Added Loggie Prometheus Monitoring \u00b6 In the Kubernetes cluster where Loggie is deployed, create the following ServiceMonitor to allow Prometheus to collect Loggie Agent metrics. apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app : loggie release : prometheus name : loggie-agent namespace : prometheus spec : namespaceSelector : matchNames : - loggie endpoints : - port : monitor selector : matchLabels : app : loggie instance : loggie At the same time, we need to add the json in the install project to Grafana to display Loggie's monitoring console. Note The Kubernetes version and the Grafana version may be different, which may lead to incompatible chart display. Modify as you need. The imported Grafana chart is as shown following:","title":"Monitoring and Alarming for Loggie"},{"location":"user-guide/monitor/loggie-monitor/#loggies-monitoring-and-alarming","text":"Loggie's monitor eventbus is designed in a publish and subscribe mode. Each component sends metrics to a specified topic, which is consumed and processed by an independent listener. For example, file source collects some indicators from the collected logs and send them to filesource topic . After aggregated and calculated by filesource listener , these indicators will be printed and exposed as Prometheus indicators. There is a loose coupling relationship between components, topics and listeners. For example, file source will regularly send the full matching indicators to filewatcher topic , filewatcher listener process and expose the indicators.","title":"Loggie's Monitoring and Alarming"},{"location":"user-guide/monitor/loggie-monitor/#monitor-configuration","text":"The monitor eventbus is configured in the global system configuration, the example is as follows: Config loggie : monitor : logger : period : 30s enabled : true listeners : filesource : ~ filewatcher : ~ reload : ~ queue : ~ sink : ~ http : enabled : true port : 9196 logger controlls the log printing of all metrics indicators. Metrics generated by the configured listeners will be aggregated and printed in Loggie log at fixed intervals (set by period ), which is convenient for backtracking and troubleshooting. listeners is used to configure whether the related listener is enabled. Prometheus metrics are exposed at /metrics on http.port by default. You can curl <podIp>:9196/metrics to view the current metrics.","title":"Monitor Configuration"},{"location":"user-guide/monitor/loggie-monitor/#core-indicators-of-log-collection","text":"Currently there are the following listeners: filesource : The indicator data about current log collection, such as which files are currently being collected, and what is the collection status? filewatcher : Timed full traversal (default 5min) to check all files matching the path, monitor the global collection status, and determine whether there are files that have not been collected in time, etc. reload : Number of reloads queue : Queue status sink : Metrics of sending, such as the number of successes or failures, etc.","title":"Core Indicators of Log Collection"},{"location":"user-guide/monitor/loggie-monitor/#deploy-prometheus-and-grafana","text":"You can use the existing Prometheus or Grafana. If you need a new deployment, please refer to https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack . Deploy with Helm: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -nprometheus --create-namespace Note Due to some reasons, some of these k8s.gcr.io images may not be available for download. You can consider downloading the chart package to replace it and then redeploy it. After confirming that the Pod is running normally, you can access grafana. The way to access grafana through a proxy can be referred to: kubectl -nprometheus port-forward --address 0 .0.0.0 service/prometheus-grafana 8181 :80 Grafana username and password can be viewed in prometheus-grafana sercret. Use base64 -d .","title":"Deploy Prometheus and Grafana"},{"location":"user-guide/monitor/loggie-monitor/#added-loggie-prometheus-monitoring","text":"In the Kubernetes cluster where Loggie is deployed, create the following ServiceMonitor to allow Prometheus to collect Loggie Agent metrics. apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app : loggie release : prometheus name : loggie-agent namespace : prometheus spec : namespaceSelector : matchNames : - loggie endpoints : - port : monitor selector : matchLabels : app : loggie instance : loggie At the same time, we need to add the json in the install project to Grafana to display Loggie's monitoring console. Note The Kubernetes version and the Grafana version may be different, which may lead to incompatible chart display. Modify as you need. The imported Grafana chart is as shown following:","title":"Added Loggie Prometheus Monitoring"},{"location":"user-guide/monitor/service-log-alarm/","text":"Business Log Alarm \u00b6 In addition to the alarm of Loggie itself, the monitoring alarm of the business log itself is also a common function. For example, if ERROR log is included in the log, an alarm can be sent. This kind of alarm will be closer to the business itself. It is a very good supplement. Usage \u00b6 There are two ways to choose: Alarm integrated in collection : Loggie can detect abnormal logs when Agent collects logs, or when Aggregator forward logs, and then sends an alarm. Independent Alarm : Deploy Loggie separately, use Elasticsearch source or other sources to query logs, and then send alarms for detected logs. Alarm integrated in collection \u00b6 Principle \u00b6 Loggie does not need to be independently deployed. However, the matching during collecting will theoretically have a certain impact on the transmission performance, but it is convenient and simple. logAlert interceptor is used to detect abnormal logs during log processing. The abnormal logs will be encapsulated as alarm events and sent to logAlert topic, and consumed by logAlert listener . logAlert listener supports sending to Prometheus AlertManager currently. If you need to support other alarm channels, please submit Issues or PR. Configuration \u00b6 Add logAlert listener : Config loggie : monitor : logger : period : 30s enabled : true listeners : logAlert : alertManagerAddress : [ \"http://127.0.0.1:9093\" ] bufferSize : 100 batchTimeout : 10s batchSize : 10 filesource : ~ filewatcher : ~ reload : ~ queue : ~ sink : ~ http : enabled : true port : 9196 Add logAlert interceptor , and reference it in ClusterLogConfig/LogConfig: Config apiVersion : loggie.io/v1beta1 kind : Interceptor metadata : name : logalert spec : interceptors : | - type: logAlert matcher: contains: [\"err\"] The alertManager's webhook can be configured for other services to receive alerts. Config receivers : - name : webhook webhook_configs : - url : http://127.0.0.1:8787/webhook send_resolved : true When successful, we can view similar logs in the alertManager: ts=2021-12-22T13:33:08.639Z caller=log.go:124 level=debug component=dispatcher msg=\"Received alert\" alert=[6b723d0][active] ts=2021-12-22T13:33:38.640Z caller=log.go:124 level=debug component=dispatcher aggrGroup={}:{} msg=flushing alerts=[[6b723d0][active]] ts=2021-12-22T13:33:38.642Z caller=log.go:124 level=debug component=dispatcher receiver=webhook integration=webhook[0] msg=\"Notify success\" attempts=1 At the same time, the webhook receives a similar alarm: Example { \"receiver\" : \"webhook\" , \"status\" : \"firing\" , \"alerts\" : [ { \"status\" : \"firing\" , \"labels\" : { \"host\" : \"fuyideMacBook-Pro.local\" , \"source\" : \"a\" }, \"annotations\" : { \"message\" : \"10.244.0.1 - - [13/Dec/2021:12:40:48 +0000] error \\\"GET / HTTP/1.1\\\" 404 683\" , \"reason\" : \"contained error\" }, \"startsAt\" : \"2021-12-22T21:33:08.638086+08:00\" , \"endsAt\" : \"0001-01-01T00:00:00Z\" , \"generatorURL\" : \"\" , \"fingerprint\" : \"6b723d0e395b14dc\" } ], \"groupLabels\" : {}, \"commonLabels\" : { \"host\" : \"node1\" , \"source\" : \"a\" }, \"commonAnnotations\" : { \"message\" : \"10.244.0.1 - - [13/Dec/2021:12:40:48 +0000] error \\\"GET / HTTP/1.1\\\" 404 683\" , \"reason\" : \"contained error\" }, \"externalURL\" : \"http://xxxxxx:9093\" , \"version\" : \"4\" , \"groupKey\" : \"{}:{}\" , \"truncatedAlerts\" : 0 } Independent Alarm \u00b6 Info Coming soon, stay tuned...","title":"Log Alert"},{"location":"user-guide/monitor/service-log-alarm/#business-log-alarm","text":"In addition to the alarm of Loggie itself, the monitoring alarm of the business log itself is also a common function. For example, if ERROR log is included in the log, an alarm can be sent. This kind of alarm will be closer to the business itself. It is a very good supplement.","title":"Business Log Alarm"},{"location":"user-guide/monitor/service-log-alarm/#usage","text":"There are two ways to choose: Alarm integrated in collection : Loggie can detect abnormal logs when Agent collects logs, or when Aggregator forward logs, and then sends an alarm. Independent Alarm : Deploy Loggie separately, use Elasticsearch source or other sources to query logs, and then send alarms for detected logs.","title":"Usage"},{"location":"user-guide/monitor/service-log-alarm/#alarm-integrated-in-collection","text":"","title":"Alarm integrated in collection"},{"location":"user-guide/monitor/service-log-alarm/#principle","text":"Loggie does not need to be independently deployed. However, the matching during collecting will theoretically have a certain impact on the transmission performance, but it is convenient and simple. logAlert interceptor is used to detect abnormal logs during log processing. The abnormal logs will be encapsulated as alarm events and sent to logAlert topic, and consumed by logAlert listener . logAlert listener supports sending to Prometheus AlertManager currently. If you need to support other alarm channels, please submit Issues or PR.","title":"Principle"},{"location":"user-guide/monitor/service-log-alarm/#configuration","text":"Add logAlert listener : Config loggie : monitor : logger : period : 30s enabled : true listeners : logAlert : alertManagerAddress : [ \"http://127.0.0.1:9093\" ] bufferSize : 100 batchTimeout : 10s batchSize : 10 filesource : ~ filewatcher : ~ reload : ~ queue : ~ sink : ~ http : enabled : true port : 9196 Add logAlert interceptor , and reference it in ClusterLogConfig/LogConfig: Config apiVersion : loggie.io/v1beta1 kind : Interceptor metadata : name : logalert spec : interceptors : | - type: logAlert matcher: contains: [\"err\"] The alertManager's webhook can be configured for other services to receive alerts. Config receivers : - name : webhook webhook_configs : - url : http://127.0.0.1:8787/webhook send_resolved : true When successful, we can view similar logs in the alertManager: ts=2021-12-22T13:33:08.639Z caller=log.go:124 level=debug component=dispatcher msg=\"Received alert\" alert=[6b723d0][active] ts=2021-12-22T13:33:38.640Z caller=log.go:124 level=debug component=dispatcher aggrGroup={}:{} msg=flushing alerts=[[6b723d0][active]] ts=2021-12-22T13:33:38.642Z caller=log.go:124 level=debug component=dispatcher receiver=webhook integration=webhook[0] msg=\"Notify success\" attempts=1 At the same time, the webhook receives a similar alarm: Example { \"receiver\" : \"webhook\" , \"status\" : \"firing\" , \"alerts\" : [ { \"status\" : \"firing\" , \"labels\" : { \"host\" : \"fuyideMacBook-Pro.local\" , \"source\" : \"a\" }, \"annotations\" : { \"message\" : \"10.244.0.1 - - [13/Dec/2021:12:40:48 +0000] error \\\"GET / HTTP/1.1\\\" 404 683\" , \"reason\" : \"contained error\" }, \"startsAt\" : \"2021-12-22T21:33:08.638086+08:00\" , \"endsAt\" : \"0001-01-01T00:00:00Z\" , \"generatorURL\" : \"\" , \"fingerprint\" : \"6b723d0e395b14dc\" } ], \"groupLabels\" : {}, \"commonLabels\" : { \"host\" : \"node1\" , \"source\" : \"a\" }, \"commonAnnotations\" : { \"message\" : \"10.244.0.1 - - [13/Dec/2021:12:40:48 +0000] error \\\"GET / HTTP/1.1\\\" 404 683\" , \"reason\" : \"contained error\" }, \"externalURL\" : \"http://xxxxxx:9093\" , \"version\" : \"4\" , \"groupKey\" : \"{}:{}\" , \"truncatedAlerts\" : 0 }","title":"Configuration"},{"location":"user-guide/monitor/service-log-alarm/#independent-alarm","text":"Info Coming soon, stay tuned...","title":"Independent Alarm"},{"location":"user-guide/troubleshot/log-collection/","text":"Quick Troubleshooting Guide of Log Collection \u00b6 Why are my logs not being collected? For log collection, the most critical and core question is whether the log has been collected, and why is the log I configured not sent? The troubleshooting ideas and methods are provided below for reference. Also, and most importantly, configure Loggie's Prometheus monitoring and Grafana charts in the environment to quickly find problems. The Core Mechanism of Log Collection \u00b6 Understanding the implementation mechanism is the basis for troubleshooting: Distribute collection tasks: Create a log collection task LogConfig CR in Kubernetes. Receive log configuration: The Agent Loggie of the node listens to the corresponding events of K8s and converts the LogConfig into a Pipelines configuration file. Collect log files: Loggie will automatically reload and then read the configuration file, and then send the corresponding log data to downstream services according to the configuration. (In host scenarios, only the steps to issue the LogConfig CR are not needed, and the rest are similar) Troubleshooting Steps \u00b6 The key to troubleshooting is to first determine which step the problem lies. Troubleshoot Log Collection Tasks \u00b6 Check the events of the log collection task LogConfig/ClusterLogConfig we want to troubleshoot: kubectl -n ${ namespace } describe lgc ${ name } If there are no events, the problmem could be: Pod Label does not match: The label specified in logConfig labelSelector does not match the pod we expect. View with the following command kubectl -n ${ namespace } get po -owide -l ${ labels } For example, use kubectl -n ns1 get po -owide -l app=tomcat,service=web to determine whether there is a matching Pod. If there are no events similar to sync success, you can troubleshoot the problem based on the events combined with the loggie log: Configuration problems or Loggie exceptions: check with the following command kubectl -n ${ loggie -namespace } logs -f ${ loggie -pod-name } \u2014-tail = ${ N } For example, kubectl -nloggie logs -f loggie-5x6vf --tail=100 . Check the loggie log of the corresponding node and handle it according to the log. Common exceptions are: The log path cannot be found: The filled path is not mounted with volume. You can carefully check the path and the path of volumeMount to see if the path is included in volumeMount. The log path does not match a specific log file: the path needs to be filled with a glob expression, eg /var/log/*.log . The fastest way is to execute ls <path> in the Pod of the business that needs to be collected, because ls also uses glob expressions to match log files. In addition, it is also necessary to pay extra attention to whether parameters such as ignoreOlder / excludeFiles are configured, which could cause the log files we want to collect ignored or excluded. Node Log Agent Troubleshooting \u00b6 1. Find the Log Agent of the Node Where the Pod Matched by LogConfig is Located \u00b6 Find a matching business pod according to the labelSelector in logConfig: kubectl -n ${ namespace } get po -owide -l ${ labels } Find any Node node where it is located ${node-name} , then kubectl -n ${ loggie -namespace } get po -owide | grep ${ node -name } Find the Loggie where the node is located. 2. View the Loggie log of the Corresponding Node \u00b6 Check whether there is any abnormality. If there is any abnormality, you need to analyze according to the abnormal log. kubectl -n ${ loggie -namespace } logs -f ${ loggie -pod-name } \u2014-tail = ${ N } 3. Check the Collection \u00b6 Call the automatic troubleshooting API Targeting the corresponding Agent, call the help API: curl <ip>:9196/api/v1/help The beginning of the API return includes a usage hint: --------- Usage: ----------------------- |--- view details: /api/v1/help?detail=<module>, module is one of: all/pipeline/log |--- query by pipeline name: /api/v1/help?pipeline=<name> |--- query by source name: /api/v1/help?source=<name> We can use curl <ip>:9196/api/v1/help?detail=all to query all the details, or use the name of the pipeline or source to search. The current return mainly includes two parts: Pipeline Status: Overall pipeline operation, configuration and some consistency checks. Log Collection Status: Details of file source log collection, including the collection status and progress of log files under each pipeline/source. Under normal circumstances, this API covers the content obtained by the following operations, and there is no need to continue the following operations. The following steps are for reference only. View the Configuration Generated by Loggie Rendering Enter into the container: kubectl -n ${ loggie -namespace } exec -it ${ loggie -pod-name } bash View the Pipeline configuration generated by the rendering: ls /opt/loggie/pipeline/ Of course, you can also choose to view the pipeline configuration generated by calling the Loggie API: curl ${loggie-pod-ip}:9196/api/v1/reload/config Confirm the Log Collection Configuration cat /opt/loggie/pipeline/ Check the log configuration. The path here is the path converted according to the path in the container filled in logconfig, which is the path of the actual node, and can be found on the node normally. Because Loggie uses containerized deployment, you can not view all configurations of nodes in the Loggie Pod. It is necessary to ensure that Loggie also mounts the relevant path prefixes. Check Log Collection Persistence Status Loggie records the collection status of each log file, so that even after Loggie restarts, it can continue based on the previous collection progress and avoid re-collecting log files. You can view it by calling the API: curl ${loggie-pod-ip}:9196/api/v1/source/file/registry?format=text | grep XXX It usually returns something like: { \"id\": 85, \"pipelineName\": \"default/tomcat\", \"sourceName\": \"tomcat-7d64c4f6c9-cm8jm/tomcat/common\", \"filename\": \"/var/lib/kubelet/pods/9397b8be-8927-44ba-8b94-73e5a4459377/volumes/kubernetes.io~empty-dir/log/catalina.2022-06-02.log\", \"jobUid\": \"3670030-65025\", \"offset\": 4960, \"collectTime\": \"2022-06-06 12:44:12.861\", \"version\": \"0.0.1\" }, filename is the log path on the actual node after the Loggie conversion. jobUid is in form of inode-deviceId . offset is the offset after the sink successfully receives the ack. we can execute this in the Loggie container: stat ${filename} View information such as the size/inode of the file, for example: stat File : /var/lib/kubelet/pods/9397b8be-8927-44ba-8b94-73e5a4459377/volumes/kubernetes.io~empty-dir/log/catalina.2022-06-02.log Size: 4960 Blocks: 16 IO Block : 4096 regular file Device: fe01h/65025d Inode: 3670030 Links : 1 Access: (0640/-rw-r-----) Uid: ( 0/ root) Gid : ( 0/ root) Access : 2022-06-06 12:44:12.859236003 +0000 Modify : 2022-06-02 08:54:33.177240007 +0000 Change : 2022-06-02 08:54:33.177240007 +0000 The collecting progress can be judged by comparing size and offset. If size=offset, it means that the file has been collected and sent successfully.","title":"Troubleshooting Quick Guide"},{"location":"user-guide/troubleshot/log-collection/#quick-troubleshooting-guide-of-log-collection","text":"Why are my logs not being collected? For log collection, the most critical and core question is whether the log has been collected, and why is the log I configured not sent? The troubleshooting ideas and methods are provided below for reference. Also, and most importantly, configure Loggie's Prometheus monitoring and Grafana charts in the environment to quickly find problems.","title":"Quick Troubleshooting Guide of Log Collection"},{"location":"user-guide/troubleshot/log-collection/#the-core-mechanism-of-log-collection","text":"Understanding the implementation mechanism is the basis for troubleshooting: Distribute collection tasks: Create a log collection task LogConfig CR in Kubernetes. Receive log configuration: The Agent Loggie of the node listens to the corresponding events of K8s and converts the LogConfig into a Pipelines configuration file. Collect log files: Loggie will automatically reload and then read the configuration file, and then send the corresponding log data to downstream services according to the configuration. (In host scenarios, only the steps to issue the LogConfig CR are not needed, and the rest are similar)","title":"The Core Mechanism of Log Collection"},{"location":"user-guide/troubleshot/log-collection/#troubleshooting-steps","text":"The key to troubleshooting is to first determine which step the problem lies.","title":"Troubleshooting Steps"},{"location":"user-guide/troubleshot/log-collection/#troubleshoot-log-collection-tasks","text":"Check the events of the log collection task LogConfig/ClusterLogConfig we want to troubleshoot: kubectl -n ${ namespace } describe lgc ${ name } If there are no events, the problmem could be: Pod Label does not match: The label specified in logConfig labelSelector does not match the pod we expect. View with the following command kubectl -n ${ namespace } get po -owide -l ${ labels } For example, use kubectl -n ns1 get po -owide -l app=tomcat,service=web to determine whether there is a matching Pod. If there are no events similar to sync success, you can troubleshoot the problem based on the events combined with the loggie log: Configuration problems or Loggie exceptions: check with the following command kubectl -n ${ loggie -namespace } logs -f ${ loggie -pod-name } \u2014-tail = ${ N } For example, kubectl -nloggie logs -f loggie-5x6vf --tail=100 . Check the loggie log of the corresponding node and handle it according to the log. Common exceptions are: The log path cannot be found: The filled path is not mounted with volume. You can carefully check the path and the path of volumeMount to see if the path is included in volumeMount. The log path does not match a specific log file: the path needs to be filled with a glob expression, eg /var/log/*.log . The fastest way is to execute ls <path> in the Pod of the business that needs to be collected, because ls also uses glob expressions to match log files. In addition, it is also necessary to pay extra attention to whether parameters such as ignoreOlder / excludeFiles are configured, which could cause the log files we want to collect ignored or excluded.","title":"Troubleshoot Log Collection Tasks"},{"location":"user-guide/troubleshot/log-collection/#node-log-agent-troubleshooting","text":"","title":"Node Log Agent Troubleshooting"},{"location":"user-guide/troubleshot/log-collection/#1-find-the-log-agent-of-the-node-where-the-pod-matched-by-logconfig-is-located","text":"Find a matching business pod according to the labelSelector in logConfig: kubectl -n ${ namespace } get po -owide -l ${ labels } Find any Node node where it is located ${node-name} , then kubectl -n ${ loggie -namespace } get po -owide | grep ${ node -name } Find the Loggie where the node is located.","title":"1.  Find the Log Agent of the Node Where the Pod Matched by LogConfig is Located"},{"location":"user-guide/troubleshot/log-collection/#2-view-the-loggie-log-of-the-corresponding-node","text":"Check whether there is any abnormality. If there is any abnormality, you need to analyze according to the abnormal log. kubectl -n ${ loggie -namespace } logs -f ${ loggie -pod-name } \u2014-tail = ${ N }","title":"2.  View the Loggie log of the Corresponding Node"},{"location":"user-guide/troubleshot/log-collection/#3-check-the-collection","text":"Call the automatic troubleshooting API Targeting the corresponding Agent, call the help API: curl <ip>:9196/api/v1/help The beginning of the API return includes a usage hint: --------- Usage: ----------------------- |--- view details: /api/v1/help?detail=<module>, module is one of: all/pipeline/log |--- query by pipeline name: /api/v1/help?pipeline=<name> |--- query by source name: /api/v1/help?source=<name> We can use curl <ip>:9196/api/v1/help?detail=all to query all the details, or use the name of the pipeline or source to search. The current return mainly includes two parts: Pipeline Status: Overall pipeline operation, configuration and some consistency checks. Log Collection Status: Details of file source log collection, including the collection status and progress of log files under each pipeline/source. Under normal circumstances, this API covers the content obtained by the following operations, and there is no need to continue the following operations. The following steps are for reference only. View the Configuration Generated by Loggie Rendering Enter into the container: kubectl -n ${ loggie -namespace } exec -it ${ loggie -pod-name } bash View the Pipeline configuration generated by the rendering: ls /opt/loggie/pipeline/ Of course, you can also choose to view the pipeline configuration generated by calling the Loggie API: curl ${loggie-pod-ip}:9196/api/v1/reload/config Confirm the Log Collection Configuration cat /opt/loggie/pipeline/ Check the log configuration. The path here is the path converted according to the path in the container filled in logconfig, which is the path of the actual node, and can be found on the node normally. Because Loggie uses containerized deployment, you can not view all configurations of nodes in the Loggie Pod. It is necessary to ensure that Loggie also mounts the relevant path prefixes. Check Log Collection Persistence Status Loggie records the collection status of each log file, so that even after Loggie restarts, it can continue based on the previous collection progress and avoid re-collecting log files. You can view it by calling the API: curl ${loggie-pod-ip}:9196/api/v1/source/file/registry?format=text | grep XXX It usually returns something like: { \"id\": 85, \"pipelineName\": \"default/tomcat\", \"sourceName\": \"tomcat-7d64c4f6c9-cm8jm/tomcat/common\", \"filename\": \"/var/lib/kubelet/pods/9397b8be-8927-44ba-8b94-73e5a4459377/volumes/kubernetes.io~empty-dir/log/catalina.2022-06-02.log\", \"jobUid\": \"3670030-65025\", \"offset\": 4960, \"collectTime\": \"2022-06-06 12:44:12.861\", \"version\": \"0.0.1\" }, filename is the log path on the actual node after the Loggie conversion. jobUid is in form of inode-deviceId . offset is the offset after the sink successfully receives the ack. we can execute this in the Loggie container: stat ${filename} View information such as the size/inode of the file, for example: stat File : /var/lib/kubelet/pods/9397b8be-8927-44ba-8b94-73e5a4459377/volumes/kubernetes.io~empty-dir/log/catalina.2022-06-02.log Size: 4960 Blocks: 16 IO Block : 4096 regular file Device: fe01h/65025d Inode: 3670030 Links : 1 Access: (0640/-rw-r-----) Uid: ( 0/ root) Gid : ( 0/ root) Access : 2022-06-06 12:44:12.859236003 +0000 Modify : 2022-06-02 08:54:33.177240007 +0000 Change : 2022-06-02 08:54:33.177240007 +0000 The collecting progress can be judged by comparing size and offset. If size=offset, it means that the file has been collected and sent successfully.","title":"3.  Check the Collection"},{"location":"user-guide/troubleshot/problems/","text":"Cases \u00b6 net.cgoLookupIP Causes Segmentation Violation \u00b6 Phenomenon When using unix sock source, Loggie crashes after startup, and there are the following logs: fatal error: unexpected signal during runtime execution [signal SIGSEGV: segmentation violation code=0x1 addr=0x47 pc=0x7f59b4528360] runtime stack: runtime.throw({0x213b1a1, 0x7f59b423b640}) /usr/local/go/src/runtime/panic.go:1198 +0x71 runtime.sigpanic() /usr/local/go/src/runtime/signal_unix.go:719 +0x396 goroutine 86 [syscall]: runtime.cgocall(0x1a39d30, 0xc000510d90) /usr/local/go/src/runtime/cgocall.go:156 +0x5c fp=0xc000510d68 sp=0xc000510d30 pc=0x40565c net._C2func_getaddrinfo(0xc00030bc40, 0x0, 0xc000724fc0, 0xc0005bd990) _cgo_gotypes.go:91 +0x56 fp=0xc000510d90 sp=0xc000510d68 pc=0x5c7bb6 net.cgoLookupIPCNAME.func1({0xc00030bc40, 0xc0004d29c0, 0x4}, 0xc00030bb80, 0xc000510e50) /usr/local/go/src/net/cgo_unix.go:163 +0x9f fp=0xc000510de8 sp=0xc000510d90 pc=0x5c98ff net.cgoLookupIPCNAME({0x20f751c, 0x3}, {0xc00030bb80, 0xc00022b5e0}) /usr/local/go/src/net/cgo_unix.go:163 +0x16d fp=0xc000510f38 sp=0xc000510de8 pc=0x5c914d net.cgoIPLookup(0x358aed0, {0x20f751c, 0xc00030bbc0}, {0xc00030bb80, 0xc000510fb8}) /usr/local/go/src/net/cgo_unix.go:220 +0x3b fp=0xc000510fa8 sp=0xc000510f38 pc=0x5c99bb net.cgoLookupIP\u00b7dwrap\u00b725() /usr/local/go/src/net/cgo_unix.go:230 +0x36 fp=0xc000510fe0 sp=0xc000510fa8 pc=0x5c9e36 runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:1581 +0x1 fp=0xc000510fe8 sp=0xc000510fe0 pc=0x46ae81 created by net.cgoLookupIP /usr/local/go/src/net/cgo_unix.go:230 +0x125 Reason For specific reasons, please refer to: go net \u3002 Name Resolution The method for resolving domain names, whether indirectly with functions like Dial or directly with functions like LookupHost and LookupAddr, varies by operating system. On Unix systems, the resolver has two options for resolving names. It can use a pure Go resolver that sends DNS requests directly to the servers listed in /etc/resolv.conf, or it can use a cgo-based resolver that calls C library routines such as getaddrinfo and getnameinfo. By default the pure Go resolver is used, because a blocked DNS request consumes only a goroutine, while a blocked C call consumes an operating system thread. When cgo is available, the cgo-based resolver is used instead under a variety of conditions: on systems that do not let programs make direct DNS requests (OS X), when the LOCALDOMAIN environment variable is present (even if empty), when the RES_OPTIONS or HOSTALIASES environment variable is non-empty, when the ASR_CONFIG environment variable is non-empty (OpenBSD only), when /etc/resolv.conf or /etc/nsswitch.conf specify the use of features that the Go resolver does not implement, and when the name being looked up ends in .local or is an mDNS name. The resolver decision can be overridden by setting the netdns value of the GODEBUG environment variable (see package runtime) to go or cgo, as in: export GODEBUG=netdns=go # force pure Go resolver export GODEBUG=netdns=cgo # force cgo resolver The decision can also be forced while building the Go source tree by setting the netgo or netcgo build tag. Solution Add environment variables to the Loggie deployment script: env : - name : GODEBUG value : netdns=go","title":"Cases"},{"location":"user-guide/troubleshot/problems/#cases","text":"","title":"Cases"},{"location":"user-guide/troubleshot/problems/#netcgolookupip-causes-segmentation-violation","text":"Phenomenon When using unix sock source, Loggie crashes after startup, and there are the following logs: fatal error: unexpected signal during runtime execution [signal SIGSEGV: segmentation violation code=0x1 addr=0x47 pc=0x7f59b4528360] runtime stack: runtime.throw({0x213b1a1, 0x7f59b423b640}) /usr/local/go/src/runtime/panic.go:1198 +0x71 runtime.sigpanic() /usr/local/go/src/runtime/signal_unix.go:719 +0x396 goroutine 86 [syscall]: runtime.cgocall(0x1a39d30, 0xc000510d90) /usr/local/go/src/runtime/cgocall.go:156 +0x5c fp=0xc000510d68 sp=0xc000510d30 pc=0x40565c net._C2func_getaddrinfo(0xc00030bc40, 0x0, 0xc000724fc0, 0xc0005bd990) _cgo_gotypes.go:91 +0x56 fp=0xc000510d90 sp=0xc000510d68 pc=0x5c7bb6 net.cgoLookupIPCNAME.func1({0xc00030bc40, 0xc0004d29c0, 0x4}, 0xc00030bb80, 0xc000510e50) /usr/local/go/src/net/cgo_unix.go:163 +0x9f fp=0xc000510de8 sp=0xc000510d90 pc=0x5c98ff net.cgoLookupIPCNAME({0x20f751c, 0x3}, {0xc00030bb80, 0xc00022b5e0}) /usr/local/go/src/net/cgo_unix.go:163 +0x16d fp=0xc000510f38 sp=0xc000510de8 pc=0x5c914d net.cgoIPLookup(0x358aed0, {0x20f751c, 0xc00030bbc0}, {0xc00030bb80, 0xc000510fb8}) /usr/local/go/src/net/cgo_unix.go:220 +0x3b fp=0xc000510fa8 sp=0xc000510f38 pc=0x5c99bb net.cgoLookupIP\u00b7dwrap\u00b725() /usr/local/go/src/net/cgo_unix.go:230 +0x36 fp=0xc000510fe0 sp=0xc000510fa8 pc=0x5c9e36 runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:1581 +0x1 fp=0xc000510fe8 sp=0xc000510fe0 pc=0x46ae81 created by net.cgoLookupIP /usr/local/go/src/net/cgo_unix.go:230 +0x125 Reason For specific reasons, please refer to: go net \u3002 Name Resolution The method for resolving domain names, whether indirectly with functions like Dial or directly with functions like LookupHost and LookupAddr, varies by operating system. On Unix systems, the resolver has two options for resolving names. It can use a pure Go resolver that sends DNS requests directly to the servers listed in /etc/resolv.conf, or it can use a cgo-based resolver that calls C library routines such as getaddrinfo and getnameinfo. By default the pure Go resolver is used, because a blocked DNS request consumes only a goroutine, while a blocked C call consumes an operating system thread. When cgo is available, the cgo-based resolver is used instead under a variety of conditions: on systems that do not let programs make direct DNS requests (OS X), when the LOCALDOMAIN environment variable is present (even if empty), when the RES_OPTIONS or HOSTALIASES environment variable is non-empty, when the ASR_CONFIG environment variable is non-empty (OpenBSD only), when /etc/resolv.conf or /etc/nsswitch.conf specify the use of features that the Go resolver does not implement, and when the name being looked up ends in .local or is an mDNS name. The resolver decision can be overridden by setting the netdns value of the GODEBUG environment variable (see package runtime) to go or cgo, as in: export GODEBUG=netdns=go # force pure Go resolver export GODEBUG=netdns=cgo # force cgo resolver The decision can also be forced while building the Go source tree by setting the netgo or netcgo build tag. Solution Add environment variables to the Loggie deployment script: env : - name : GODEBUG value : netdns=go","title":"net.cgoLookupIP Causes Segmentation Violation"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/","text":"Use Loggie to Collect Container Logs \u00b6 Before reading this article, it is recommended to refer to Log collection in Kubernetes . How does Loggie Collect Container Logs? \u00b6 Due to the good extensibility of Kubernetes, users can define their own CRDs to express their desired states, and develop Controllers with the help of some frameworks. Based on this idea, what kind of logs a service needs to collect and what kind of log configuration is required are the expectations of users. This requires us to develop a log collection Controller to achieve. Therefore, the user only needs to describe the logs of which Pods need to be collected and log path in pod in the CRD LogConfig. The core architecture is shown in the following figure: Loggie will perceive Pod and CRD events and dynamically update the configuration. At the same time, Loggie can find the corresponding file on the node for collection according to the Volume mounted on the log file path. In addition, according to the configuration, the Env/Annotation/Label on the Pod can be automatically added to the log as meta information. At the same time, compared with the way of mounting all nodes to the same path for collection, it also solves the problem of not being able to fine-tune the configuration for a single service and collecting irrelevant logs. Benefits are not limited to these. Loggie can perform corresponding adaptation and support based on Kubernetes in terms of dynamic configuration delivery and monitoring indicators. CRD Instructions \u00b6 Loggie currently has the following CRDs: LogConfig : namespace-level CRD, used to collect Pod container logs, which mainly includes the source configuration of the collection, as well as the associated sink and interceptor. ClusterLogConfig : cluster-level CRD, which indicates the cluster-level collection of Pod container logs, the collection of logs on nodes, and the distribution of general pipeline configuration for a Loggie cluster. Sink : a sink backend that needs to be associated in ClusterLogConfig/LogConfig. Interceptor : an interceptors group that needs to be associated in ClusterLogConfig/LogConfig. The process using CRD is as follows: Preparation \u00b6 Architecture \u00b6 What is the overall architecture? The Loggie Agent sends data directly to the backend such as Elasticsearch or other storage, etc. The Loggie Agent sends data to the Loggie Aggregator for transit processing, and then to other back-end storage The Loggie Agent sends data to Kafka, and oggie Aggregator consumes Kafka and sends data to the backend ... This article only focuses on the acquisition side. If you need to deploy the Loggie Aggregator, please refer to Loggie Aggregator . Before collecting container logs, make sure that the Loggie DaemonSet has been deployed in Kubernetes. Deploy Loggie in Kubernetes Info We recommend using the DaemonSet method to collect container logs, Loggie plan to supports the automatic injection of Loggie Sidecar to collect logs in the future. RoadMap Usage \u00b6 How do business pods mount logs? It is recommended that if you are not sensitive to the possibility of log loss, such as operation and maintenance logs, you can use emptyDir. If the logs are important and cannot be lost, use hostPath and configure subPathExpr on volumeMount to realize path isolation. Example tomcat-emptydir.yml apiVersion : apps/v1 kind : Deployment metadata : labels : app : tomcat name : tomcat namespace : default spec : replicas : 1 selector : matchLabels : app : tomcat template : metadata : labels : app : tomcat spec : containers : - name : tomcat image : tomcat volumeMounts : - mountPath : /usr/local/tomcat/logs name : log volumes : - emptyDir : {} name : log tomcat-hostpath.yml apiVersion : apps/v1 kind : Deployment metadata : labels : app : tomcat name : tomcat namespace : default spec : replicas : 1 selector : matchLabels : app : tomcat template : metadata : labels : app : tomcat spec : containers : - env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name - name : NAMESPACE valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.namespace image : tomcat name : tomcat volumeMounts : - mountPath : /log name : datalog subPathExpr : $(NAMESPACE)/$(POD_NAME) volumes : - hostPath : path : /data/log type : \"\" name : datalog Caution Make sure that the log path mounted by the Pod is not shared by multiple Pods. For example, two Pods under a Deployment use the hostPath without subPathExpr to mount the log directory. If the two Pods are scheduled on the same node, they will be printed to the same log directory and file, which may cause collection exceptions. We first create the tomcat Deployment of the above example for log collection. kubectl apply -f tomcat-emptydir.yml Next, we will collect the tomcat logs and send them to Elasticsearch. If you want to collect other container logs or send them to other backends, you only need to modify the configuration. Deploy Elasticsearch and Kibana (optional) \u00b6 Since this demo sends data to Elasticsearch, here we deploy Elasticsearch and Kibana. If you already have Elasticsearch and Kibana in your environment, this step can be ignored. If you want to build a set of Elasticsearch and Kibana by yourself, here is the suggestion: Deploy Elasticsearch With Helm Deploy Kibana With Helm If there is no helm installed, you need to download helm \u3002 Use the following command: helm repo add elastic https://helm.elastic.co helm install elasticsearch elastic/elasticsearch --set replicas = 1 helm install kibana elastic/kibana And: kubectl port-forward service/kibana-kibana 5601 :http You can enter localhost:5601 in the browser to access the Kibana page. Collect Container Logs \u00b6 Create Sink \u00b6 In order to indicate the Elasticsearch to which we are about to send logs, we need to configure the corresponding sink. There are two ways here: If the entire cluster has only one storage backend, we can configure the defaults parameter in the global configuration file configMap. For details, please reference . Use Sink CRD and refer to it in logConfig. This method can be extended to multiple backends. Different logConfigs can be configured to use different backend storage. In most cases, we recommend this method. Create a sink as follows. Sink is a cluster-level CRD. It can be modified to other configurations in spec.sink. Example cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : default spec : sink : | type: elasticsearch index: \"loggie\" hosts: [\"elasticsearch-master.default.svc:9200\"] EOF kubectl get sink default to check if the creation is successful. Create Interceptor (Optional) \u00b6 Like Sink, there are two ways to configure interceptor: In the global configuration file configMap, configure the defaults parameter. This means that all Loggie Agents use the default interceptors configuration. Use the Interceptor CRD and be refered to in logConfig. This method will be more flexible and suitable for scenarios with certain requirements. Loggie currently has three built-in interceptors metric (sending monitoring metrics)\u3001 maxbytes (maximum event bytes limit)\u3001 retry . Even if we do not use the above two ways to configure, the three default interceptors will still be used automatically. Caution In the global defaults, configuring interceptors will overwrite the existing built-in interceptors. So if you use global defaults to configure interceptors, please add all the built-in interceptors, unless you are sure you don't need them. The logConfig configured using the interceptor CRD will not override the default and built-in interceptors, and will be added to the entire pipeline interceptors chain. An example of creating an interceptor is as follows: Example cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : Interceptor metadata : name : default spec : interceptors : | - type: rateLimit qps: 90000 EOF Here we create an rateLimit interceptor, which can be used to limit the current of sending logs. Use kubectl get interceptor default or kubectl get icp default to check whether the creation is successful. Create logConfig \u00b6 After creating the sink and interceptor, the most important thing is to create logConfig, indicating logs of which Pod we want to collect, and which logs of Pod to collect. The spec.selector part of logConfig indicates the distribution scope of the log configuration. For collecting Pod logs, you need to configure type: pod and labelSelector to select the specified Pods. That is, collect the logs of these Pods. An example is as follows: spec : selector : type : pod labelSelector : app : tomcat In addition, we need to configure the file source in pipeline.sources, that is, which logs of Pods to collect. pipeline : sources : | - type: file name: mylog paths: - stdout - /var/log/*.log Note: Path stdout , indicating the standard output path of the collection container. The log file filled in path is the path in the container. (When the configuration is finally generated, Loggie will automatically update the path in pod to the path on the node, without the user's concern) Please use glob expression to fill in path. When type: pod , pipeline.name will be automatically generated by loggie as ${namespace}-${logConfigName} . The final generated source.name will be ${podName}-${containerName}-${sourceName} . Finally, we use sinkRef and interceptorRef to reference the sink and interceptor created above. A logConfig example as follows: Example cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : tomcat namespace : default spec : selector : type : pod labelSelector : app : tomcat pipeline : sources : | - type: file name: common paths: - stdout - /usr/local/tomcat/logs/*.log sinkRef : default interceptorRef : default EOF Use kubectl get logconfig tomcat or kubectl get lgc tomcat to check if the creation is successful. At the same time, you can use kubectl describe lgc tomcat to check the status by viewing the events of logConfig. If there are events similar to the following, the configuration has been delivered successfully. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal syncSuccess 55s loggie/kind-control-plane Sync type pod [tomcat-684c698b66-hvztn] success We can also use kubectl -nloggie logs -f ${loggie-name} to check the log collection by viewing the logs of Loggie on specified node. After the sending is successful, we can query the collected logs on Kibana. Automatically Parse Container Stdout Raw Logs \u00b6 Under normal circumstances, the standard output we collect is not the printed log content, but a entity encapsulated by the container runtime. For example, the standard output of docker is in json form: { \"log\" : \"I0610 08:29:07.698664 Waiting for caches to sync\\n\" , \"stream\" : \"stderr\" , \"time:\" 2021-06-10 T 08 : 29 : 07.698731204 Z\"} The original log content printed by the business is stored in the log field. The standard output of containerd looks like this: 2021-12-01T03:13:58.298476921Z stderr F INFO [main] Starting service [Catalina] 2021-12-01T03:13:58.298476921Z stderr F is the prefix content added at runtime, and remaining is the original log. Especially if we configure multi-line log conllection, because the collected log content is inconsistent with the original output log, it will cause problems with matching multiple lines of the collected standard output log. Therefore, Loggie provides a one-key switch configuration. In the system configuration, set the parseStdout to true. parseStdout config : loggie : discovery : enabled : true kubernetes : parseStdout : true Loggie will automatically add source codec to parse the original business log when rendering LogConfig. Note: It is only valid when the path is only stdout : [valid] sources : | - type: file name: common paths: - stdout [invalid] sources : | - type: file name: common paths: - stdout - /usr/local/tomcat/logs/*.log The above needs to be changed to two sources: [valid] sources : | - type: file name: stdout paths: - stdout - type: file name: tomcat paths: - /usr/local/tomcat/logs/*.log Currently only the original log content will be retained into body , and the rest will be discarded. Automatic parsing of stdout is actually achieved by automatically adding source codec when LogConfig is rendered into Pipeline configuration. Container Log Collection Without Mounting Volumes \u00b6 Although we recommend using mounted volume (emptyDir/hostPath+subPathExpr) to hang out log files for collection by Loggie, there are still many cases where we cannot uniformly mount log path of Pods of the business. For example, some basic components cannot configure independent log volumes, or the business is simply unwilling to change the deployment configuration. Loggie provides the ability to collect container logs without mounting, and can automatically identify and collect log files in the container root filesystem. You only need to set the configuration in values.yml in helm chart rootFsCollectionEnabled to true, and fill in the container runtime (docker/containerd) of the actual environment, as follows: rootFsCollectionEnabled config : loggie : discovery : enabled : true kubernetes : containerRuntime : containerd rootFsCollectionEnabled : false After the modification is complete, helm upgrade again. The helm template will automatically add some additional mount paths and configurations. If you upgrade from a lower version, you need to modify the deployed Daemonset yaml. Please refer to issues #208 for specific principles.","title":"Collect Container Logs"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#use-loggie-to-collect-container-logs","text":"Before reading this article, it is recommended to refer to Log collection in Kubernetes .","title":"Use Loggie to Collect Container Logs"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#how-does-loggie-collect-container-logs","text":"Due to the good extensibility of Kubernetes, users can define their own CRDs to express their desired states, and develop Controllers with the help of some frameworks. Based on this idea, what kind of logs a service needs to collect and what kind of log configuration is required are the expectations of users. This requires us to develop a log collection Controller to achieve. Therefore, the user only needs to describe the logs of which Pods need to be collected and log path in pod in the CRD LogConfig. The core architecture is shown in the following figure: Loggie will perceive Pod and CRD events and dynamically update the configuration. At the same time, Loggie can find the corresponding file on the node for collection according to the Volume mounted on the log file path. In addition, according to the configuration, the Env/Annotation/Label on the Pod can be automatically added to the log as meta information. At the same time, compared with the way of mounting all nodes to the same path for collection, it also solves the problem of not being able to fine-tune the configuration for a single service and collecting irrelevant logs. Benefits are not limited to these. Loggie can perform corresponding adaptation and support based on Kubernetes in terms of dynamic configuration delivery and monitoring indicators.","title":"How does Loggie Collect Container Logs?"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#crd-instructions","text":"Loggie currently has the following CRDs: LogConfig : namespace-level CRD, used to collect Pod container logs, which mainly includes the source configuration of the collection, as well as the associated sink and interceptor. ClusterLogConfig : cluster-level CRD, which indicates the cluster-level collection of Pod container logs, the collection of logs on nodes, and the distribution of general pipeline configuration for a Loggie cluster. Sink : a sink backend that needs to be associated in ClusterLogConfig/LogConfig. Interceptor : an interceptors group that needs to be associated in ClusterLogConfig/LogConfig. The process using CRD is as follows:","title":"CRD Instructions"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#preparation","text":"","title":"Preparation"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#architecture","text":"What is the overall architecture? The Loggie Agent sends data directly to the backend such as Elasticsearch or other storage, etc. The Loggie Agent sends data to the Loggie Aggregator for transit processing, and then to other back-end storage The Loggie Agent sends data to Kafka, and oggie Aggregator consumes Kafka and sends data to the backend ... This article only focuses on the acquisition side. If you need to deploy the Loggie Aggregator, please refer to Loggie Aggregator . Before collecting container logs, make sure that the Loggie DaemonSet has been deployed in Kubernetes. Deploy Loggie in Kubernetes Info We recommend using the DaemonSet method to collect container logs, Loggie plan to supports the automatic injection of Loggie Sidecar to collect logs in the future. RoadMap","title":"Architecture"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#usage","text":"How do business pods mount logs? It is recommended that if you are not sensitive to the possibility of log loss, such as operation and maintenance logs, you can use emptyDir. If the logs are important and cannot be lost, use hostPath and configure subPathExpr on volumeMount to realize path isolation. Example tomcat-emptydir.yml apiVersion : apps/v1 kind : Deployment metadata : labels : app : tomcat name : tomcat namespace : default spec : replicas : 1 selector : matchLabels : app : tomcat template : metadata : labels : app : tomcat spec : containers : - name : tomcat image : tomcat volumeMounts : - mountPath : /usr/local/tomcat/logs name : log volumes : - emptyDir : {} name : log tomcat-hostpath.yml apiVersion : apps/v1 kind : Deployment metadata : labels : app : tomcat name : tomcat namespace : default spec : replicas : 1 selector : matchLabels : app : tomcat template : metadata : labels : app : tomcat spec : containers : - env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name - name : NAMESPACE valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.namespace image : tomcat name : tomcat volumeMounts : - mountPath : /log name : datalog subPathExpr : $(NAMESPACE)/$(POD_NAME) volumes : - hostPath : path : /data/log type : \"\" name : datalog Caution Make sure that the log path mounted by the Pod is not shared by multiple Pods. For example, two Pods under a Deployment use the hostPath without subPathExpr to mount the log directory. If the two Pods are scheduled on the same node, they will be printed to the same log directory and file, which may cause collection exceptions. We first create the tomcat Deployment of the above example for log collection. kubectl apply -f tomcat-emptydir.yml Next, we will collect the tomcat logs and send them to Elasticsearch. If you want to collect other container logs or send them to other backends, you only need to modify the configuration.","title":"Usage"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#deploy-elasticsearch-and-kibana-optional","text":"Since this demo sends data to Elasticsearch, here we deploy Elasticsearch and Kibana. If you already have Elasticsearch and Kibana in your environment, this step can be ignored. If you want to build a set of Elasticsearch and Kibana by yourself, here is the suggestion: Deploy Elasticsearch With Helm Deploy Kibana With Helm If there is no helm installed, you need to download helm \u3002 Use the following command: helm repo add elastic https://helm.elastic.co helm install elasticsearch elastic/elasticsearch --set replicas = 1 helm install kibana elastic/kibana And: kubectl port-forward service/kibana-kibana 5601 :http You can enter localhost:5601 in the browser to access the Kibana page.","title":"Deploy Elasticsearch and Kibana (optional)"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#collect-container-logs","text":"","title":"Collect Container Logs"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#create-sink","text":"In order to indicate the Elasticsearch to which we are about to send logs, we need to configure the corresponding sink. There are two ways here: If the entire cluster has only one storage backend, we can configure the defaults parameter in the global configuration file configMap. For details, please reference . Use Sink CRD and refer to it in logConfig. This method can be extended to multiple backends. Different logConfigs can be configured to use different backend storage. In most cases, we recommend this method. Create a sink as follows. Sink is a cluster-level CRD. It can be modified to other configurations in spec.sink. Example cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : Sink metadata : name : default spec : sink : | type: elasticsearch index: \"loggie\" hosts: [\"elasticsearch-master.default.svc:9200\"] EOF kubectl get sink default to check if the creation is successful.","title":"Create Sink"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#create-interceptor-optional","text":"Like Sink, there are two ways to configure interceptor: In the global configuration file configMap, configure the defaults parameter. This means that all Loggie Agents use the default interceptors configuration. Use the Interceptor CRD and be refered to in logConfig. This method will be more flexible and suitable for scenarios with certain requirements. Loggie currently has three built-in interceptors metric (sending monitoring metrics)\u3001 maxbytes (maximum event bytes limit)\u3001 retry . Even if we do not use the above two ways to configure, the three default interceptors will still be used automatically. Caution In the global defaults, configuring interceptors will overwrite the existing built-in interceptors. So if you use global defaults to configure interceptors, please add all the built-in interceptors, unless you are sure you don't need them. The logConfig configured using the interceptor CRD will not override the default and built-in interceptors, and will be added to the entire pipeline interceptors chain. An example of creating an interceptor is as follows: Example cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : Interceptor metadata : name : default spec : interceptors : | - type: rateLimit qps: 90000 EOF Here we create an rateLimit interceptor, which can be used to limit the current of sending logs. Use kubectl get interceptor default or kubectl get icp default to check whether the creation is successful.","title":"Create Interceptor (Optional)"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#create-logconfig","text":"After creating the sink and interceptor, the most important thing is to create logConfig, indicating logs of which Pod we want to collect, and which logs of Pod to collect. The spec.selector part of logConfig indicates the distribution scope of the log configuration. For collecting Pod logs, you need to configure type: pod and labelSelector to select the specified Pods. That is, collect the logs of these Pods. An example is as follows: spec : selector : type : pod labelSelector : app : tomcat In addition, we need to configure the file source in pipeline.sources, that is, which logs of Pods to collect. pipeline : sources : | - type: file name: mylog paths: - stdout - /var/log/*.log Note: Path stdout , indicating the standard output path of the collection container. The log file filled in path is the path in the container. (When the configuration is finally generated, Loggie will automatically update the path in pod to the path on the node, without the user's concern) Please use glob expression to fill in path. When type: pod , pipeline.name will be automatically generated by loggie as ${namespace}-${logConfigName} . The final generated source.name will be ${podName}-${containerName}-${sourceName} . Finally, we use sinkRef and interceptorRef to reference the sink and interceptor created above. A logConfig example as follows: Example cat << EOF | kubectl apply -f - apiVersion : loggie.io/v1beta1 kind : LogConfig metadata : name : tomcat namespace : default spec : selector : type : pod labelSelector : app : tomcat pipeline : sources : | - type: file name: common paths: - stdout - /usr/local/tomcat/logs/*.log sinkRef : default interceptorRef : default EOF Use kubectl get logconfig tomcat or kubectl get lgc tomcat to check if the creation is successful. At the same time, you can use kubectl describe lgc tomcat to check the status by viewing the events of logConfig. If there are events similar to the following, the configuration has been delivered successfully. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal syncSuccess 55s loggie/kind-control-plane Sync type pod [tomcat-684c698b66-hvztn] success We can also use kubectl -nloggie logs -f ${loggie-name} to check the log collection by viewing the logs of Loggie on specified node. After the sending is successful, we can query the collected logs on Kibana.","title":"Create logConfig"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#automatically-parse-container-stdout-raw-logs","text":"Under normal circumstances, the standard output we collect is not the printed log content, but a entity encapsulated by the container runtime. For example, the standard output of docker is in json form: { \"log\" : \"I0610 08:29:07.698664 Waiting for caches to sync\\n\" , \"stream\" : \"stderr\" , \"time:\" 2021-06-10 T 08 : 29 : 07.698731204 Z\"} The original log content printed by the business is stored in the log field. The standard output of containerd looks like this: 2021-12-01T03:13:58.298476921Z stderr F INFO [main] Starting service [Catalina] 2021-12-01T03:13:58.298476921Z stderr F is the prefix content added at runtime, and remaining is the original log. Especially if we configure multi-line log conllection, because the collected log content is inconsistent with the original output log, it will cause problems with matching multiple lines of the collected standard output log. Therefore, Loggie provides a one-key switch configuration. In the system configuration, set the parseStdout to true. parseStdout config : loggie : discovery : enabled : true kubernetes : parseStdout : true Loggie will automatically add source codec to parse the original business log when rendering LogConfig. Note: It is only valid when the path is only stdout : [valid] sources : | - type: file name: common paths: - stdout [invalid] sources : | - type: file name: common paths: - stdout - /usr/local/tomcat/logs/*.log The above needs to be changed to two sources: [valid] sources : | - type: file name: stdout paths: - stdout - type: file name: tomcat paths: - /usr/local/tomcat/logs/*.log Currently only the original log content will be retained into body , and the rest will be discarded. Automatic parsing of stdout is actually achieved by automatically adding source codec when LogConfig is rendered into Pipeline configuration.","title":"Automatically Parse Container Stdout Raw Logs"},{"location":"user-guide/use-in-kubernetes/collect-container-logs/#container-log-collection-without-mounting-volumes","text":"Although we recommend using mounted volume (emptyDir/hostPath+subPathExpr) to hang out log files for collection by Loggie, there are still many cases where we cannot uniformly mount log path of Pods of the business. For example, some basic components cannot configure independent log volumes, or the business is simply unwilling to change the deployment configuration. Loggie provides the ability to collect container logs without mounting, and can automatically identify and collect log files in the container root filesystem. You only need to set the configuration in values.yml in helm chart rootFsCollectionEnabled to true, and fill in the container runtime (docker/containerd) of the actual environment, as follows: rootFsCollectionEnabled config : loggie : discovery : enabled : true kubernetes : containerRuntime : containerd rootFsCollectionEnabled : false After the modification is complete, helm upgrade again. The helm template will automatically add some additional mount paths and configurations. If you upgrade from a lower version, you need to modify the deployed Daemonset yaml. Please refer to issues #208 for specific principles.","title":"Container Log Collection Without Mounting Volumes"},{"location":"user-guide/use-in-kubernetes/collect-node-logs/","text":"Use Loggie to Collect Node Logs \u00b6 In a Kubernetes cluster, in addition to collecting logs in Pods, there may also be some requirements for collecting kubelet logs and system logs on Node nodes. Configuration \u00b6 Different from the collecting container logs, node log collection needs to use the cluster-level ClusterLogConfig. Selector should be type: node and nodeSelector is used to select which nodes to deliver the configuration to. At the same time, ensure that these labels are included on the Node. An example: Example apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : varlog spec : selector : type : node nodeSelector : nodepool : demo pipeline : sources : | - type: file name: varlog paths: - /var/log/*.log sinkRef : default interceptorRef : default In addition, it should be noted that if you need to collect logs of a certain path on the Node, you need to mount the same path in Loggie, otherwise, due to container isolation, Loggie cannot obtain the logs of the node. For example, to collect /var/log/logs on the Node, you need to add the Loggie Agent to mount the path.","title":"Collect Node Logs"},{"location":"user-guide/use-in-kubernetes/collect-node-logs/#use-loggie-to-collect-node-logs","text":"In a Kubernetes cluster, in addition to collecting logs in Pods, there may also be some requirements for collecting kubelet logs and system logs on Node nodes.","title":"Use Loggie to Collect Node Logs"},{"location":"user-guide/use-in-kubernetes/collect-node-logs/#configuration","text":"Different from the collecting container logs, node log collection needs to use the cluster-level ClusterLogConfig. Selector should be type: node and nodeSelector is used to select which nodes to deliver the configuration to. At the same time, ensure that these labels are included on the Node. An example: Example apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : varlog spec : selector : type : node nodeSelector : nodepool : demo pipeline : sources : | - type: file name: varlog paths: - /var/log/*.log sinkRef : default interceptorRef : default In addition, it should be noted that if you need to collect logs of a certain path on the Node, you need to mount the same path in Loggie, otherwise, due to container isolation, Loggie cannot obtain the logs of the node. For example, to collect /var/log/logs on the Node, you need to add the Loggie Agent to mount the path.","title":"Configuration"},{"location":"user-guide/use-in-kubernetes/general-usage/","text":"Log collection in Kubernetes \u00b6 Cite Compared with the traditional host log collection, in the Kubernetes cluster, there are some differences in collecting container logs, and the usage methods are also different. Here we list some common deployment and methods for reference. 1. From Host to Container \u00b6 In the traditional era of using virtual/cloud/physical machines, business processes are deployed on fixed nodes, business logs are directly output to the host machine. People only manually (or use automated tools) deploy log collection agents on nodes, and add the Agent configuration. Then logs get collected. In Kubernetes, it is not so simple. Dynamic migration : In Kubernetes clusters, Pods are actively or passively migrated, frequently destroyed and created. We cannot manually issue log collection configurations to each service as in the traditional way. Diversity of log storage : There are many different types of log storage for containers, such as stdout, hostPath, emptyDir, pv, etc. Kubernetes meta information : Log data will be centrally stored after collection. When querying logs, you need to retrieve and filter them based on dimensions such as namespace, pod, container, node, and even container environment variables and labels. At this time, Agent is required to collect these meta information and inject into the log by default. The above are all requirements and pain points different from the traditional log collection and configuration method. The reason is that the traditional method is separated from Kubernetes, cannot perceive Kubernetes, and cannot integrate with Kubernetes. 2. Log form in Kubernetes \u00b6 In order to collect container logs, let's first take a look at what solutions are generally available on the market. 2.1 Types of Logs Collected \u00b6 First of all, it should be mentioned that based on the 12 elements of cloud native, it is recommended that the business container output logs to stdout instead of printing log files. Of course, the actual situation is that it is difficult for us to do this, and the reasons are probably: The business side needs to modify the log configuration. Some complex services classify log files, such as audit logs, access logs, etc., which are generally output as independent log files. Log collector needs to process differently according to different file classifications. So under normal circumstances, we need to collect at the same time: standard output-stdout log file 2.2 Agent Deployment \u00b6 To collect container logs, the Agent can be deployed in two ways: DaemonSet : Deploy an Agent per node. Sidecar : Add a Sidecar container to each Pod and run the Log Agent. The advantages and disadvantages of both deployment methods are obvious: Resource occupancy: In containerized mode, a Node may run a lot of Pods. At this time, the method of DaemonSet is much smaller than that of Sidecar, and the more Pods on the node, the more more obvious. Intrusiveness: In the Sidecar method, the Agent needs to be injected into the business Pod. Regardless of whether there is a process of encapsulation, or the default injection by the Kubernetes webhook, the original deployment method is still changed. Stability: In most cases, log collector agent needs to ensure stability. The most important thing is not to affect the business. If the sidecar method is used, an exception or oom occurs in the agent, which can easily affect the business container. In addition, when there are many agents, it will cause certain hidden dangers to downstream services such as Kafka in terms of the number of connections. Isolation: In the case of DaemonSet, all the logs of the node share the same Agent, and Sidecar only collects business logs in the same Pod. So, the isolation of Sidecar will theoretically be better. Performance: Since Sidecar only collects logs in the Pod, the pressure is relatively small. In extreme cases, the probability of reaching the performance bottleneck of the Agent is much smaller than that of the DaemonSet method. Tip Under normal circumstances, DaemonSet is preferred to collect logs. If the log volume of a single Pod is particularly large, which exceeds the general Agent sending throughput, you can use Sidecar to collect logs for the Pod alone. 2.3 Collection Method \u00b6 DaemonSet + Stdout \u00b6 If the container runtime is docker, under normal circumstances, we can find the stdout log of the container in the docker path of the node Default should be /var/lib/docker/containers/{containerId}/{containerId}-json.log . Before Kubernetes 1.14, kubelet would create a symlink /var/log/pods/<podUID>/<containerName>/<num>.log to the stdout file. Like this: root@master0:/var/log/pods# tree . | -- 6687e53201c01e3fad31e7d72fbb92a6 | ` -- kube-apiserver | | -- 865 .log -> /var/lib/docker/containers/3a35ae0a1d0b26455fbd9b267cd9d6ac3fbd3f0b12ee03b4b22b80dc5a1cde03/3a35ae0a1d0b26455fbd9b267cd9d6ac3fbd3f0b12ee03b4b22b80dc5a1cde03-json.log | ` -- 866 .log -> /var/lib/docker/containers/15a6924f14fcbf15dd37d1c185c5b95154fa2c5f3de9513204b1066bbe474662/15a6924f14fcbf15dd37d1c185c5b95154fa2c5f3de9513204b1066bbe474662-json.log | -- a1083c6d-3b12-11ea-9af1-fa163e28f309 | ` -- kube-proxy | | -- 3 .log -> /var/lib/docker/containers/4b63b5a90a8f9ca6b6f20b49b5ab2564f92df21a5590f46de2a46b031e55c80e/4b63b5a90a8f9ca6b6f20b49b5ab2564f92df21a5590f46de2a46b031e55c80e-json.log | ` -- 4 .log -> /var/lib/docker/containers/fc7c315d33935887ca3479a38cfca4cca66fad782b8a120c548ad0b9f0ff7207/fc7c315d33935887ca3479a38cfca4cca66fad782b8a120c548ad0b9f0ff7207-json.log After Kubernetes version 1.14, it changed to /var/log/pods/<namespace>_<pod_name>_<pod_id>/<container_name>/<num>.log . root@master-0:/var/log/pods# tree . | -- kube-system_kube-apiserver-kind-control-plane_bd1c21fe1f0ef615e0b5e41299f1be61 | ` -- kube-apiserver | ` -- 0 .log | -- kube-system_kube-proxy-gcrfq_f07260b8-6055-4c19-9491-4a825579528f | ` -- kube-proxy | ` -- 0 .log ` -- loggie_loggie-csd4g_f1cc32e9-1002-4e64-bd58-fc6094394e06 ` -- loggie ` -- 0 .log Therefore, for the Agent to collect standard output logs, it is to collect these log files on the node. A simple and rude collection method is to use DaemonSet to deploy the log agent, mount /var/log/pods , and configure log file path similar with /var/log/pod/*/*.log to collect the standard output of all containers on the node. But the limitation of this method includes: It is impossible to inject more meta information, such as the label/env of some pods. Before k8s 1.14 version, even the namespace/pod and other information cannot be obtained in the collected path. It is difficult to configure special configurations for a single service. For example, a file needs to use special multi-line log collection or log segmentation. A lot of unnecessary container logs will be collected, causing pressure on collection, transmission, and storage. Of course, some current log agents such as Filebeat/Fluentd have targeted support. For example, information such as namespace/pod can be injected into the log, but most of the problems are still not solved. Therefore, this method is only suitable for simple business scenarios, and it is difficult to meet other log requirements in the future. DaemonSet + log file \u00b6 If the Pod not only outputs stdout, but also log files, you need to consider mounting the log files to the node, and the Agent deployed with DaemonSet also needs to mount the same directory, otherwise the Agent cannot view it. There are the following ways to mount the log path for the business Pod: (1) emtpyDir The life cycle of emtpyDir follows the Pod, and the logs stored in the Pod will disappear after the Pod is destroyed. Advantages: Simple to use, different Pods use their own emtpyDir, which has a certain degree of isolation. Disadvantages: If the log is not collected in time, there is a possibility of loss after the Pod is consumed. The log files mounted using emptyDir generally have the following paths on the node: /var/lib/kubelet/pods/${pod.UID}/volumes/kubernetes.io~empty-dir/${volumeName} (2) hostPath The lifecycle has nothing to do with Pods. If pods are migrated or destroyed, log files remain on existing disks. Advantages: The life cycle has nothing to do with the Pod. Even if the Pod is destroyed, the log files are still on the node disk. If the Agent does not collect logs, the log files can still be found. Disadvantages: There is no isolation by default, and the log path for mounting needs to be controlled. In addition, after the Pod is migrated, the residual log files accumulate for a long time and occupy the disk, and the quota of disk occupied by the log is not under control. To resolve isolation and avoid multiple Pods printing logs to the same path and file, we need to use the subPathExpr field to construct the subPath directory name from environment variable get from the Downward API . The VolumeSubpathEnvExpansion feature has been enabled by default since Kubernetes 1.15. Refer to feature-gates and using-subpath-expanded-environment \u3002 An example using subPathExpr is shown below: apiVersion : apps/v1 kind : Deployment metadata : labels : app : nginx name : nginx namespace : default spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name - name : NAMESPACE valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.namespace image : nginx name : nginx resources : {} volumeMounts : - mountPath : /data/log name : datalog subPathExpr : $(NAMESPACE)/$(POD_NAME) volumes : - hostPath : path : /data/log type : \"\" name : datalog After creation, we can find a similar directory structure in /data/log under the node where we are located: . `-- default |-- nginx-888dc6778-krfqr `-- nginx-888dc6778-sw8vd The log file stored in each Pod is under the /data/log/$(NAMESPACE)/$(POD_NAME) path of the node. (3) Pv Pv access modes include: ReadWriteOnce\uff08RWO\uff09: read and write permissions, and can only be mounted by a single Node. ReadOnlyMany\uff08ROX\uff09: Read-only permission, allowing to be mounted by multiple Nodes. ReadWriteMany\uff08RWX\uff09: Read and write permission, allowing to be mounted by multiple Nodes. For most businesses, stateless deployment is used, and the same Pv needs to be mounted. For some stateful services such as middleware, statefulset is generally used, and each Pod uses an independent Pv. Advantages: Logs in storage are not easy to lose; Disadvantages: There is a certain complexity of use and operation and maintenance; there are problems about isolation when multiple Pods share the same Pv; many log agents are not mature enough to collect log files on cloud disks, and there may be some hidden dangers; Although the corresponding log files mounted by Pv can also be found on Node, the path on Node will be different depending on the underlying implementation of Pv. At present, most log agents on the market are unaware of these mounting methods, so what you can do is similar to the above method of using stdout, that is, simply and rudely let the agent mount all the paths, and use the wildcard method to collect all logs. The limitations of usage are the same as for stdout. In addition, some agents have certain support for collecting docker stdout. There are some usage variants, such as using webhook to inject a sidecar, reading the log file in the Pod, converting it to the stdout of the sidecar, and then collecting the stdout log of the sidecar. It will not be described in detail here. (4) no mount In many cases, users neglect to mount the log path volume. When connecting to an existing system, the business side is often unwilling to change it. Can the log files in the container be collected at this time? Loggie tries to solve this problem (the function can be turned on by configuration), but this method still needs more tests in long-term production practice. Welcome to try it out, please refer to the next section for specific detials. Sidecar + Stdout/Log File \u00b6 If you need to use the sidecar method, you need to mount the log agent with the same log path at the same time, and the configuration file of the agent is generally mounted using ConfigMap. For a comparison of of using Sidecar and DaemonSet, please refer to the above. Summary Most popular open source agents only partially support container Stdout. For example, they support adding some K8s related meta-information when collecting, but do not support individual configuration for some Pods, and they do not support log file collection in the container. As a result, it is impossible to meet the needs in various complex business scenarios. How Does Logie Solve These Problems? \u00b6 I believe everyone has encountered or thought about the problems listed above. How does Loggie solve these problems? Please read the next section Collect Container Logs .","title":"Collect Logs in Kubernetes"},{"location":"user-guide/use-in-kubernetes/general-usage/#log-collection-in-kubernetes","text":"Cite Compared with the traditional host log collection, in the Kubernetes cluster, there are some differences in collecting container logs, and the usage methods are also different. Here we list some common deployment and methods for reference.","title":"Log collection in Kubernetes"},{"location":"user-guide/use-in-kubernetes/general-usage/#1-from-host-to-container","text":"In the traditional era of using virtual/cloud/physical machines, business processes are deployed on fixed nodes, business logs are directly output to the host machine. People only manually (or use automated tools) deploy log collection agents on nodes, and add the Agent configuration. Then logs get collected. In Kubernetes, it is not so simple. Dynamic migration : In Kubernetes clusters, Pods are actively or passively migrated, frequently destroyed and created. We cannot manually issue log collection configurations to each service as in the traditional way. Diversity of log storage : There are many different types of log storage for containers, such as stdout, hostPath, emptyDir, pv, etc. Kubernetes meta information : Log data will be centrally stored after collection. When querying logs, you need to retrieve and filter them based on dimensions such as namespace, pod, container, node, and even container environment variables and labels. At this time, Agent is required to collect these meta information and inject into the log by default. The above are all requirements and pain points different from the traditional log collection and configuration method. The reason is that the traditional method is separated from Kubernetes, cannot perceive Kubernetes, and cannot integrate with Kubernetes.","title":"1. From Host to Container"},{"location":"user-guide/use-in-kubernetes/general-usage/#2-log-form-in-kubernetes","text":"In order to collect container logs, let's first take a look at what solutions are generally available on the market.","title":"2. Log form in Kubernetes"},{"location":"user-guide/use-in-kubernetes/general-usage/#21-types-of-logs-collected","text":"First of all, it should be mentioned that based on the 12 elements of cloud native, it is recommended that the business container output logs to stdout instead of printing log files. Of course, the actual situation is that it is difficult for us to do this, and the reasons are probably: The business side needs to modify the log configuration. Some complex services classify log files, such as audit logs, access logs, etc., which are generally output as independent log files. Log collector needs to process differently according to different file classifications. So under normal circumstances, we need to collect at the same time: standard output-stdout log file","title":"2.1 Types of Logs Collected"},{"location":"user-guide/use-in-kubernetes/general-usage/#22-agent-deployment","text":"To collect container logs, the Agent can be deployed in two ways: DaemonSet : Deploy an Agent per node. Sidecar : Add a Sidecar container to each Pod and run the Log Agent. The advantages and disadvantages of both deployment methods are obvious: Resource occupancy: In containerized mode, a Node may run a lot of Pods. At this time, the method of DaemonSet is much smaller than that of Sidecar, and the more Pods on the node, the more more obvious. Intrusiveness: In the Sidecar method, the Agent needs to be injected into the business Pod. Regardless of whether there is a process of encapsulation, or the default injection by the Kubernetes webhook, the original deployment method is still changed. Stability: In most cases, log collector agent needs to ensure stability. The most important thing is not to affect the business. If the sidecar method is used, an exception or oom occurs in the agent, which can easily affect the business container. In addition, when there are many agents, it will cause certain hidden dangers to downstream services such as Kafka in terms of the number of connections. Isolation: In the case of DaemonSet, all the logs of the node share the same Agent, and Sidecar only collects business logs in the same Pod. So, the isolation of Sidecar will theoretically be better. Performance: Since Sidecar only collects logs in the Pod, the pressure is relatively small. In extreme cases, the probability of reaching the performance bottleneck of the Agent is much smaller than that of the DaemonSet method. Tip Under normal circumstances, DaemonSet is preferred to collect logs. If the log volume of a single Pod is particularly large, which exceeds the general Agent sending throughput, you can use Sidecar to collect logs for the Pod alone.","title":"2.2 Agent Deployment"},{"location":"user-guide/use-in-kubernetes/general-usage/#23-collection-method","text":"","title":"2.3 Collection Method"},{"location":"user-guide/use-in-kubernetes/general-usage/#daemonset-stdout","text":"If the container runtime is docker, under normal circumstances, we can find the stdout log of the container in the docker path of the node Default should be /var/lib/docker/containers/{containerId}/{containerId}-json.log . Before Kubernetes 1.14, kubelet would create a symlink /var/log/pods/<podUID>/<containerName>/<num>.log to the stdout file. Like this: root@master0:/var/log/pods# tree . | -- 6687e53201c01e3fad31e7d72fbb92a6 | ` -- kube-apiserver | | -- 865 .log -> /var/lib/docker/containers/3a35ae0a1d0b26455fbd9b267cd9d6ac3fbd3f0b12ee03b4b22b80dc5a1cde03/3a35ae0a1d0b26455fbd9b267cd9d6ac3fbd3f0b12ee03b4b22b80dc5a1cde03-json.log | ` -- 866 .log -> /var/lib/docker/containers/15a6924f14fcbf15dd37d1c185c5b95154fa2c5f3de9513204b1066bbe474662/15a6924f14fcbf15dd37d1c185c5b95154fa2c5f3de9513204b1066bbe474662-json.log | -- a1083c6d-3b12-11ea-9af1-fa163e28f309 | ` -- kube-proxy | | -- 3 .log -> /var/lib/docker/containers/4b63b5a90a8f9ca6b6f20b49b5ab2564f92df21a5590f46de2a46b031e55c80e/4b63b5a90a8f9ca6b6f20b49b5ab2564f92df21a5590f46de2a46b031e55c80e-json.log | ` -- 4 .log -> /var/lib/docker/containers/fc7c315d33935887ca3479a38cfca4cca66fad782b8a120c548ad0b9f0ff7207/fc7c315d33935887ca3479a38cfca4cca66fad782b8a120c548ad0b9f0ff7207-json.log After Kubernetes version 1.14, it changed to /var/log/pods/<namespace>_<pod_name>_<pod_id>/<container_name>/<num>.log . root@master-0:/var/log/pods# tree . | -- kube-system_kube-apiserver-kind-control-plane_bd1c21fe1f0ef615e0b5e41299f1be61 | ` -- kube-apiserver | ` -- 0 .log | -- kube-system_kube-proxy-gcrfq_f07260b8-6055-4c19-9491-4a825579528f | ` -- kube-proxy | ` -- 0 .log ` -- loggie_loggie-csd4g_f1cc32e9-1002-4e64-bd58-fc6094394e06 ` -- loggie ` -- 0 .log Therefore, for the Agent to collect standard output logs, it is to collect these log files on the node. A simple and rude collection method is to use DaemonSet to deploy the log agent, mount /var/log/pods , and configure log file path similar with /var/log/pod/*/*.log to collect the standard output of all containers on the node. But the limitation of this method includes: It is impossible to inject more meta information, such as the label/env of some pods. Before k8s 1.14 version, even the namespace/pod and other information cannot be obtained in the collected path. It is difficult to configure special configurations for a single service. For example, a file needs to use special multi-line log collection or log segmentation. A lot of unnecessary container logs will be collected, causing pressure on collection, transmission, and storage. Of course, some current log agents such as Filebeat/Fluentd have targeted support. For example, information such as namespace/pod can be injected into the log, but most of the problems are still not solved. Therefore, this method is only suitable for simple business scenarios, and it is difficult to meet other log requirements in the future.","title":"DaemonSet + Stdout"},{"location":"user-guide/use-in-kubernetes/general-usage/#daemonset-log-file","text":"If the Pod not only outputs stdout, but also log files, you need to consider mounting the log files to the node, and the Agent deployed with DaemonSet also needs to mount the same directory, otherwise the Agent cannot view it. There are the following ways to mount the log path for the business Pod: (1) emtpyDir The life cycle of emtpyDir follows the Pod, and the logs stored in the Pod will disappear after the Pod is destroyed. Advantages: Simple to use, different Pods use their own emtpyDir, which has a certain degree of isolation. Disadvantages: If the log is not collected in time, there is a possibility of loss after the Pod is consumed. The log files mounted using emptyDir generally have the following paths on the node: /var/lib/kubelet/pods/${pod.UID}/volumes/kubernetes.io~empty-dir/${volumeName} (2) hostPath The lifecycle has nothing to do with Pods. If pods are migrated or destroyed, log files remain on existing disks. Advantages: The life cycle has nothing to do with the Pod. Even if the Pod is destroyed, the log files are still on the node disk. If the Agent does not collect logs, the log files can still be found. Disadvantages: There is no isolation by default, and the log path for mounting needs to be controlled. In addition, after the Pod is migrated, the residual log files accumulate for a long time and occupy the disk, and the quota of disk occupied by the log is not under control. To resolve isolation and avoid multiple Pods printing logs to the same path and file, we need to use the subPathExpr field to construct the subPath directory name from environment variable get from the Downward API . The VolumeSubpathEnvExpansion feature has been enabled by default since Kubernetes 1.15. Refer to feature-gates and using-subpath-expanded-environment \u3002 An example using subPathExpr is shown below: apiVersion : apps/v1 kind : Deployment metadata : labels : app : nginx name : nginx namespace : default spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name - name : NAMESPACE valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.namespace image : nginx name : nginx resources : {} volumeMounts : - mountPath : /data/log name : datalog subPathExpr : $(NAMESPACE)/$(POD_NAME) volumes : - hostPath : path : /data/log type : \"\" name : datalog After creation, we can find a similar directory structure in /data/log under the node where we are located: . `-- default |-- nginx-888dc6778-krfqr `-- nginx-888dc6778-sw8vd The log file stored in each Pod is under the /data/log/$(NAMESPACE)/$(POD_NAME) path of the node. (3) Pv Pv access modes include: ReadWriteOnce\uff08RWO\uff09: read and write permissions, and can only be mounted by a single Node. ReadOnlyMany\uff08ROX\uff09: Read-only permission, allowing to be mounted by multiple Nodes. ReadWriteMany\uff08RWX\uff09: Read and write permission, allowing to be mounted by multiple Nodes. For most businesses, stateless deployment is used, and the same Pv needs to be mounted. For some stateful services such as middleware, statefulset is generally used, and each Pod uses an independent Pv. Advantages: Logs in storage are not easy to lose; Disadvantages: There is a certain complexity of use and operation and maintenance; there are problems about isolation when multiple Pods share the same Pv; many log agents are not mature enough to collect log files on cloud disks, and there may be some hidden dangers; Although the corresponding log files mounted by Pv can also be found on Node, the path on Node will be different depending on the underlying implementation of Pv. At present, most log agents on the market are unaware of these mounting methods, so what you can do is similar to the above method of using stdout, that is, simply and rudely let the agent mount all the paths, and use the wildcard method to collect all logs. The limitations of usage are the same as for stdout. In addition, some agents have certain support for collecting docker stdout. There are some usage variants, such as using webhook to inject a sidecar, reading the log file in the Pod, converting it to the stdout of the sidecar, and then collecting the stdout log of the sidecar. It will not be described in detail here. (4) no mount In many cases, users neglect to mount the log path volume. When connecting to an existing system, the business side is often unwilling to change it. Can the log files in the container be collected at this time? Loggie tries to solve this problem (the function can be turned on by configuration), but this method still needs more tests in long-term production practice. Welcome to try it out, please refer to the next section for specific detials.","title":"DaemonSet + log file"},{"location":"user-guide/use-in-kubernetes/general-usage/#sidecar-stdoutlog-file","text":"If you need to use the sidecar method, you need to mount the log agent with the same log path at the same time, and the configuration file of the agent is generally mounted using ConfigMap. For a comparison of of using Sidecar and DaemonSet, please refer to the above. Summary Most popular open source agents only partially support container Stdout. For example, they support adding some K8s related meta-information when collecting, but do not support individual configuration for some Pods, and they do not support log file collection in the container. As a result, it is impossible to meet the needs in various complex business scenarios.","title":"Sidecar + Stdout/Log File"},{"location":"user-guide/use-in-kubernetes/general-usage/#how-does-logie-solve-these-problems","text":"I believe everyone has encountered or thought about the problems listed above. How does Loggie solve these problems? Please read the next section Collect Container Logs .","title":"How Does Logie Solve These Problems?"},{"location":"user-guide/use-in-kubernetes/kube-event-source/","text":"Collect Kubernetes Events \u00b6 In addition to using Logconfig to collect logs, Loggie can also configure any source/sink/interceptor through CRD. In essence, Loggie is a data stream that supports multiple Pipelines, integrating common functions such as queue retry, data processing, configuration delivery, monitoring alarms and other functions, which reduces the development cost for similar requirements. Collecting Kubernetes Events is a good example of this. Kubernetes Events are events generated by Kubernetes' own components and some controllers. We can use kubectl describe to view the event information of associated resources. Collecting and recording these events can help us trace back, troubleshoot, audit, and summarize problems, and better understand internal state of Kubernetes clusters. Preparation \u00b6 Similar to Loggie Aggregator, we can Deploy Aggregator cluster separately or reuse the existing aggregator cluster. Configuration Example \u00b6 Configure the kubeEvents source and use type: cluster to distribute configuration to the Aggregator cluster. Config apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : kubeevent spec : selector : type : cluster cluster : aggregator pipeline : sources : | - type: kubeEvent name: event sinkRef : dev By default, whether it is sent to Elasticsearch or other sinks, the output is in a format similar to the following: event { \"body\" : \"{\\\"metadata\\\":{\\\"name\\\":\\\"loggie-aggregator.16c277f8fc4ff0d0\\\",\\\"namespace\\\":\\\"loggie-aggregator\\\",\\\"uid\\\":\\\"084cea27-cd4a-4ce4-97ef-12e70f37880e\\\",\\\"resourceVersion\\\":\\\"2975193\\\",\\\"creationTimestamp\\\":\\\"2021-12-20T12:58:45Z\\\",\\\"managedFields\\\":[{\\\"manager\\\":\\\"kube-controller-manager\\\",\\\"operation\\\":\\\"Update\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"time\\\":\\\"2021-12-20T12:58:45Z\\\",\\\"fieldsType\\\":\\\"FieldsV1\\\",\\\"fieldsV1\\\":{\\\"f:count\\\":{},\\\"f:firstTimestamp\\\":{},\\\"f:involvedObject\\\":{\\\"f:apiVersion\\\":{},\\\"f:kind\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{},\\\"f:resourceVersion\\\":{},\\\"f:uid\\\":{}},\\\"f:lastTimestamp\\\":{},\\\"f:message\\\":{},\\\"f:reason\\\":{},\\\"f:source\\\":{\\\"f:component\\\":{}},\\\"f:type\\\":{}}}]},\\\"involvedObject\\\":{\\\"kind\\\":\\\"DaemonSet\\\",\\\"namespace\\\":\\\"loggie-aggregator\\\",\\\"name\\\":\\\"loggie-aggregator\\\",\\\"uid\\\":\\\"7cdf4792-815d-4eba-8a81-d60131ad1fc4\\\",\\\"apiVersion\\\":\\\"apps/v1\\\",\\\"resourceVersion\\\":\\\"2975170\\\"},\\\"reason\\\":\\\"SuccessfulCreate\\\",\\\"message\\\":\\\"Created pod: loggie-aggregator-pbkjk\\\",\\\"source\\\":{\\\"component\\\":\\\"daemonset-controller\\\"},\\\"firstTimestamp\\\":\\\"2021-12-20T12:58:45Z\\\",\\\"lastTimestamp\\\":\\\"2021-12-20T12:58:45Z\\\",\\\"count\\\":1,\\\"type\\\":\\\"Normal\\\",\\\"eventTime\\\":null,\\\"reportingComponent\\\":\\\"\\\",\\\"reportingInstance\\\":\\\"\\\"}\" , \"systemPipelineName\" : \"default/kubeevent/\" , \"systemSourceName\" : \"event\" } To facilitate analysis and display, we can add some interceptors to json-decode the collected events data. The configuration example is as follows. For details, please refer to Log Segmentation \u3002 Config interceptor apiVersion : loggie.io/v1beta1 kind : Interceptor metadata : name : jsondecode spec : interceptors : | - type: normalize name: json processors: - jsonDecode: ~ - drop: targets: [\"body\"] clusterLogConfig apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : kubeevent spec : selector : type : cluster cluster : aggregator pipeline : sources : | - type: kubeEvent name: event interceptorRef : jsondecode sinkRef : dev The data after jsonDecode in normalize interceptor is as follows: event { \"metadata\" : { \"name\" : \"loggie-aggregator.16c277f8fc4ff0d0\" , \"namespace\" : \"loggie-aggregator\" , \"uid\" : \"084cea27-cd4a-4ce4-97ef-12e70f37880e\" , \"resourceVersion\" : \"2975193\" , \"creationTimestamp\" : \"2021-12-20T12:58:45Z\" , \"managedFields\" : [ { \"fieldsType\" : \"FieldsV1\" , \"fieldsV1\" : { \"f:type\" : { }, \"f:count\" : { }, \"f:firstTimestamp\" : { }, \"f:involvedObject\" : { \"f:apiVersion\" : { }, \"f:kind\" : { }, \"f:name\" : { }, \"f:namespace\" : { }, \"f:resourceVersion\" : { }, \"f:uid\" : { } }, \"f:lastTimestamp\" : { }, \"f:message\" : { }, \"f:reason\" : { }, \"f:source\" : { \"f:component\" : { } } }, \"manager\" : \"kube-controller-manager\" , \"operation\" : \"Update\" , \"apiVersion\" : \"v1\" , \"time\" : \"2021-12-20T12:58:45Z\" } ] }, \"reportingComponent\" : \"\" , \"type\" : \"Normal\" , \"message\" : \"Created pod: loggie-aggregator-pbkjk\" , \"reason\" : \"SuccessfulCreate\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"daemonset-controller\" }, \"count\" : 1 , \"lastTimestamp\" : \"2021-12-20T12:58:45Z\" , \"firstTimestamp\" : \"2021-12-20T12:58:45Z\" , \"eventTime\" : null , \"involvedObject\" : { \"kind\" : \"DaemonSet\" , \"namespace\" : \"loggie-aggregator\" , \"name\" : \"loggie-aggregator\" , \"uid\" : \"7cdf4792-815d-4eba-8a81-d60131ad1fc4\" , \"apiVersion\" : \"apps/v1\" , \"resourceVersion\" : \"2975170\" }, } If you feel that there are too many data fields or the format does not meet the requirements, you can also configure the normalize interceptor to modify it.","title":"Collect Kubernetes Events"},{"location":"user-guide/use-in-kubernetes/kube-event-source/#collect-kubernetes-events","text":"In addition to using Logconfig to collect logs, Loggie can also configure any source/sink/interceptor through CRD. In essence, Loggie is a data stream that supports multiple Pipelines, integrating common functions such as queue retry, data processing, configuration delivery, monitoring alarms and other functions, which reduces the development cost for similar requirements. Collecting Kubernetes Events is a good example of this. Kubernetes Events are events generated by Kubernetes' own components and some controllers. We can use kubectl describe to view the event information of associated resources. Collecting and recording these events can help us trace back, troubleshoot, audit, and summarize problems, and better understand internal state of Kubernetes clusters.","title":"Collect Kubernetes Events"},{"location":"user-guide/use-in-kubernetes/kube-event-source/#preparation","text":"Similar to Loggie Aggregator, we can Deploy Aggregator cluster separately or reuse the existing aggregator cluster.","title":"Preparation"},{"location":"user-guide/use-in-kubernetes/kube-event-source/#configuration-example","text":"Configure the kubeEvents source and use type: cluster to distribute configuration to the Aggregator cluster. Config apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : kubeevent spec : selector : type : cluster cluster : aggregator pipeline : sources : | - type: kubeEvent name: event sinkRef : dev By default, whether it is sent to Elasticsearch or other sinks, the output is in a format similar to the following: event { \"body\" : \"{\\\"metadata\\\":{\\\"name\\\":\\\"loggie-aggregator.16c277f8fc4ff0d0\\\",\\\"namespace\\\":\\\"loggie-aggregator\\\",\\\"uid\\\":\\\"084cea27-cd4a-4ce4-97ef-12e70f37880e\\\",\\\"resourceVersion\\\":\\\"2975193\\\",\\\"creationTimestamp\\\":\\\"2021-12-20T12:58:45Z\\\",\\\"managedFields\\\":[{\\\"manager\\\":\\\"kube-controller-manager\\\",\\\"operation\\\":\\\"Update\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"time\\\":\\\"2021-12-20T12:58:45Z\\\",\\\"fieldsType\\\":\\\"FieldsV1\\\",\\\"fieldsV1\\\":{\\\"f:count\\\":{},\\\"f:firstTimestamp\\\":{},\\\"f:involvedObject\\\":{\\\"f:apiVersion\\\":{},\\\"f:kind\\\":{},\\\"f:name\\\":{},\\\"f:namespace\\\":{},\\\"f:resourceVersion\\\":{},\\\"f:uid\\\":{}},\\\"f:lastTimestamp\\\":{},\\\"f:message\\\":{},\\\"f:reason\\\":{},\\\"f:source\\\":{\\\"f:component\\\":{}},\\\"f:type\\\":{}}}]},\\\"involvedObject\\\":{\\\"kind\\\":\\\"DaemonSet\\\",\\\"namespace\\\":\\\"loggie-aggregator\\\",\\\"name\\\":\\\"loggie-aggregator\\\",\\\"uid\\\":\\\"7cdf4792-815d-4eba-8a81-d60131ad1fc4\\\",\\\"apiVersion\\\":\\\"apps/v1\\\",\\\"resourceVersion\\\":\\\"2975170\\\"},\\\"reason\\\":\\\"SuccessfulCreate\\\",\\\"message\\\":\\\"Created pod: loggie-aggregator-pbkjk\\\",\\\"source\\\":{\\\"component\\\":\\\"daemonset-controller\\\"},\\\"firstTimestamp\\\":\\\"2021-12-20T12:58:45Z\\\",\\\"lastTimestamp\\\":\\\"2021-12-20T12:58:45Z\\\",\\\"count\\\":1,\\\"type\\\":\\\"Normal\\\",\\\"eventTime\\\":null,\\\"reportingComponent\\\":\\\"\\\",\\\"reportingInstance\\\":\\\"\\\"}\" , \"systemPipelineName\" : \"default/kubeevent/\" , \"systemSourceName\" : \"event\" } To facilitate analysis and display, we can add some interceptors to json-decode the collected events data. The configuration example is as follows. For details, please refer to Log Segmentation \u3002 Config interceptor apiVersion : loggie.io/v1beta1 kind : Interceptor metadata : name : jsondecode spec : interceptors : | - type: normalize name: json processors: - jsonDecode: ~ - drop: targets: [\"body\"] clusterLogConfig apiVersion : loggie.io/v1beta1 kind : ClusterLogConfig metadata : name : kubeevent spec : selector : type : cluster cluster : aggregator pipeline : sources : | - type: kubeEvent name: event interceptorRef : jsondecode sinkRef : dev The data after jsonDecode in normalize interceptor is as follows: event { \"metadata\" : { \"name\" : \"loggie-aggregator.16c277f8fc4ff0d0\" , \"namespace\" : \"loggie-aggregator\" , \"uid\" : \"084cea27-cd4a-4ce4-97ef-12e70f37880e\" , \"resourceVersion\" : \"2975193\" , \"creationTimestamp\" : \"2021-12-20T12:58:45Z\" , \"managedFields\" : [ { \"fieldsType\" : \"FieldsV1\" , \"fieldsV1\" : { \"f:type\" : { }, \"f:count\" : { }, \"f:firstTimestamp\" : { }, \"f:involvedObject\" : { \"f:apiVersion\" : { }, \"f:kind\" : { }, \"f:name\" : { }, \"f:namespace\" : { }, \"f:resourceVersion\" : { }, \"f:uid\" : { } }, \"f:lastTimestamp\" : { }, \"f:message\" : { }, \"f:reason\" : { }, \"f:source\" : { \"f:component\" : { } } }, \"manager\" : \"kube-controller-manager\" , \"operation\" : \"Update\" , \"apiVersion\" : \"v1\" , \"time\" : \"2021-12-20T12:58:45Z\" } ] }, \"reportingComponent\" : \"\" , \"type\" : \"Normal\" , \"message\" : \"Created pod: loggie-aggregator-pbkjk\" , \"reason\" : \"SuccessfulCreate\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"daemonset-controller\" }, \"count\" : 1 , \"lastTimestamp\" : \"2021-12-20T12:58:45Z\" , \"firstTimestamp\" : \"2021-12-20T12:58:45Z\" , \"eventTime\" : null , \"involvedObject\" : { \"kind\" : \"DaemonSet\" , \"namespace\" : \"loggie-aggregator\" , \"name\" : \"loggie-aggregator\" , \"uid\" : \"7cdf4792-815d-4eba-8a81-d60131ad1fc4\" , \"apiVersion\" : \"apps/v1\" , \"resourceVersion\" : \"2975170\" }, } If you feel that there are too many data fields or the format does not meet the requirements, you can also configure the normalize interceptor to modify it.","title":"Configuration Example"},{"location":"user-guide/use-in-kubernetes/sidecar/","text":"Logs Collected in Sidecar Method \u00b6 Although it is not recommended to use the sidecar method to collect container logs by default, in some limited scenarios, only the sidecar method can be used to collect container logs. Here we give an example for reference. General Idea \u00b6 As shown below: Loggie and the business container are deployed in the same Pod, and the same log file volume needs to be mounted. In addition, the configuration of Loggie can be mounted to the container through configMap. Loggie collects the logs of the container according to the provided configuration file in configMap, and send them to the backend. Injecte Loggie Sidecar \u00b6 apiVersion : apps/v1 kind : Deployment metadata : labels : app : tomcat name : tomcat namespace : default spec : replicas : 1 selector : matchLabels : app : tomcat template : metadata : labels : app : tomcat spec : containers : - name : tomcat image : tomcat volumeMounts : - mountPath : /usr/local/tomcat/logs name : log - name : loggie args : - -config.system=/opt/loggie/loggie.yml - -config.pipeline=/opt/loggie/pipeline.yml image : loggieio/loggie:main volumeMounts : # same log volume - mountPath : /usr/local/tomcat/logs name : log # loggie configuration configMap - mountPath : /opt/loggie name : loggie-config # loggie data - mountPath : /data name : registry volumes : - emptyDir : {} name : log - emptyDir : {} name : registry - name : loggie-config configMap : name : tomcat-loggie-config --- apiVersion : v1 kind : ConfigMap metadata : name : tomcat-loggie-config namespace : default data : loggie.yml : | loggie: reload: enabled: true period: 10s monitor: logger: period: 30s enabled: true listeners: filesource: ~ filewatcher: ~ reload: ~ sink: ~ http: enabled: true port: 9196 pipeline.yml : | pipelines: - name: \"tomcat\" sources: - type: \"file\" name: \"tomcatlog\" paths: - \"/usr/local/tomcat/logs/*.log\" fields: namespace: default deployment: tomcat fieldsFromEnv: podname: HOSTNAME sink: type: \"dev\" printEvents: true codec: pretty: true You can use the loggie sidecar to collect container logs by referring to the above exapmle. Node: At present, it is not recommended to enable kubernetes discovery in the configuration file. Since Kubernetes will be requested after opening, when there are many Pods, it will cause certain pressure on Kubernetes. Therefore, LogConfig CRD cannot be used, and configMap needs to be used to mount the configuration file. Since Kubernetes discovery is not used, the fields here will not be automatically added with the Pod's meta information, and need to be obtained from the Pod's environment variables using the fieldsFromEnv. Tips Fixed information such as namespace can be configured in fields, or referenced to env by using the downward API. The Env environment variable obtained by fieldsFromEnv is not limited to the env field configured in the Pod yaml, but any environment variable in the Loggie container. We can execute the env command in the container to view it. Modifying the parameters in the configMap will take a period of time to be refreshed to the Pod. If you want it to take effect immediately, you need to rebuild the Pod. Please pay attention to whether it will affect the business. Info In the future, Loggie will support automatic Sidecar injection and automatic generation of ConfigMap mounts through LogConfig, so as to achieve the same experience as using DaemonSet.","title":"Sidecar Mode"},{"location":"user-guide/use-in-kubernetes/sidecar/#logs-collected-in-sidecar-method","text":"Although it is not recommended to use the sidecar method to collect container logs by default, in some limited scenarios, only the sidecar method can be used to collect container logs. Here we give an example for reference.","title":"Logs Collected in Sidecar Method"},{"location":"user-guide/use-in-kubernetes/sidecar/#general-idea","text":"As shown below: Loggie and the business container are deployed in the same Pod, and the same log file volume needs to be mounted. In addition, the configuration of Loggie can be mounted to the container through configMap. Loggie collects the logs of the container according to the provided configuration file in configMap, and send them to the backend.","title":"General Idea"},{"location":"user-guide/use-in-kubernetes/sidecar/#injecte-loggie-sidecar","text":"apiVersion : apps/v1 kind : Deployment metadata : labels : app : tomcat name : tomcat namespace : default spec : replicas : 1 selector : matchLabels : app : tomcat template : metadata : labels : app : tomcat spec : containers : - name : tomcat image : tomcat volumeMounts : - mountPath : /usr/local/tomcat/logs name : log - name : loggie args : - -config.system=/opt/loggie/loggie.yml - -config.pipeline=/opt/loggie/pipeline.yml image : loggieio/loggie:main volumeMounts : # same log volume - mountPath : /usr/local/tomcat/logs name : log # loggie configuration configMap - mountPath : /opt/loggie name : loggie-config # loggie data - mountPath : /data name : registry volumes : - emptyDir : {} name : log - emptyDir : {} name : registry - name : loggie-config configMap : name : tomcat-loggie-config --- apiVersion : v1 kind : ConfigMap metadata : name : tomcat-loggie-config namespace : default data : loggie.yml : | loggie: reload: enabled: true period: 10s monitor: logger: period: 30s enabled: true listeners: filesource: ~ filewatcher: ~ reload: ~ sink: ~ http: enabled: true port: 9196 pipeline.yml : | pipelines: - name: \"tomcat\" sources: - type: \"file\" name: \"tomcatlog\" paths: - \"/usr/local/tomcat/logs/*.log\" fields: namespace: default deployment: tomcat fieldsFromEnv: podname: HOSTNAME sink: type: \"dev\" printEvents: true codec: pretty: true You can use the loggie sidecar to collect container logs by referring to the above exapmle. Node: At present, it is not recommended to enable kubernetes discovery in the configuration file. Since Kubernetes will be requested after opening, when there are many Pods, it will cause certain pressure on Kubernetes. Therefore, LogConfig CRD cannot be used, and configMap needs to be used to mount the configuration file. Since Kubernetes discovery is not used, the fields here will not be automatically added with the Pod's meta information, and need to be obtained from the Pod's environment variables using the fieldsFromEnv. Tips Fixed information such as namespace can be configured in fields, or referenced to env by using the downward API. The Env environment variable obtained by fieldsFromEnv is not limited to the env field configured in the Pod yaml, but any environment variable in the Loggie container. We can execute the env command in the container to view it. Modifying the parameters in the configMap will take a period of time to be refreshed to the Pod. If you want it to take effect immediately, you need to rebuild the Pod. Please pay attention to whether it will affect the business. Info In the future, Loggie will support automatic Sidecar injection and automatic generation of ConfigMap mounts through LogConfig, so as to achieve the same experience as using DaemonSet.","title":"Injecte Loggie Sidecar"}]}